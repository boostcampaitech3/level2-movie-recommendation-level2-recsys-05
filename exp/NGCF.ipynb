{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from box import Box\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 학습 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data_path' : \"/opt/ml/input/data/train\" , # 데이터 경로\n",
    "    \n",
    "    'submission_path' : \"../submission\",\n",
    "    'submission_name' : 'NGCF_submission.csv', \n",
    "\n",
    "    'model_path' : \"../model\", # 모델 저장 경로\n",
    "    'model_name' : 'NGCF_v1.pt',\n",
    "\n",
    "    'num_epochs' : 50,\n",
    "    \"reg\" : 1e-5,\n",
    "    'lr' : 0.0001,\n",
    "    \"emb_dim\" : 512,\n",
    "    \"layers\" : [512, 512],\n",
    "    'batch_size' : 1024,\n",
    "    \"node_dropout\" : 0.2,\n",
    "    \"mess_dropout\" : 0.2,\n",
    "\n",
    "    'valid_samples' : 10, # 검증에 사용할 sample 수\n",
    "    'seed' : 22,\n",
    "    'n_batch' : 30,\n",
    "}\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "config = Box(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(config.model_path):\n",
    "    os.mkdir(config.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(config.submission_path):\n",
    "    os.mkdir(config.submission_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeGraphDataSet():\n",
    "    \"\"\"\n",
    "    GraphDataSet 생성\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.df = pd.read_csv(os.path.join(self.config.data_path, 'train_ratings.csv'))\n",
    "\n",
    "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('item')\n",
    "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('user')\n",
    "        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n",
    "\n",
    "        self.df['item_idx'] = self.df['item'].apply(lambda x : self.item_encoder[x])\n",
    "        self.df['user_idx'] = self.df['user'].apply(lambda x : self.user_encoder[x])\n",
    "        \n",
    "        self.exist_users = [i for i in range(self.num_user)]\n",
    "        self.exist_items = [i for i in range(self.num_item)]\n",
    "        self.user_train, self.user_valid = self.generate_sequence_data()\n",
    "        self.R_train, self.R_valid, self.R_total = self.generate_dok_matrix()\n",
    "        self.ngcf_adj_matrix = self.generate_ngcf_adj_matrix()\n",
    "        self.n_train = len(self.R_train)\n",
    "        self.batch_size = self.config.batch_size\n",
    "\n",
    "    def generate_encoder_decoder(self, col : str) -> dict:\n",
    "        \"\"\"\n",
    "        encoder, decoder 생성\n",
    "\n",
    "        Args:\n",
    "            col (str): 생성할 columns 명\n",
    "        Returns:\n",
    "            dict: 생성된 user encoder, decoder\n",
    "        \"\"\"\n",
    "\n",
    "        encoder = {}\n",
    "        decoder = {}\n",
    "        ids = self.df[col].unique()\n",
    "\n",
    "        for idx, _id in enumerate(ids):\n",
    "            encoder[_id] = idx\n",
    "            decoder[idx] = _id\n",
    "\n",
    "        return encoder, decoder\n",
    "    \n",
    "    def generate_sequence_data(self) -> dict:\n",
    "        \"\"\"\n",
    "        sequence_data 생성\n",
    "\n",
    "        Returns:\n",
    "            dict: train user sequence / valid user sequence\n",
    "        \"\"\"\n",
    "        users = defaultdict(list)\n",
    "        user_train = {}\n",
    "        user_valid = {}\n",
    "        for user, item, time in zip(self.df['user_idx'], self.df['item_idx'], self.df['time']):\n",
    "            users[user].append(item)\n",
    "        \n",
    "        for user in users:\n",
    "            np.random.seed(self.config.seed)\n",
    "\n",
    "            user_total = users[user]\n",
    "            valid = np.random.choice(user_total, size = self.config.valid_samples, replace = False).tolist()\n",
    "            train = list(set(user_total) - set(valid))\n",
    "\n",
    "            user_train[user] = train\n",
    "            user_valid[user] = valid # valid_samples 개수 만큼 검증에 활용 (현재 Task와 가장 유사하게)\n",
    "\n",
    "        return user_train, user_valid\n",
    "    \n",
    "    def generate_dok_matrix(self):\n",
    "        R_train = sp.dok_matrix((self.num_user, self.num_item), dtype=np.float32)\n",
    "        R_valid = sp.dok_matrix((self.num_user, self.num_item), dtype=np.float32)\n",
    "        R_total = sp.dok_matrix((self.num_user, self.num_item), dtype=np.float32)\n",
    "        user_list = self.exist_users\n",
    "        for user in user_list:\n",
    "            train_items = self.user_train[user]\n",
    "            valid_items = self.user_valid[user]\n",
    "            \n",
    "            for train_item in train_items:\n",
    "                R_train[user, train_item] = 1.0\n",
    "                R_total[user, train_item] = 1.0\n",
    "            \n",
    "            for valid_item in valid_items:\n",
    "                R_valid[user, valid_item] = 1.0\n",
    "                R_total[user, valid_item] = 1.0\n",
    "        \n",
    "        return R_train, R_valid, R_total\n",
    "\n",
    "    def generate_ngcf_adj_matrix(self):\n",
    "        adj_mat = sp.dok_matrix((self.num_user + self.num_item, self.num_user + self.num_item), dtype=np.float32)\n",
    "        adj_mat = adj_mat.tolil() # to_list\n",
    "        R = self.R_train.tolil()\n",
    "\n",
    "        adj_mat[:self.num_user, self.num_user:] = R\n",
    "        adj_mat[self.num_user:, :self.num_user] = R.T\n",
    "        adj_mat = adj_mat.todok() # to_dok_matrix\n",
    "\n",
    "        def normalized_adj_single(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "            d_inv = np.power(rowsum, -.5).flatten()  \n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "            norm_adj = d_mat_inv.dot(adj).dot(d_mat_inv)\n",
    "\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        ngcf_adj_matrix = normalized_adj_single(adj_mat)\n",
    "        return ngcf_adj_matrix.tocsr()\n",
    "\n",
    "    def sampling(self):\n",
    "        users = random.sample(self.exist_users, self.config.batch_size)\n",
    "\n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            pos_items = self.user_train[u]\n",
    "            pos_batch = random.sample(pos_items, num)\n",
    "            return pos_batch\n",
    "        \n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            neg_items = list(set(self.exist_items) - set(self.user_train[u]))\n",
    "            neg_batch = random.sample(neg_items, num)\n",
    "            return neg_batch\n",
    "        \n",
    "        pos_items, neg_items = [], []\n",
    "        for user in users:\n",
    "            pos_items += sample_pos_items_for_u(user, 1)\n",
    "            neg_items += sample_neg_items_for_u(user, 1)\n",
    "        \n",
    "        return users, pos_items, neg_items\n",
    "\n",
    "    def get_train_valid_data(self):\n",
    "        return self.user_train, self.user_valid\n",
    "\n",
    "    def get_R_data(self):\n",
    "        return self.R_train, self.R_valid, self.R_total\n",
    "\n",
    "    def get_ngcf_adj_matrix_data(self):\n",
    "        return self.ngcf_adj_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGCF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_dim, layers, reg, node_dropout, mess_dropout, adj_mtx):\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize Class attributes\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.emb_dim = emb_dim\n",
    "        self.l_matrix = adj_mtx\n",
    "        self.l_plus_i_matrix = adj_mtx + sp.eye(adj_mtx.shape[0])\n",
    "        self.reg = reg\n",
    "        self.layers = layers\n",
    "        self.n_layers = len(self.layers)\n",
    "        self.node_dropout = node_dropout\n",
    "        self.mess_dropout = mess_dropout\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weight_dict = self._init_weights()\n",
    "        print(\"Weights initialized.\")\n",
    "\n",
    "        # Create Matrix 'L+I', PyTorch sparse tensor of SP adjacency_mtx\n",
    "        self.L_plus_I = self._convert_sp_mat_to_sp_tensor(self.l_plus_i_matrix)\n",
    "        self.L = self._convert_sp_mat_to_sp_tensor(self.l_matrix)\n",
    "\n",
    "    # initialize weights\n",
    "    def _init_weights(self):\n",
    "        print(\"Initializing weights...\")\n",
    "        weight_dict = nn.ParameterDict()\n",
    "\n",
    "        initializer = torch.nn.init.xavier_uniform_\n",
    "        \n",
    "        weight_dict['user_embedding'] = nn.Parameter(initializer(torch.empty(self.n_users, self.emb_dim).to(device)))\n",
    "        weight_dict['item_embedding'] = nn.Parameter(initializer(torch.empty(self.n_items, self.emb_dim).to(device)))\n",
    "\n",
    "        weight_size_list = [self.emb_dim] + self.layers\n",
    "\n",
    "        for k in range(self.n_layers):\n",
    "            weight_dict['W_one_%d' %k] = nn.Parameter(initializer(torch.empty(weight_size_list[k], weight_size_list[k+1]).to(device)))\n",
    "            weight_dict['b_one_%d' %k] = nn.Parameter(initializer(torch.empty(1, weight_size_list[k+1]).to(device)))\n",
    "            \n",
    "            weight_dict['W_two_%d' %k] = nn.Parameter(initializer(torch.empty(weight_size_list[k], weight_size_list[k+1]).to(device)))\n",
    "            weight_dict['b_two_%d' %k] = nn.Parameter(initializer(torch.empty(1, weight_size_list[k+1]).to(device)))\n",
    "           \n",
    "        return weight_dict\n",
    "\n",
    "    # convert sparse matrix into sparse PyTorch tensor\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        \"\"\"\n",
    "        Convert scipy sparse matrix to PyTorch sparse matrix\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        X = Adjacency matrix, scipy sparse matrix\n",
    "        \"\"\"\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        i = torch.LongTensor(np.mat([coo.row, coo.col]))\n",
    "        v = torch.FloatTensor(coo.data)\n",
    "        res = torch.sparse.FloatTensor(i, v, coo.shape).to(device)\n",
    "        return res\n",
    "\n",
    "    # apply node_dropout\n",
    "    def _droupout_sparse(self, X):\n",
    "        \"\"\"\n",
    "        Drop individual locations in X\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "        X = adjacency matrix (PyTorch sparse tensor)\n",
    "        dropout = fraction of nodes to drop\n",
    "        noise_shape = number of non non-zero entries of X\n",
    "        \"\"\"\n",
    "        node_dropout_mask = ((self.node_dropout) + torch.rand(X._nnz())).floor().bool().to(device)\n",
    "        i = X.coalesce().indices()\n",
    "        v = X.coalesce()._values()\n",
    "        i[:,node_dropout_mask] = 0\n",
    "        v[node_dropout_mask] = 0\n",
    "        X_dropout = torch.sparse.FloatTensor(i, v, X.shape).to(X.device)\n",
    "\n",
    "        return  X_dropout.mul(1/(1-self.node_dropout))\n",
    "\n",
    "    def forward(self, u, i, j):\n",
    "        \"\"\"\n",
    "        Computes the forward pass\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "        u = user\n",
    "        i = positive item (user interacted with item)\n",
    "        j = negative item (user did not interact with item)\n",
    "        \"\"\"\n",
    "        # apply drop-out mask\n",
    "        L_plus_I_hat = self._droupout_sparse(self.L_plus_I) if self.node_dropout > 0 else self.L_plus_I\n",
    "        L_hat = self._droupout_sparse(self.L) if self.node_dropout > 0 else self.L\n",
    "        \n",
    "        # 논문 수식 (1)\n",
    "        ego_embeddings = torch.cat([self.weight_dict['user_embedding'], self.weight_dict['item_embedding']], 0)\n",
    "\n",
    "        final_embeddings = [ego_embeddings]\n",
    "\n",
    "        # forward pass for 'n' propagation layers\n",
    "        for k in range(self.n_layers):\n",
    "            \n",
    "            \n",
    "            ########## Fill below ###########\n",
    "            ### 논문 수식 (7) ###\n",
    "            \n",
    "            # (L+I)E\n",
    "            side_L_plus_I_embeddings = torch.sparse.mm(L_plus_I_hat, final_embeddings[k]) #힌트 : use torch.sparse.mm \n",
    "            \n",
    "            # (L+I)EW_1 + b_1\n",
    "            simple_embeddings = torch.matmul(side_L_plus_I_embeddings, self.weight_dict['W_one_%d' % k]) + self.weight_dict['b_one_%d' % k] #힌트 : use torch.matmul, self.weight_dict['W_one_%d' % k], self.weight_dict['b_one_%d' % k]\n",
    "            \n",
    "            # LE\n",
    "            side_L_embeddings = torch.sparse.mm(L_hat, final_embeddings[k]) #힌트 : use torch.sparse.mm                                \n",
    "            \n",
    "            # LEE\n",
    "            interaction_embeddings = torch.mul(side_L_embeddings, final_embeddings[k]) #힌트 : use torch.mul\n",
    "                                             \n",
    "            # LEEW_2 + b_2\n",
    "            interaction_embeddings = torch.matmul(interaction_embeddings, self.weight_dict['W_two_%d' % k]) + self.weight_dict['b_two_%d' % k]  #힌트 : use torch.matmul, self.weight_dict['W_two_%d' % k], self.weight_dict['b_two_%d' % k]\n",
    "\n",
    "            # non-linear activation \n",
    "            ego_embeddings =  F.leaky_relu(simple_embeddings + interaction_embeddings) #힌트: use simple_embeddings, interaction_embeddings\n",
    "            \n",
    "            ########## Fill above ###########\n",
    "            \n",
    "            # add message dropout\n",
    "            mess_dropout_mask = nn.Dropout(self.mess_dropout)\n",
    "            ego_embeddings = mess_dropout_mask(ego_embeddings)\n",
    "\n",
    "            # Perform L2 normalization\n",
    "            norm_embeddings = F.normalize(ego_embeddings, p=2, dim=1)\n",
    "            \n",
    "            ### 논문 수식 (9) ###\n",
    "            final_embeddings.append(norm_embeddings)                                            \n",
    "\n",
    "        \n",
    "        final_embeddings = torch.cat(final_embeddings, 1)                           \n",
    "    \n",
    "        # back to user/item dimension\n",
    "        u_final_embeddings, i_final_embeddings = final_embeddings.split([self.n_users, self.n_items], 0)\n",
    "\n",
    "        self.u_final_embeddings = nn.Parameter(u_final_embeddings)\n",
    "        self.i_final_embeddings = nn.Parameter(i_final_embeddings)\n",
    "        \n",
    "        u_emb = u_final_embeddings[u] # user embeddings\n",
    "        p_emb = i_final_embeddings[i] # positive item embeddings\n",
    "        n_emb = i_final_embeddings[j] # negative item embeddings\n",
    "    \n",
    "        \n",
    "        ########## Fill below ###########\n",
    "        ### 논문 수식 (10) ###\n",
    "        \n",
    "        y_ui = torch.sum(torch.mul(u_emb, p_emb), dim = 1) # 힌트 : use torch.mul, sum() method                             \n",
    "        y_uj = torch.sum(torch.mul(u_emb, n_emb), dim = 1) # 힌트 : use torch.mul, sum() method \n",
    "        \n",
    "        ########## Fill above ########### \n",
    "        \n",
    "        ########## Fill below ########### \n",
    "        ### 논문 수식 (11) ###\n",
    "        \n",
    "        log_prob = torch.mean(torch.log(torch.sigmoid(y_ui - y_uj))) # 힌트 : use torch.log, torch.sigmoid, mean() method\n",
    "        bpr_loss = -log_prob        \n",
    "        if self.reg > 0.:\n",
    "            l2norm = (torch.sum(u_emb**2)/2. + torch.sum(p_emb**2)/2. + torch.sum(n_emb**2)/2.) / u_emb.shape[0]\n",
    "            l2reg = self.reg * l2norm # FILL HERE #\n",
    "            bpr_loss += l2reg\n",
    "        \n",
    "        ########## Fill above ###########\n",
    "        \n",
    "        return bpr_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, make_graph_data_set, optimizer, n_batch):\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "    for step in range(1, n_batch + 1):\n",
    "        user, pos, neg = make_graph_data_set.sampling()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(user, pos, neg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_val += loss.item()\n",
    "    loss_val /= n_batch\n",
    "    return loss_val\n",
    "\n",
    "def split_matrix(X, n_splits=10):\n",
    "    splits = []\n",
    "    chunk_size = X.shape[0] // n_splits\n",
    "    for i in range(n_splits):\n",
    "        start = i * chunk_size\n",
    "        end = X.shape[0] if i == n_splits - 1 else (i + 1) * chunk_size\n",
    "        splits.append(X[start:end])\n",
    "    return splits\n",
    "\n",
    "def compute_ndcg_k(pred_items, test_items, test_indices, k):\n",
    "    \n",
    "    r = (test_items * pred_items).gather(1, test_indices)\n",
    "    f = torch.from_numpy(np.log2(np.arange(2, k+2))).float().to(device)\n",
    "    \n",
    "    dcg = (r[:, :k]/f).sum(1)                                               \n",
    "    dcg_max = (torch.sort(r, dim=1, descending=True)[0][:, :k]/f).sum(1)   \n",
    "    ndcg = dcg/dcg_max                                                     \n",
    "    \n",
    "    ndcg[torch.isnan(ndcg)] = 0\n",
    "    return ndcg\n",
    "\n",
    "def evaluate(u_emb, i_emb, Rtr, Rte, k = 10):\n",
    "\n",
    "    # split matrices\n",
    "    ue_splits = split_matrix(u_emb)\n",
    "    tr_splits = split_matrix(Rtr)\n",
    "    te_splits = split_matrix(Rte)\n",
    "\n",
    "    recall_k, ndcg_k= [], []\n",
    "    # compute results for split matrices\n",
    "    for ue_f, tr_f, te_f in zip(ue_splits, tr_splits, te_splits):\n",
    "\n",
    "        scores = torch.mm(ue_f, i_emb.t())\n",
    "\n",
    "        test_items = torch.from_numpy(te_f.todense()).float().to(device)\n",
    "        non_train_items = torch.from_numpy(1-(tr_f.todense())).float().to(device)\n",
    "        scores = scores * non_train_items\n",
    "\n",
    "        _, test_indices = torch.topk(scores, dim=1, k=k)\n",
    "        \n",
    "        pred_items = torch.zeros_like(scores).float()\n",
    "        pred_items.scatter_(dim=1, index=test_indices, src=torch.ones_like(test_indices).float().to(device))\n",
    "\n",
    "        topk_preds = torch.zeros_like(scores).float()\n",
    "        topk_preds.scatter_(dim=1, index=test_indices[:, :k], src=torch.ones_like(test_indices).float())\n",
    "        \n",
    "        TP = (test_items * topk_preds).sum(1)                      \n",
    "        rec = TP/test_items.sum(1)\n",
    "   \n",
    "        ndcg = compute_ndcg_k(pred_items, test_items, test_indices, k)\n",
    "\n",
    "        recall_k.append(rec)\n",
    "        ndcg_k.append(ndcg)\n",
    "\n",
    "    return torch.cat(ndcg_k).mean(), torch.cat(recall_k).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_graph_data_set = MakeGraphDataSet(config = config)\n",
    "ngcf_adj_matrix = make_graph_data_set.get_ngcf_adj_matrix_data()\n",
    "R_train, R_valid, R_total = make_graph_data_set.get_R_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing weights...\n",
      "Weights initialized.\n"
     ]
    }
   ],
   "source": [
    "model = NGCF(\n",
    "    n_users = make_graph_data_set.num_user,\n",
    "    n_items = make_graph_data_set.num_item,\n",
    "    emb_dim = config.emb_dim,\n",
    "    layers = config.layers,\n",
    "    reg = config.reg,\n",
    "    node_dropout = config.node_dropout,\n",
    "    mess_dropout = config.mess_dropout,\n",
    "    adj_mtx = ngcf_adj_matrix,\n",
    "    ).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   1| Train loss: 0.71478| NDCG@10: 0.00440| HIT@10: 0.00093: 100%|██████████| 1/1 [03:11<00:00, 191.30s/it]\n",
      "Epoch:   2| Train loss: 0.69208| NDCG@10: 0.01050| HIT@10: 0.00234: 100%|██████████| 1/1 [03:12<00:00, 192.95s/it]\n",
      "Epoch:   3| Train loss: 0.68492| NDCG@10: 0.01687| HIT@10: 0.00381: 100%|██████████| 1/1 [03:11<00:00, 192.00s/it]\n",
      "Epoch:   4| Train loss: 0.67697| NDCG@10: 0.02030| HIT@10: 0.00449: 100%|██████████| 1/1 [03:11<00:00, 191.95s/it]\n",
      "Epoch:   5| Train loss: 0.62087| NDCG@10: 0.02803| HIT@10: 0.00642: 100%|██████████| 1/1 [03:10<00:00, 190.85s/it]\n",
      "Epoch:   6| Train loss: 0.51995| NDCG@10: 0.03267| HIT@10: 0.00740: 100%|██████████| 1/1 [03:09<00:00, 189.69s/it]\n",
      "Epoch:   7| Train loss: 0.49010| NDCG@10: 0.03495| HIT@10: 0.00785: 100%|██████████| 1/1 [03:12<00:00, 192.71s/it]\n",
      "Epoch:   8| Train loss: 0.47771| NDCG@10: 0.04032| HIT@10: 0.00904: 100%|██████████| 1/1 [03:12<00:00, 192.35s/it]\n",
      "Epoch:   9| Train loss: 0.47207| NDCG@10: 0.04869| HIT@10: 0.01105: 100%|██████████| 1/1 [03:12<00:00, 192.60s/it]\n",
      "Epoch:  10| Train loss: 0.46729| NDCG@10: 0.04227| HIT@10: 0.00970: 100%|██████████| 1/1 [03:08<00:00, 188.64s/it]\n",
      "Epoch:  11| Train loss: 0.46221| NDCG@10: 0.04642| HIT@10: 0.01054: 100%|██████████| 1/1 [03:09<00:00, 189.42s/it]\n",
      "Epoch:  12| Train loss: 0.45805| NDCG@10: 0.05219| HIT@10: 0.01192: 100%|██████████| 1/1 [03:11<00:00, 191.62s/it]\n",
      "Epoch:  13| Train loss: 0.45512| NDCG@10: 0.03641| HIT@10: 0.00845: 100%|██████████| 1/1 [03:09<00:00, 189.67s/it]\n",
      "Epoch:  14| Train loss: 0.45052| NDCG@10: 0.04267| HIT@10: 0.00991: 100%|██████████| 1/1 [03:10<00:00, 190.13s/it]\n",
      "Epoch:  15| Train loss: 0.44485| NDCG@10: 0.04894| HIT@10: 0.01137: 100%|██████████| 1/1 [03:09<00:00, 189.51s/it]\n",
      "Epoch:  16| Train loss: 0.43992| NDCG@10: 0.05674| HIT@10: 0.01287: 100%|██████████| 1/1 [03:12<00:00, 192.47s/it]\n",
      "Epoch:  17| Train loss: 0.43133| NDCG@10: 0.06884| HIT@10: 0.01587: 100%|██████████| 1/1 [03:10<00:00, 190.51s/it]\n",
      "Epoch:  18| Train loss: 0.41281| NDCG@10: 0.07942| HIT@10: 0.01836: 100%|██████████| 1/1 [03:11<00:00, 191.57s/it]\n",
      "Epoch:  19| Train loss: 0.39955| NDCG@10: 0.08121| HIT@10: 0.01882: 100%|██████████| 1/1 [03:11<00:00, 191.62s/it]\n",
      "Epoch:  20| Train loss: 0.39348| NDCG@10: 0.08293| HIT@10: 0.01955: 100%|██████████| 1/1 [03:11<00:00, 191.57s/it]\n",
      "Epoch:  21| Train loss: 0.38870| NDCG@10: 0.09752| HIT@10: 0.02268: 100%|██████████| 1/1 [03:11<00:00, 192.00s/it]\n",
      "Epoch:  22| Train loss: 0.38452| NDCG@10: 0.08017| HIT@10: 0.01882: 100%|██████████| 1/1 [03:09<00:00, 189.95s/it]\n",
      "Epoch:  23| Train loss: 0.38057| NDCG@10: 0.10778| HIT@10: 0.02632: 100%|██████████| 1/1 [03:10<00:00, 190.45s/it]\n",
      "Epoch:  24| Train loss: 0.37645| NDCG@10: 0.10301| HIT@10: 0.02486: 100%|██████████| 1/1 [03:08<00:00, 188.79s/it]\n",
      "Epoch:  25| Train loss: 0.37414| NDCG@10: 0.15097| HIT@10: 0.03728: 100%|██████████| 1/1 [03:11<00:00, 191.23s/it]\n",
      "Epoch:  26| Train loss: 0.37016| NDCG@10: 0.13321| HIT@10: 0.03376: 100%|██████████| 1/1 [03:08<00:00, 188.33s/it]\n",
      "Epoch:  27| Train loss: 0.36819| NDCG@10: 0.14423| HIT@10: 0.03754: 100%|██████████| 1/1 [03:11<00:00, 191.36s/it]\n",
      "Epoch:  28| Train loss: 0.36562| NDCG@10: 0.16245| HIT@10: 0.04175: 100%|██████████| 1/1 [03:10<00:00, 190.80s/it]\n",
      "Epoch:  29| Train loss: 0.36273| NDCG@10: 0.16760| HIT@10: 0.04472: 100%|██████████| 1/1 [03:11<00:00, 191.20s/it]\n",
      "Epoch:  30| Train loss: 0.36049| NDCG@10: 0.17515| HIT@10: 0.04572: 100%|██████████| 1/1 [03:11<00:00, 191.48s/it]\n",
      "Epoch:  31| Train loss: 0.35784| NDCG@10: 0.18736| HIT@10: 0.05031: 100%|██████████| 1/1 [03:11<00:00, 191.82s/it]\n",
      "Epoch:  32| Train loss: 0.35471| NDCG@10: 0.18383| HIT@10: 0.04919: 100%|██████████| 1/1 [03:10<00:00, 190.34s/it]\n",
      "Epoch:  33| Train loss: 0.35317| NDCG@10: 0.19398| HIT@10: 0.05257: 100%|██████████| 1/1 [03:10<00:00, 190.78s/it]\n",
      "Epoch:  34| Train loss: 0.35102| NDCG@10: 0.19721| HIT@10: 0.05356: 100%|██████████| 1/1 [03:12<00:00, 192.39s/it]\n",
      "Epoch:  35| Train loss: 0.34800| NDCG@10: 0.21801| HIT@10: 0.06023: 100%|██████████| 1/1 [03:11<00:00, 191.69s/it]\n",
      "Epoch:  36| Train loss: 0.34548| NDCG@10: 0.20902| HIT@10: 0.05692: 100%|██████████| 1/1 [03:09<00:00, 189.25s/it]\n",
      "Epoch:  37| Train loss: 0.34382| NDCG@10: 0.20885| HIT@10: 0.05883: 100%|██████████| 1/1 [03:09<00:00, 189.57s/it]\n",
      "Epoch:  38| Train loss: 0.34078| NDCG@10: 0.21918| HIT@10: 0.05975: 100%|██████████| 1/1 [03:09<00:00, 189.33s/it]\n",
      "Epoch:  39| Train loss: 0.33765| NDCG@10: 0.22726| HIT@10: 0.06327: 100%|██████████| 1/1 [03:11<00:00, 191.10s/it]\n",
      "Epoch:  40| Train loss: 0.33500| NDCG@10: 0.23527| HIT@10: 0.06483: 100%|██████████| 1/1 [03:11<00:00, 191.28s/it]\n",
      "Epoch:  41| Train loss: 0.33249| NDCG@10: 0.23729| HIT@10: 0.06484: 100%|██████████| 1/1 [03:12<00:00, 192.68s/it]\n",
      "Epoch:  42| Train loss: 0.33103| NDCG@10: 0.24932| HIT@10: 0.06783: 100%|██████████| 1/1 [03:11<00:00, 191.41s/it]\n",
      "Epoch:  43| Train loss: 0.32882| NDCG@10: 0.24583| HIT@10: 0.06680: 100%|██████████| 1/1 [03:10<00:00, 190.75s/it]\n",
      "Epoch:  44| Train loss: 0.32542| NDCG@10: 0.24051| HIT@10: 0.06725: 100%|██████████| 1/1 [03:10<00:00, 190.48s/it]\n",
      "Epoch:  45| Train loss: 0.32313| NDCG@10: 0.25097| HIT@10: 0.06803: 100%|██████████| 1/1 [03:11<00:00, 191.53s/it]\n",
      "Epoch:  46| Train loss: 0.31893| NDCG@10: 0.24085| HIT@10: 0.06794: 100%|██████████| 1/1 [03:08<00:00, 188.04s/it]\n",
      "Epoch:  47| Train loss: 0.31496| NDCG@10: 0.24480| HIT@10: 0.06904: 100%|██████████| 1/1 [03:09<00:00, 189.67s/it]\n",
      "Epoch:  48| Train loss: 0.31329| NDCG@10: 0.26454| HIT@10: 0.07297: 100%|██████████| 1/1 [03:11<00:00, 191.90s/it]\n",
      "Epoch:  49| Train loss: 0.31094| NDCG@10: 0.26029| HIT@10: 0.07303: 100%|██████████| 1/1 [03:10<00:00, 190.19s/it]\n",
      "Epoch:  50| Train loss: 0.30915| NDCG@10: 0.26914| HIT@10: 0.07489: 100%|██████████| 1/1 [03:09<00:00, 189.71s/it]\n"
     ]
    }
   ],
   "source": [
    "best_hit = 0\n",
    "for epoch in range(1, config.num_epochs + 1):\n",
    "    tbar = tqdm(range(1))\n",
    "    for _ in tbar:\n",
    "        train_loss = train(\n",
    "            model = model, \n",
    "            make_graph_data_set = make_graph_data_set, \n",
    "            optimizer = optimizer,\n",
    "            n_batch = config.n_batch,\n",
    "            )\n",
    "        with torch.no_grad():\n",
    "            ndcg, hit = evaluate(\n",
    "                u_emb = model.u_final_embeddings.detach(), \n",
    "                i_emb = model.i_final_embeddings.detach(), \n",
    "                Rtr = R_train, \n",
    "                Rte = R_valid, \n",
    "                k = 10,\n",
    "                )\n",
    "\n",
    "        if best_hit < hit:\n",
    "            best_hit = hit\n",
    "            torch.save(model.state_dict(), os.path.join(config.model_path, config.model_name))\n",
    "\n",
    "        tbar.set_description(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch:   1| Train loss: 0.71478| NDCG@10: 0.00440| HIT@10: 0.00093: 100%|██████████| 1/1 [03:11<00:00, 191.30s/it]\n",
    "Epoch:   2| Train loss: 0.69208| NDCG@10: 0.01050| HIT@10: 0.00234: 100%|██████████| 1/1 [03:12<00:00, 192.95s/it]\n",
    "Epoch:   3| Train loss: 0.68492| NDCG@10: 0.01687| HIT@10: 0.00381: 100%|██████████| 1/1 [03:11<00:00, 192.00s/it]\n",
    "Epoch:   4| Train loss: 0.67697| NDCG@10: 0.02030| HIT@10: 0.00449: 100%|██████████| 1/1 [03:11<00:00, 191.95s/it]\n",
    "Epoch:   5| Train loss: 0.62087| NDCG@10: 0.02803| HIT@10: 0.00642: 100%|██████████| 1/1 [03:10<00:00, 190.85s/it]\n",
    "Epoch:   6| Train loss: 0.51995| NDCG@10: 0.03267| HIT@10: 0.00740: 100%|██████████| 1/1 [03:09<00:00, 189.69s/it]\n",
    "Epoch:   7| Train loss: 0.49010| NDCG@10: 0.03495| HIT@10: 0.00785: 100%|██████████| 1/1 [03:12<00:00, 192.71s/it]\n",
    "Epoch:   8| Train loss: 0.47771| NDCG@10: 0.04032| HIT@10: 0.00904: 100%|██████████| 1/1 [03:12<00:00, 192.35s/it]\n",
    "Epoch:   9| Train loss: 0.47207| NDCG@10: 0.04869| HIT@10: 0.01105: 100%|██████████| 1/1 [03:12<00:00, 192.60s/it]\n",
    "Epoch:  10| Train loss: 0.46729| NDCG@10: 0.04227| HIT@10: 0.00970: 100%|██████████| 1/1 [03:08<00:00, 188.64s/it]\n",
    "Epoch:  11| Train loss: 0.46221| NDCG@10: 0.04642| HIT@10: 0.01054: 100%|██████████| 1/1 [03:09<00:00, 189.42s/it]\n",
    "Epoch:  12| Train loss: 0.45805| NDCG@10: 0.05219| HIT@10: 0.01192: 100%|██████████| 1/1 [03:11<00:00, 191.62s/it]\n",
    "Epoch:  13| Train loss: 0.45512| NDCG@10: 0.03641| HIT@10: 0.00845: 100%|██████████| 1/1 [03:09<00:00, 189.67s/it]\n",
    "Epoch:  14| Train loss: 0.45052| NDCG@10: 0.04267| HIT@10: 0.00991: 100%|██████████| 1/1 [03:10<00:00, 190.13s/it]\n",
    "Epoch:  15| Train loss: 0.44485| NDCG@10: 0.04894| HIT@10: 0.01137: 100%|██████████| 1/1 [03:09<00:00, 189.51s/it]\n",
    "Epoch:  16| Train loss: 0.43992| NDCG@10: 0.05674| HIT@10: 0.01287: 100%|██████████| 1/1 [03:12<00:00, 192.47s/it]\n",
    "Epoch:  17| Train loss: 0.43133| NDCG@10: 0.06884| HIT@10: 0.01587: 100%|██████████| 1/1 [03:10<00:00, 190.51s/it]\n",
    "Epoch:  18| Train loss: 0.41281| NDCG@10: 0.07942| HIT@10: 0.01836: 100%|██████████| 1/1 [03:11<00:00, 191.57s/it]\n",
    "Epoch:  19| Train loss: 0.39955| NDCG@10: 0.08121| HIT@10: 0.01882: 100%|██████████| 1/1 [03:11<00:00, 191.62s/it]\n",
    "Epoch:  20| Train loss: 0.39348| NDCG@10: 0.08293| HIT@10: 0.01955: 100%|██████████| 1/1 [03:11<00:00, 191.57s/it]\n",
    "Epoch:  21| Train loss: 0.38870| NDCG@10: 0.09752| HIT@10: 0.02268: 100%|██████████| 1/1 [03:11<00:00, 192.00s/it]\n",
    "Epoch:  22| Train loss: 0.38452| NDCG@10: 0.08017| HIT@10: 0.01882: 100%|██████████| 1/1 [03:09<00:00, 189.95s/it]\n",
    "Epoch:  23| Train loss: 0.38057| NDCG@10: 0.10778| HIT@10: 0.02632: 100%|██████████| 1/1 [03:10<00:00, 190.45s/it]\n",
    "Epoch:  24| Train loss: 0.37645| NDCG@10: 0.10301| HIT@10: 0.02486: 100%|██████████| 1/1 [03:08<00:00, 188.79s/it]\n",
    "Epoch:  25| Train loss: 0.37414| NDCG@10: 0.15097| HIT@10: 0.03728: 100%|██████████| 1/1 [03:11<00:00, 191.23s/it]\n",
    "Epoch:  26| Train loss: 0.37016| NDCG@10: 0.13321| HIT@10: 0.03376: 100%|██████████| 1/1 [03:08<00:00, 188.33s/it]\n",
    "Epoch:  27| Train loss: 0.36819| NDCG@10: 0.14423| HIT@10: 0.03754: 100%|██████████| 1/1 [03:11<00:00, 191.36s/it]\n",
    "Epoch:  28| Train loss: 0.36562| NDCG@10: 0.16245| HIT@10: 0.04175: 100%|██████████| 1/1 [03:10<00:00, 190.80s/it]\n",
    "Epoch:  29| Train loss: 0.36273| NDCG@10: 0.16760| HIT@10: 0.04472: 100%|██████████| 1/1 [03:11<00:00, 191.20s/it]\n",
    "Epoch:  30| Train loss: 0.36049| NDCG@10: 0.17515| HIT@10: 0.04572: 100%|██████████| 1/1 [03:11<00:00, 191.48s/it]\n",
    "Epoch:  31| Train loss: 0.35784| NDCG@10: 0.18736| HIT@10: 0.05031: 100%|██████████| 1/1 [03:11<00:00, 191.82s/it]\n",
    "Epoch:  32| Train loss: 0.35471| NDCG@10: 0.18383| HIT@10: 0.04919: 100%|██████████| 1/1 [03:10<00:00, 190.34s/it]\n",
    "Epoch:  33| Train loss: 0.35317| NDCG@10: 0.19398| HIT@10: 0.05257: 100%|██████████| 1/1 [03:10<00:00, 190.78s/it]\n",
    "Epoch:  34| Train loss: 0.35102| NDCG@10: 0.19721| HIT@10: 0.05356: 100%|██████████| 1/1 [03:12<00:00, 192.39s/it]\n",
    "Epoch:  35| Train loss: 0.34800| NDCG@10: 0.21801| HIT@10: 0.06023: 100%|██████████| 1/1 [03:11<00:00, 191.69s/it]\n",
    "Epoch:  36| Train loss: 0.34548| NDCG@10: 0.20902| HIT@10: 0.05692: 100%|██████████| 1/1 [03:09<00:00, 189.25s/it]\n",
    "Epoch:  37| Train loss: 0.34382| NDCG@10: 0.20885| HIT@10: 0.05883: 100%|██████████| 1/1 [03:09<00:00, 189.57s/it]\n",
    "Epoch:  38| Train loss: 0.34078| NDCG@10: 0.21918| HIT@10: 0.05975: 100%|██████████| 1/1 [03:09<00:00, 189.33s/it]\n",
    "Epoch:  39| Train loss: 0.33765| NDCG@10: 0.22726| HIT@10: 0.06327: 100%|██████████| 1/1 [03:11<00:00, 191.10s/it]\n",
    "Epoch:  40| Train loss: 0.33500| NDCG@10: 0.23527| HIT@10: 0.06483: 100%|██████████| 1/1 [03:11<00:00, 191.28s/it]\n",
    "Epoch:  41| Train loss: 0.33249| NDCG@10: 0.23729| HIT@10: 0.06484: 100%|██████████| 1/1 [03:12<00:00, 192.68s/it]\n",
    "Epoch:  42| Train loss: 0.33103| NDCG@10: 0.24932| HIT@10: 0.06783: 100%|██████████| 1/1 [03:11<00:00, 191.41s/it]\n",
    "Epoch:  43| Train loss: 0.32882| NDCG@10: 0.24583| HIT@10: 0.06680: 100%|██████████| 1/1 [03:10<00:00, 190.75s/it]\n",
    "Epoch:  44| Train loss: 0.32542| NDCG@10: 0.24051| HIT@10: 0.06725: 100%|██████████| 1/1 [03:10<00:00, 190.48s/it]\n",
    "Epoch:  45| Train loss: 0.32313| NDCG@10: 0.25097| HIT@10: 0.06803: 100%|██████████| 1/1 [03:11<00:00, 191.53s/it]\n",
    "Epoch:  46| Train loss: 0.31893| NDCG@10: 0.24085| HIT@10: 0.06794: 100%|██████████| 1/1 [03:08<00:00, 188.04s/it]\n",
    "Epoch:  47| Train loss: 0.31496| NDCG@10: 0.24480| HIT@10: 0.06904: 100%|██████████| 1/1 [03:09<00:00, 189.67s/it]\n",
    "Epoch:  48| Train loss: 0.31329| NDCG@10: 0.26454| HIT@10: 0.07297: 100%|██████████| 1/1 [03:11<00:00, 191.90s/it]\n",
    "Epoch:  49| Train loss: 0.31094| NDCG@10: 0.26029| HIT@10: 0.07303: 100%|██████████| 1/1 [03:10<00:00, 190.19s/it]\n",
    "Epoch:  50| Train loss: 0.30915| NDCG@10: 0.26914| HIT@10: 0.07489: 100%|██████████| 1/1 [03:09<00:00, 189.71s/it]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(config.model_path, config.model_name)))\n",
    "submision = []\n",
    "with torch.no_grad():\n",
    "    k = 10\n",
    "    u_emb = model.u_final_embeddings.detach()\n",
    "    i_emb = model.i_final_embeddings.detach()\n",
    "\n",
    "    scores = torch.mm(u_emb, i_emb.t())\n",
    "\n",
    "    test_items = torch.from_numpy(R_total.todense()).float().to(device)\n",
    "    non_train_items = torch.from_numpy(1-(R_total.todense())).float().to(device)\n",
    "    scores = scores * non_train_items\n",
    "\n",
    "    _, test_indices = torch.topk(scores, dim=1, k=k)\n",
    "\n",
    "    pred_items = torch.zeros_like(scores).float()\n",
    "    pred_items.scatter_(dim=1, index=test_indices, src=torch.ones_like(test_indices).float().to(device))\n",
    "    pred_items = pred_items.argsort(dim = 1)\n",
    "    for user, item_list in enumerate(pred_items):\n",
    "        item_list = item_list[-10:].cpu().numpy().tolist()\n",
    "        for item in item_list:\n",
    "            submision.append(\n",
    "                {   \n",
    "                    'user' : make_graph_data_set.user_decoder[user],\n",
    "                    'item' : make_graph_data_set.item_decoder[item],\n",
    "                }\n",
    "            )\n",
    "\n",
    "submision = pd.DataFrame(submision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "submision.to_csv(os.path.join(config.submission_path, config.submission_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>1704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>1193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>3147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>70286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313595</th>\n",
       "      <td>138493</td>\n",
       "      <td>608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313596</th>\n",
       "      <td>138493</td>\n",
       "      <td>8368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313597</th>\n",
       "      <td>138493</td>\n",
       "      <td>1704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313598</th>\n",
       "      <td>138493</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313599</th>\n",
       "      <td>138493</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>313600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user   item\n",
       "0           11   1265\n",
       "1           11   1704\n",
       "2           11   1193\n",
       "3           11   3147\n",
       "4           11  70286\n",
       "...        ...    ...\n",
       "313595  138493    608\n",
       "313596  138493   8368\n",
       "313597  138493   1704\n",
       "313598  138493     50\n",
       "313599  138493     47\n",
       "\n",
       "[313600 rows x 2 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submision"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
