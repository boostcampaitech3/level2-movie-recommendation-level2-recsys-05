{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SErkT_jPdGp6"
   },
   "source": [
    "# DeepFM\n",
    "\n",
    "이번 실습에서는 DeepFM 모델을 이해하고 구현해보겠습니다.  \n",
    "\n",
    "DeepFM 모델은 Factorization machines와 neural network를 합친 모델로, Wide & Deep model과 유사하지만, feature engineering이 필요하지 않다는 특징을 가지고 있습니다.  \n",
    "<br/>\n",
    "사용자가 영화에 대해 Rating한 데이터, 영화의 장르 데이터를 이용하여 Train/Test data를 생성한 다음, Train data로 학습한 모델을 Test data에 대해 평가해봅니다.   \n",
    "사용한 데이터는 Implicit feedback data로, 사용자가 시청한 영화(Positive instances)는 rating = 1로 기록됩니다. 따라서 시청하지 않은 영화에 대해 각 유저별로 Negative instances sampling을 진행합니다.   \n",
    "<br/>\n",
    "**구현에 앞서, DeepFM 논문을 꼭 읽어보시길 권장합니다.**\n",
    "\n",
    "* 참고  \n",
    "    - DeepFM: A Factorization-Machine based Neural Network for CTR Prediction (https://arxiv.org/pdf/1703.04247.pdf)  \n",
    "    - Wide & Deep Learning for Recommender Systems (https://arxiv.org/pdf/1606.07792.pdf)\n",
    "    - Factorization Machines (https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5694074)\n",
    "    - https://d2l.ai/chapter_recommender-systems/deepfm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvcgI1nQdGp8"
   },
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hJSpT7rFdGp8"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FeUz2WmydGp9"
   },
   "source": [
    "# Data preprocessing\n",
    "0. Dataset 다운로드  \n",
    "<br/>\n",
    "1. Rating df 생성  \n",
    "rating 데이터(train_ratings.csv)를 불러와 [user, item, rating]의 컬럼으로 구성된 데이터 프레임을 생성합니다.   \n",
    "<br/>\n",
    "2. Genre df 생성   \n",
    "genre 정보가 담긴 데이터(genres.tsv)를 불러와 genre이름을 id로 변경하고, [item, genre]의 컬럼으로 구성된 데이터 프레임을 생성합니다.    \n",
    "<br/>\n",
    "3. Negative instances 생성   \n",
    "rating 데이터는 implicit feedback data(rating :0/1)로, positive instances로 구성되어 있습니다. 따라서 rating이 없는 item중 negative instances를 뽑아서 데이터에 추가하게 됩니다.   \n",
    "<br/>\n",
    "4. Join dfs   \n",
    "rating df와 genre df를 join하여 [user, item, rating, genre]의 컬럼으로 구성된 데이터 프레임을 생성합니다.   \n",
    "<br/>\n",
    "5. zero-based index로 mapping   \n",
    "Embedding을 위해서 user,item,genre를 zero-based index로 mapping합니다.\n",
    "    - user : 0-31359\n",
    "    - item : 0-6806\n",
    "    - genre : 0-17  \n",
    "<br/>\n",
    "6. feature matrix X, label tensor y 생성   \n",
    "[user, item, genre] 3개의 field로 구성된 feature matrix를 생성합니다.   \n",
    "<br/>\n",
    "7. data loader 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABiFIo3BkUI-"
   },
   "source": [
    "## 데이터 다운로드\n",
    "이곳에 대회 사이트(AI Stages)에 있는 data의 URL을 입력해주세요. \n",
    "- 데이터 URL은 변경될 수 있습니다.\n",
    "- 예) `!wget https://aistages-prod-server-public.s3.amazonaws.com/app/Competitions/000176/data/data.tar.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXlzunb0dGp-"
   },
   "outputs": [],
   "source": [
    "# # 0. Dataset 다운로드\n",
    "# !wget <대회 데이터 URL>\n",
    "# !tar -xf data.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yB1Na9htdGp-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw rating df\n",
      "           user   item  rating\n",
      "0            11   4643     1.0\n",
      "1            11    170     1.0\n",
      "2            11    531     1.0\n",
      "3            11    616     1.0\n",
      "4            11   2140     1.0\n",
      "...         ...    ...     ...\n",
      "5154466  138493  44022     1.0\n",
      "5154467  138493   4958     1.0\n",
      "5154468  138493  68319     1.0\n",
      "5154469  138493  40819     1.0\n",
      "5154470  138493  27311     1.0\n",
      "\n",
      "[5154471 rows x 3 columns]\n",
      "Raw genre df - changed to id\n",
      "         item  genre\n",
      "0         318     13\n",
      "2        2571     15\n",
      "5        2959     15\n",
      "9         296      2\n",
      "13        356      2\n",
      "...       ...    ...\n",
      "15925   73106      2\n",
      "15926  109850     15\n",
      "15929    8605     15\n",
      "15931    3689      2\n",
      "15932    8130      7\n",
      "\n",
      "[6807 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1. Rating df 생성'\n",
    "rating_data = \"/opt/ml/input/data/train/train_ratings.csv\"\n",
    "\n",
    "raw_rating_df = pd.read_csv(rating_data)\n",
    "raw_rating_df\n",
    "raw_rating_df['rating'] = 1.0 # implicit feedback\n",
    "raw_rating_df.drop(['time'],axis=1,inplace=True)\n",
    "print(\"Raw rating df\")\n",
    "print(raw_rating_df)\n",
    "\n",
    "users = set(raw_rating_df.loc[:, 'user'])\n",
    "items = set(raw_rating_df.loc[:, 'item'])\n",
    "\n",
    "#2. Genre df 생성\n",
    "genre_data = \"/opt/ml/input/data/train/genres.tsv\"\n",
    "\n",
    "raw_genre_df = pd.read_csv(genre_data, sep='\\t')\n",
    "raw_genre_df = raw_genre_df.drop_duplicates(subset=['item']) #item별 하나의 장르만 남도록 drop\n",
    "# print(raw_genre_df)\n",
    "\n",
    "genre_dict = {genre:i for i, genre in enumerate(set(raw_genre_df['genre']))}\n",
    "raw_genre_df['genre']  = raw_genre_df['genre'].map(lambda x : genre_dict[x]) #genre id로 변경\n",
    "print(\"Raw genre df - changed to id\")\n",
    "print(raw_genre_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "stnVq1AFdGp-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Nagetive instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31360/31360 [05:06<00:00, 102.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "          user  item  rating  genre\n",
      "0            0  2505     1.0      7\n",
      "1            0  5414     1.0      7\n",
      "2            0   871     1.0      1\n",
      "3            0  1233     1.0      2\n",
      "4            0   947     1.0      0\n",
      "...        ...   ...     ...    ...\n",
      "6722466  31359  5572     1.0      7\n",
      "6722467  31359    92     1.0      0\n",
      "6722468  31359   854     1.0      7\n",
      "6722469  31359   555     1.0     10\n",
      "6722470  31359  4777     1.0      7\n",
      "\n",
      "[6722471 rows x 4 columns]\n",
      "# of data : 6722471\n",
      "# of users : 31360\n",
      "# of items : 6807\n",
      "# of genres : 18\n"
     ]
    }
   ],
   "source": [
    "# 3. Negative instance 생성\n",
    "print(\"Create Nagetive instances\")\n",
    "num_negative = 50\n",
    "user_group_dfs = list(raw_rating_df.groupby('user')['item'])\n",
    "first_row = True\n",
    "user_neg_dfs = pd.DataFrame()\n",
    "\n",
    "for u, u_items in tqdm(user_group_dfs):\n",
    "    u_items = set(u_items)\n",
    "    i_user_neg_item = np.random.choice(list(items - u_items), num_negative, replace=False)\n",
    "    \n",
    "    i_user_neg_df = pd.DataFrame({'user': [u]*num_negative, 'item': i_user_neg_item, 'rating': [0]*num_negative})\n",
    "    if first_row == True:\n",
    "        user_neg_dfs = i_user_neg_df\n",
    "        first_row = False\n",
    "    else:\n",
    "        user_neg_dfs = pd.concat([user_neg_dfs, i_user_neg_df], axis = 0, sort=False)\n",
    "\n",
    "raw_rating_df = pd.concat([raw_rating_df, user_neg_dfs], axis = 0, sort=False)\n",
    "\n",
    "# 4. Join dfs\n",
    "joined_rating_df = pd.merge(raw_rating_df, raw_genre_df, left_on='item', right_on='item', how='inner')\n",
    "# print(\"Joined rating df\")\n",
    "# print(joined_rating_df)\n",
    "\n",
    "# 5. user, item을 zero-based index로 mapping\n",
    "users = list(set(joined_rating_df.loc[:,'user']))\n",
    "users.sort()\n",
    "items =  list(set((joined_rating_df.loc[:, 'item'])))\n",
    "items.sort()\n",
    "genres =  list(set((joined_rating_df.loc[:, 'genre'])))\n",
    "genres.sort()\n",
    "\n",
    "if len(users)-1 != max(users):\n",
    "    users_dict = {users[i]: i for i in range(len(users))}\n",
    "    joined_rating_df['user']  = joined_rating_df['user'].map(lambda x : users_dict[x])\n",
    "    users = list(set(joined_rating_df.loc[:,'user']))\n",
    "    \n",
    "if len(items)-1 != max(items):\n",
    "    items_dict = {items[i]: i for i in range(len(items))}\n",
    "    joined_rating_df['item']  = joined_rating_df['item'].map(lambda x : items_dict[x])\n",
    "    items =  list(set((joined_rating_df.loc[:, 'item'])))\n",
    "\n",
    "joined_rating_df = joined_rating_df.sort_values(by=['user'])\n",
    "joined_rating_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data = joined_rating_df\n",
    "print(\"Data\")\n",
    "print(data)\n",
    "\n",
    "n_data = len(data)\n",
    "n_user = len(users)\n",
    "n_item = len(items)\n",
    "n_genre = len(genres)\n",
    "\n",
    "print(\"# of data : {}\\n# of users : {}\\n# of items : {}\\n# of genres : {}\".format(n_data, n_user, n_item, n_genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vx20eHxGdGp_"
   },
   "outputs": [],
   "source": [
    "#6. feature matrix X, label tensor y 생성\n",
    "user_col = torch.tensor(data.loc[:,'user'])\n",
    "item_col = torch.tensor(data.loc[:,'item'])\n",
    "genre_col = torch.tensor(data.loc[:,'genre'])\n",
    "\n",
    "offsets = [0, n_user, n_user+n_item]\n",
    "for col, offset in zip([user_col, item_col, genre_col], offsets):\n",
    "    col += offset\n",
    "\n",
    "X = torch.cat([user_col.unsqueeze(1), item_col.unsqueeze(1), genre_col.unsqueeze(1)], dim=1)\n",
    "y = torch.tensor(list(data.loc[:,'rating']))\n",
    "\n",
    "#7. data loader 생성\n",
    "class RatingDataset(Dataset):\n",
    "    def __init__(self, input_tensor, target_tensor):\n",
    "        self.input_tensor = input_tensor.long()\n",
    "        self.target_tensor = target_tensor.long()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_tensor[index], self.target_tensor[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.target_tensor.size(0)\n",
    "\n",
    "\n",
    "dataset = RatingDataset(X, y)\n",
    "train_ratio = 0.9\n",
    "\n",
    "train_size = int(train_ratio * len(data))\n",
    "test_size = len(data) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfb7KEEMdGqA"
   },
   "source": [
    "   # Model architecture (DeepFM)\n",
    "   DeepFM 모델은 1) FM component와  2) Deep component가 병렬적으로 결합되어 있습니다. 구조는 다음과 같습니다.\n",
    "<img src='https://drive.google.com/uc?id=1vwcxUJQTIsg5QH9CuH5PcUEfExhToUHR'>  \n",
    "각 구조는 다음과 같습니다.  \n",
    "   **1. FM component**  \n",
    "       FM component는 우리가 아는 2-way Factorization machines(degree=2)입니다. FM은 variables 간의 interaction을 다음과 같이 모델링 합니다.   \n",
    "     **<center> equation (1) </center>**\n",
    "   $$\\hat{y}(x):=w_0 + \\sum_{i=1}^{n}w_ix_i + \\sum_{i=1}^{n}\\sum_{j=i+1}^{n}<\\mathbf{v}_i,\\mathbf{v}_j>x_ix_j$$   \n",
    "   이때, 세번째 interaction term을 전개하여 다음과 같이 쓸 수 있습니다.(논문 참고)  \n",
    "   구현 코드는 전개된 식을 바탕으로 합니다.   \n",
    "     **<center> equation (2)> </center>**\n",
    "   $$\\sum_{i=1}^{n}\\sum_{j=i+1}^{n}<\\mathbf{v}_i,\\mathbf{v}_j>x_ix_j = \\frac{1}{2}\\sum_{f=1}^{k}((\\sum_{i=1}^{n}v_{i,f}x_i)^2-\\sum_{i=1}^{n}v_{i,f}^2x_i^2)$$   \n",
    "           \n",
    "   **2. Deep component**  \n",
    "       Deep component는 MLP Layers로 구성되어 있습니다.   \n",
    "       구현 코드는 Input dimension이 30-20-10인 3 layer MLP 구조입니다.\n",
    "  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "oXlycCBkdGqA"
   },
   "outputs": [],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    def __init__(self, input_dims, embedding_dim, mlp_dims, drop_rate=0.1):\n",
    "        super(DeepFM, self).__init__()\n",
    "        total_input_dim = int(sum(input_dims)) # n_user + n_movie + n_genre\n",
    "\n",
    "        # Fm component의 constant bias term과 1차 bias term\n",
    "        self.bias = nn.Parameter(torch.zeros((1,)))\n",
    "        self.fc = nn.Embedding(total_input_dim, 1)\n",
    "        \n",
    "        self.embedding = nn.Embedding(total_input_dim, embedding_dim) \n",
    "        self.embedding_dim = len(input_dims) * embedding_dim\n",
    "\n",
    "        mlp_layers = []\n",
    "        for i, dim in enumerate(mlp_dims):\n",
    "            if i==0:\n",
    "                mlp_layers.append(nn.Linear(self.embedding_dim, dim))\n",
    "            else:\n",
    "                 mlp_layers.append(nn.Linear(mlp_dims[i-1], dim))      #TODO 1 : linear layer를 넣어주세요.\n",
    "            mlp_layers.append(nn.ReLU(True))                   \n",
    "            mlp_layers.append(nn.Dropout(drop_rate))\n",
    "            \n",
    "        mlp_layers.append(nn.Linear(mlp_dims[-1], 1)) \n",
    "        self.mlp_layers = nn.Sequential(*mlp_layers)\n",
    "\n",
    "    def fm(self, x):\n",
    "        # x : (batch_size, total_num_input)\n",
    "        embed_x = self.embedding(x) # batch, 3, embedding_dim\n",
    "\n",
    "        fm_y = self.bias + torch.sum(self.fc(x), dim=1) # batch, 1\n",
    "        square_of_sum = torch.sum(embed_x, dim=1) ** 2  # batch, embedding_dim\n",
    "        sum_of_square = torch.sum(embed_x ** 2, dim=1)  # batch, embedding_dim\n",
    "        fm_y += 0.5 * torch.sum(square_of_sum - sum_of_square, dim=1, keepdim=True) # batch, 1\n",
    "        return fm_y\n",
    "    \n",
    "    def mlp(self, x):\n",
    "        embed_x = self.embedding(x) # batch, 3, embedding_dim\n",
    "        \n",
    "        inputs = embed_x.view(-1, self.embedding_dim) # batch, 3 * embedding_dim\n",
    "        mlp_y = self.mlp_layers(inputs) # batch, 1\n",
    "        return mlp_y\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed_x = self.embedding(x)\n",
    "        #fm component\n",
    "        fm_y = self.fm(x).squeeze(1)\n",
    "        \n",
    "        #deep component\n",
    "        mlp_y = self.mlp(x).squeeze(1)\n",
    "        \n",
    "        y = torch.sigmoid(fm_y + mlp_y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mAwfPocdGqB"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "RgBJPVuadGqB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [11:29<00:00, 68.91s/it]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "input_dims = [n_user, n_item, n_genre]\n",
    "embedding_dim = 10\n",
    "model = DeepFM(input_dims, embedding_dim, mlp_dims=[30, 20, 10]).to(device)\n",
    "bce_loss = nn.BCELoss() # Binary Cross Entropy loss\n",
    "lr, num_epochs = 0.01, 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for e in tqdm(range(num_epochs)) :\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = bce_loss(output, y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIKWIOZ5dGqB"
   },
   "source": [
    "# Evaluation\n",
    "평가는 모델이 postive instance에 대해 0.5이상, negative instance에 대해 0.5미만의 값을 예측한 Accuracy를 측정하여 진행됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "igRcm9_6dGqC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Acc : 90.39%\n"
     ]
    }
   ],
   "source": [
    "correct_result_sum = 0\n",
    "for x, y in test_loader:\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    model.eval()\n",
    "    output = model(x)\n",
    "    result = torch.round(output)\n",
    "    correct_result_sum += (result == y).sum().float()\n",
    "\n",
    "acc = correct_result_sum/len(test_dataset)*100\n",
    "print(\"Final Acc : {:.2f}%\".format(acc.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Mission3 _Factorization machines with PyTorch (DeepFM)-문제.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
