{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Ug-br-pPu9vZ"},"outputs":[],"source":["import math\n","import numpy as np\n","import scipy.sparse as sp\n","import pandas as pd\n","from tqdm import tqdm\n","from collections import defaultdict\n","import os\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","from box import Box\n","\n","import warnings\n","\n","warnings.filterwarnings(action='ignore')\n","torch.set_printoptions(sci_mode=True)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["class GMF(nn.Module):\n","    def __init__(self, num_user, num_item, num_factor):\n","        super(GMF, self).__init__()\n","        self.user_emb = nn.Embedding(num_user, num_factor)\n","        self.item_emb = nn.Embedding(num_item, num_factor)\n","        \n","        self.predict_layer = nn.Sequential(\n","            nn.Linear(num_factor, 1, bias = False)\n","        )\n","\n","        self._init_weight_()\n","    \n","    def _init_weight_(self):\n","        nn.init.normal_(self.user_emb.weight, std=0.01)\n","        nn.init.normal_(self.item_emb.weight, std=0.01)\n","        for m in self.predict_layer:\n","            if isinstance(m, nn.Linear):\n","                nn.init.kaiming_uniform_(m.weight, a=1)\n","    \n","    def forward(self, user, item):\n","        user_emb = self.user_emb(user)\n","        item_emb = self.item_emb(item)\n","\n","        output = self.predict_layer(user_emb * item_emb)\n","\n","        return output.view(-1)"]},{"cell_type":"markdown","metadata":{"id":"pbRKDSg4u9vc"},"source":["# 1. 학습 설정"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"MEhK_fLIu9vd"},"outputs":[],"source":["config = {\n","    'data_path' : \"/opt/ml/input/data/train\" , # 데이터 경로\n","    'model_path' : \"../model\",\n","    \n","    'submission_path' : \"../submission\",\n","    'submission_name' : 'EASE_v6_submission.csv',\n","\n","    'candidate_item_num' : 5,\n","    'valid_samples' : 10, # 검증에 사용할 sample 수\n","    'seed' : 22,\n","    'reg' : 750,\n","}\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","config = Box(config)"]},{"cell_type":"markdown","metadata":{"id":"wjDxy0fJu9vf"},"source":["# 2. 데이터 전처리"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"W64BYWl0u9vg"},"outputs":[],"source":["class MakeMatrixDataSet():\n","    \"\"\"\n","    MatrixDataSet 생성\n","    \"\"\"\n","    def __init__(self, config):\n","        self.config = config\n","        self.df = pd.read_csv(os.path.join(self.config.data_path, 'train_ratings.csv'))\n","        \n","        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('item')\n","        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('user')\n","        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n","\n","        self.df['item_idx'] = self.df['item'].apply(lambda x : self.item_encoder[x])\n","        self.df['user_idx'] = self.df['user'].apply(lambda x : self.user_encoder[x])\n","\n","        self.user_train, self.user_valid = self.generate_sequence_data()\n","\n","    def generate_encoder_decoder(self, col : str) -> dict:\n","        \"\"\"\n","        encoder, decoder 생성\n","\n","        Args:\n","            col (str): 생성할 columns 명\n","        Returns:\n","            dict: 생성된 user encoder, decoder\n","        \"\"\"\n","\n","        encoder = {}\n","        decoder = {}\n","        ids = self.df[col].unique()\n","\n","        for idx, _id in enumerate(ids):\n","            encoder[_id] = idx\n","            decoder[idx] = _id\n","\n","        return encoder, decoder\n","    \n","    def generate_sequence_data(self) -> dict:\n","        \"\"\"\n","        sequence_data 생성\n","\n","        Returns:\n","            dict: train user sequence / valid user sequence\n","        \"\"\"\n","        users = defaultdict(list)\n","        user_train = {}\n","        user_valid = {}\n","        for user, item, time in zip(self.df['user_idx'], self.df['item_idx'], self.df['time']):\n","            users[user].append(item)\n","        \n","        for user in users:\n","            np.random.seed(self.config.seed)\n","\n","            user_total = users[user]\n","            valid = np.random.choice(user_total, size = self.config.valid_samples, replace = False).tolist()\n","            train = list(set(user_total) - set(valid))\n","\n","            user_train[user] = train\n","            user_valid[user] = valid # valid_samples 개수 만큼 검증에 활용 (현재 Task와 가장 유사하게)\n","\n","        return user_train, user_valid\n","    \n","    def get_train_valid_data(self):\n","        return self.user_train, self.user_valid\n","\n","    def make_matrix(self, user_list, train = True):\n","        \"\"\"\n","        user_item_dict를 바탕으로 행렬 생성\n","        \"\"\"\n","        mat = torch.zeros(size = (user_list.size(0), self.num_item))\n","        for idx, user in enumerate(user_list):\n","            if train:\n","                mat[idx, self.user_train[user.item()]] = 1\n","            else:\n","                mat[idx, self.user_train[user.item()] + self.user_valid[user.item()]] = 1\n","        return mat\n","\n","    def make_sparse_matrix(self, test = False):\n","        X = sp.dok_matrix((self.num_user, self.num_item), dtype=np.float32)\n","        \n","        for user in self.user_train.keys():\n","            item_list = self.user_train[user]\n","            X[user, item_list] = 1.0\n","        \n","        if test:\n","            for user in self.user_valid.keys():\n","                item_list = self.user_valid[user]\n","                X[user, item_list] = 1.0\n","\n","        return X.tocsr()\n","    \n","\n","    def make_year_candidate_item(self, train = True):\n","        user_pro_df = pd.read_csv(os.path.join(self.config.data_path, 'user_pro.csv'))\n","        item_pro_df = pd.read_csv(os.path.join(self.config.data_path, 'item_pro.csv'))\n","\n","        item_pro_df['item'] = item_pro_df['item'].apply(lambda x : self.item_encoder[x])\n","        item_pro_df['year'] = item_pro_df['year'].astype(int)\n","\n","        user_pro_df['user'] = user_pro_df['user'].apply(lambda x : self.user_encoder[x])\n","        user_pro_df['max_year'] = user_pro_df['max_year'].astype(int)\n","        \n","        year2item_list = {}\n","        year_list = user_pro_df['max_year'].unique().tolist()\n","        for year in year_list:\n","            item_list = item_pro_df[item_pro_df['year'] <= year + 1]['item'].tolist()\n","            year2item_list[year + 1] = item_list\n","\n","        all_item_list = [i for i in range(self.num_item)]\n","        group_df = user_pro_df.groupby('user')\n","        candidate = {}\n","        if train:\n","            for user, df in group_df:\n","                max_year = df['max_year'].values[0]\n","                candidate_item_list = year2item_list[max_year + 1]\n","                candidate_item_list = set(all_item_list) - set(candidate_item_list)\n","                candidate_item_list = list(candidate_item_list | set(self.user_train[user]))\n","                candidate[user] = candidate_item_list\n","        else:\n","            for user, df in group_df:\n","                max_year = df['max_year'].values[0]\n","                candidate_item_list = year2item_list[max_year + 1]\n","                candidate_item_list = set(all_item_list) - set(candidate_item_list)\n","                candidate_item_list = candidate_item_list | set(self.user_train[user])\n","                candidate_item_list = list(candidate_item_list | set(self.user_valid[user]))\n","                candidate[user] = candidate_item_list\n","\n","        return candidate\n","\n","    def make_cos_candidate_item(self, candidate_item_num, train = True):\n","        gmf = GMF(\n","            num_user = self.num_user, \n","            num_item = self.num_item, \n","            num_factor = 512).to(device)\n","\n","        gmf.load_state_dict(torch.load(os.path.join(self.config.model_path, 'GMF_v1.pt')))\n","        movie_emb = gmf.item_emb.weight.data.cpu()\n","        \n","        cos_mm = torch.nn.CosineSimilarity(dim=1)\n","        cos_sim_list = []\n","        for target_item in range(len(movie_emb)):\n","            cos_sim_score = cos_mm(movie_emb[target_item], movie_emb)\n","            cos_sim_index = cos_sim_score.argsort()\n","            cos_sim_list.append(cos_sim_index.numpy()[::-1][:candidate_item_num + 1].tolist())\n","        \n","        cos_sim_list = np.array(cos_sim_list)\n","\n","        candidate = {}\n","        if train:\n","            for user in self.user_train.keys():\n","                candidate_item_list = set(cos_sim_list[self.user_train[user], :].reshape(-1).tolist())\n","                candidate_item_list = list(candidate_item_list - set(self.user_train[user]))\n","                candidate[user] = candidate_item_list\n","        else:\n","            for user in self.user_train.keys():\n","                candidate_item_list = set(cos_sim_list[self.user_train[user] + self.user_valid[user], :].reshape(-1).tolist())\n","                candidate_item_list = candidate_item_list - set(self.user_train[user])\n","                candidate_item_list = list(candidate_item_list - set(self.user_valid[user]))\n","                candidate[user] = candidate_item_list\n","        \n","        return candidate\n","\n","    def m_s_m(self, candidate):\n","        X = sp.dok_matrix((self.num_user, self.num_item), dtype=np.float32)\n","        for user in candidate.keys():\n","            item_list = candidate[user]\n","            X[user, item_list] = 1.0\n","\n","        return X.tocsr()"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"IldCGmY8u9vh"},"outputs":[],"source":["class AEDataSet(Dataset):\n","    def __init__(self, num_user):\n","        self.num_user = num_user\n","        self.users = [i for i in range(num_user)]\n","\n","    def __len__(self):\n","        return self.num_user\n","\n","    def __getitem__(self, idx): \n","        user = self.users[idx]\n","        return torch.LongTensor([user])"]},{"cell_type":"markdown","metadata":{"id":"ysia457Su9vi"},"source":["# 3. 모델"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"TVXWUTLpSh8_"},"outputs":[],"source":["class EASE():\n","    def __init__(self, X, reg):\n","        self.X = self._convert_sp_mat_to_sp_tensor(X)\n","        self.reg = reg\n","    \n","    def _convert_sp_mat_to_sp_tensor(self, X):\n","        \"\"\"\n","        Convert scipy sparse matrix to PyTorch sparse matrix\n","\n","        Arguments:\n","        ----------\n","        X = Adjacency matrix, scipy sparse matrix\n","        \"\"\"\n","        coo = X.tocoo().astype(np.float32)\n","        i = torch.LongTensor(np.mat([coo.row, coo.col]))\n","        v = torch.FloatTensor(coo.data)\n","        res = torch.sparse.FloatTensor(i, v, coo.shape).to(device)\n","        return res\n","    \n","    def fit(self):\n","        '''\n","\n","        진짜 정말 간단한 식으로 모델을 만듬\n","\n","        '''\n","        G = self.X.to_dense().t() @ self.X.to_dense()\n","        diagIndices = torch.eye(G.shape[0]) == 1\n","        G[diagIndices] += self.reg\n","\n","        P = G.inverse()\n","        B = P / (-1 * P.diag())\n","        B[diagIndices] = 0\n","\n","        self.pred = self.X.to_dense() @ B"]},{"cell_type":"markdown","metadata":{"id":"GwSexh43u9vk"},"source":["# 4. 학습 함수"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"nws4JO2_rgQP"},"outputs":[],"source":["def get_ndcg(pred_list, true_list):\n","    idcg = sum((1 / np.log2(rank + 2) for rank in range(1, len(pred_list))))\n","    dcg = 0\n","    for rank, pred in enumerate(pred_list):\n","        if pred in true_list:\n","            dcg += 1 / np.log2(rank + 2)\n","    ndcg = dcg / idcg\n","    return ndcg\n","\n","# hit == recall == precision\n","def get_hit(pred_list, true_list):\n","    hit_list = set(true_list) & set(pred_list)\n","    hit = len(hit_list) / len(true_list)\n","    return hit\n","\n","def evaluate(model, X, user_train, user_valid):\n","\n","    NDCG = 0.0 # NDCG@10\n","    HIT = 0.0 # HIT@10\n","\n","    recon_mat = model.pred.cpu()\n","    score = recon_mat * torch.from_numpy(1 - X)\n","    rec_list = score.argsort(dim = 1)\n","\n","    for user, rec in enumerate(rec_list):\n","        uv = user_valid[user]\n","        up = rec[-10:].cpu().numpy().tolist()\n","        NDCG += get_ndcg(pred_list = up, true_list = uv)\n","        HIT += get_hit(pred_list = up, true_list = uv)\n","\n","    NDCG /= len(rec_list)\n","    HIT /= len(rec_list)\n","\n","    return NDCG, HIT\n","\n","\n","def predict(model, X):\n","    user2rec = {}\n","\n","    recon_mat = model.pred.cpu()\n","    score = recon_mat * torch.from_numpy(1 - X)\n","    rec_list = score.argsort(dim = 1)\n","\n","    for user, rec in enumerate(rec_list):\n","        up = rec[-10:].cpu().numpy().tolist()\n","        user2rec[user] = up\n","    \n","    return user2rec"]},{"cell_type":"markdown","metadata":{"id":"gupkaJHMslCi"},"source":["# 5. 학습"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"HFOr6Wmbq9pW"},"outputs":[],"source":["make_matrix_data_set = MakeMatrixDataSet(config = config)\n","user_train, user_valid = make_matrix_data_set.get_train_valid_data()\n","X = make_matrix_data_set.make_sparse_matrix()"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"mzhvIGPhrYov"},"outputs":[],"source":["model = EASE(X = X, reg = 750)\n","model.fit()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["candidate = make_matrix_data_set.make_year_candidate_item(train = True)\n","# candidate = make_matrix_data_set.make_cos_candidate_item(candidate_item_num = config.candidate_item_num, train = True)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["new_X = make_matrix_data_set.m_s_m(candidate)\n","# new_X = 1 - new_X.todense() # make_cos_candidate_item 사용할 때"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1103,"status":"ok","timestamp":1648373220204,"user":{"displayName":"이성범","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11166281350151773159"},"user_tz":-540},"id":"D1yj8M7bsp6X","outputId":"20892967-6ca0-41b3-fa29-56ab76c92c0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["NDCG@10: 0.81342| HIT@10: 0.20386\n"]}],"source":["ndcg, hit = evaluate(model = model, X = new_X.todense(), user_train = user_train, user_valid = user_valid)\n","print(f'NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["NDCG@10: 0.81333| HIT@10: 0.20384\n"]}],"source":["ndcg, hit = evaluate(model = model, X = X.todense(), user_train = user_train, user_valid = user_valid)\n","print(f'NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["reg: 4800| NDCG@10: 0.79857| HIT@10: 0.20000\n","reg: 4700| NDCG@10: 0.79786| HIT@10: 0.19994\n","reg: 4600| NDCG@10: 0.79932| HIT@10: 0.20013\n","reg: 4500| NDCG@10: 0.80055| HIT@10: 0.20017\n","reg: 4400| NDCG@10: 0.80065| HIT@10: 0.20021\n","reg: 4300| NDCG@10: 0.80003| HIT@10: 0.20011\n","reg: 4200| NDCG@10: 0.79924| HIT@10: 0.19998\n"]}],"source":["for reg in [4800, 4700, 4600, 4500, 4400, 4300, 4200, 4100, 4000]:\n","    model = EASE(X = X.T, reg = reg)\n","    model.fit()\n","    ndcg, hit = evaluate(model = model, X = X.todense(), user_train = user_train, user_valid = user_valid)\n","    print(f'reg: {reg}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for candidate_item_num in range(1, 20):\n","    candidate = make_matrix_data_set.make_cos_candidate_item(candidate_item_num = candidate_item_num, train = True)\n","    new_X = make_matrix_data_set.m_s_m(candidate)\n","    new_X = 1 - new_X.todense()\n","\n","    ndcg, hit = evaluate(model = model, X = new_X, user_train = user_train, user_valid = user_valid)\n","    print(f'candidate_item_num: {candidate_item_num}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"]},{"cell_type":"markdown","metadata":{},"source":["reg = 1000\n","\n","규제가 높아지면 후보 집단 생성이 더 좋은 성능을 가져다 주지는 않음\n","\n","이 이유는 현재 후보 집단 임베딩의 낮은 성능 때문인 것으로 생각됨\n","\n","```\n","candidate_item_num: 1| NDCG@10: 0.61763| HIT@10: 0.16217\n","candidate_item_num: 2| NDCG@10: 0.74284| HIT@10: 0.18944\n","candidate_item_num: 3| NDCG@10: 0.77578| HIT@10: 0.19632\n","candidate_item_num: 4| NDCG@10: 0.79034| HIT@10: 0.19934\n","candidate_item_num: 5| NDCG@10: 0.79921| HIT@10: 0.20084\n","candidate_item_num: 6| NDCG@10: 0.80333| HIT@10: 0.20157\n","candidate_item_num: 7| NDCG@10: 0.80574| HIT@10: 0.20211\n","candidate_item_num: 8| NDCG@10: 0.80748| HIT@10: 0.20246\n","candidate_item_num: 9| NDCG@10: 0.80879| HIT@10: 0.20280\n","\n","all = NDCG@10: 0.81406| HIT@10: 0.20379\n","\n","year = NDCG@10: 0.81417| HIT@10: 0.20382\n","\n","```"]},{"cell_type":"markdown","metadata":{},"source":["reg = 0.001\n","```\n","candidate_item_num: 1| NDCG@10: 0.58045| HIT@10: 0.14915\n","candidate_item_num: 2| NDCG@10: 0.67349| HIT@10: 0.16913\n","candidate_item_num: 3| NDCG@10: 0.69687| HIT@10: 0.17347\n","candidate_item_num: 4| NDCG@10: 0.70453| HIT@10: 0.17469\n","candidate_item_num: 5| NDCG@10: 0.70501| HIT@10: 0.17461\n","candidate_item_num: 6| NDCG@10: 0.70404| HIT@10: 0.17441\n","candidate_item_num: 7| NDCG@10: 0.70468| HIT@10: 0.17448\n","candidate_item_num: 8| NDCG@10: 0.70447| HIT@10: 0.17434\n","\n","```"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["make_matrix_data_set = MakeMatrixDataSet(config = config)\n","user_train, user_valid = make_matrix_data_set.get_train_valid_data()\n","X = make_matrix_data_set.make_sparse_matrix()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for reg in [2000, 1750, 1500, 1250, 1000, 750, 500, 500, 250]:\n","    model = EASE(X = X, reg = reg)\n","    model.fit()\n","    ndcg, hit = evaluate(model = model, X = X.todense(), user_train = user_train, user_valid = user_valid)\n","    print(f'reg: {reg}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"]},{"cell_type":"markdown","metadata":{},"source":["```\n","reg: 2000| NDCG@10: 0.80268| HIT@10: 0.20152\n","reg: 1750| NDCG@10: 0.80528| HIT@10: 0.20213\n","reg: 1500| NDCG@10: 0.80705| HIT@10: 0.20266\n","reg: 1250| NDCG@10: 0.81055| HIT@10: 0.20325\n","reg: 1000| NDCG@10: 0.81406| HIT@10: 0.20379\n","reg: 750| NDCG@10: 0.81333| HIT@10: 0.20384\n","reg: 500| NDCG@10: 0.81348| HIT@10: 0.20368\n","reg: 250| NDCG@10: 0.80783| HIT@10: 0.20184\n","```"]},{"cell_type":"markdown","metadata":{},"source":["```\n","reg: 1000000| NDCG@10: 0.42655| HIT@10: 0.10462\n","reg: 500000| NDCG@10: 0.45418| HIT@10: 0.11143\n","reg: 100000| NDCG@10: 0.56748| HIT@10: 0.14133\n","reg: 50000| NDCG@10: 0.62661| HIT@10: 0.15648\n","reg: 10000| NDCG@10: 0.74031| HIT@10: 0.18604\n","reg: 5000| NDCG@10: 0.77833| HIT@10: 0.19488\n","reg: 1000| NDCG@10: 0.81406| HIT@10: 0.20379\n","reg: 500| NDCG@10: 0.81348| HIT@10: 0.20368\n","reg: 100| NDCG@10: 0.79378| HIT@10: 0.19739\n","reg: 50| NDCG@10: 0.77457| HIT@10: 0.19264\n","\n","```"]},{"cell_type":"markdown","metadata":{},"source":["```\n","reg: 1000000| NDCG@10: 0.46015| HIT@10: 0.11131\n","reg: 100000| NDCG@10: 0.57234| HIT@10: 0.14224\n","reg: 10000| NDCG@10: 0.72272| HIT@10: 0.18252\n","reg: 1000| NDCG@10: 0.79034| HIT@10: 0.19934\n","reg: 100| NDCG@10: 0.77785| HIT@10: 0.19445\n","reg: 10| NDCG@10: 0.73045| HIT@10: 0.18189\n","reg: 1| NDCG@10: 0.70640| HIT@10: 0.17564\n","reg: 0.1| NDCG@10: 0.70441| HIT@10: 0.17476\n","reg: 0.01| NDCG@10: 0.70451| HIT@10: 0.17470\n","reg: 0.001| NDCG@10: 0.70453| HIT@10: 0.17469\n","reg: 0.0001| NDCG@10: 0.70453| HIT@10: 0.17469\n","reg: 1e-05| NDCG@10: 0.70453| HIT@10: 0.17469\n","\n","```"]},{"cell_type":"markdown","metadata":{},"source":["# 6. 예측"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["X_test = make_matrix_data_set.make_sparse_matrix(test = True)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["model = EASE(X = X_test, reg = config.reg)\n","model.fit()"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["candidate = make_matrix_data_set.make_year_candidate_item(train = False)\n","new_X = make_matrix_data_set.m_s_m(candidate)\n","# new_X = 1 - new_X.todense()"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["user2rec_list = predict(\n","    model = model, \n","    X = new_X.todense(),\n","    )\n","\n","submision = []\n","users = [i for i in range(0, make_matrix_data_set.num_user)]\n","for user in users:\n","    rec_item_list = user2rec_list[user]\n","    for item in rec_item_list:\n","        submision.append(\n","            {   \n","                'user' : make_matrix_data_set.user_decoder[user],\n","                'item' : make_matrix_data_set.item_decoder[item],\n","            }\n","        )\n","\n","submision = pd.DataFrame(submision)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["submision.to_csv(os.path.join(config.submission_path, config.submission_name), index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# 유저 기반 / 아이템 기반 앙상블"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["make_matrix_data_set = MakeMatrixDataSet(config = config)\n","user_train, user_valid = make_matrix_data_set.get_train_valid_data()\n","X = make_matrix_data_set.make_sparse_matrix()"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["model = EASE(X = X, reg = 750)\n","model.fit()"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["model2 = EASE(X = X.T, reg = 4400)\n","model2.fit()"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def evaluate3(model, model2, X, user_train, user_valid):\n","\n","    NDCG = 0.0 # NDCG@10\n","    HIT = 0.0 # HIT@10\n","\n","    recon_mat = model.pred.cpu()\n","    score = recon_mat * torch.from_numpy(1 - X)\n","    rec_list = score.argsort(dim = 1)\n","\n","    recon_mat2 = model2.pred.T.cpu()\n","    score2 = recon_mat2 * torch.from_numpy(1 - X)\n","    rec_list2 = score2.argsort(dim = 1)\n","\n","    for user, (rec1, rec2) in enumerate(zip(rec_list, rec_list2)):\n","        uv = user_valid[user]\n","\n","        # ranking\n","        rec1 = rec1[-10:].cpu().numpy().tolist()[::-1]\n","        rec2 = rec2[-10:].cpu().numpy().tolist()[::-1]\n","\n","        up = list(set(rec1 + rec2))\n","        \n","        NDCG += get_ndcg(pred_list = up, true_list = uv)\n","        HIT += get_hit(pred_list = up, true_list = uv)\n","\n","    NDCG /= len(rec_list)\n","    HIT /= len(rec_list)\n","\n","    return NDCG, HIT"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["NDCG@10: 0.23462| HIT@10: 0.22666\n"]}],"source":["ndcg, hit = evaluate3(model = model, model2 = model2, X = X.todense(), user_train = user_train, user_valid = user_valid)\n","print(f'NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"]},{"cell_type":"markdown","metadata":{},"source":["랭킹만 잘 하면 0.18 까지 성능이 오를 수 있을 거 같음"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def evaluate2(model, model2, X, user_train, user_valid, candidate_cnt):\n","\n","    NDCG = 0.0 # NDCG@10\n","    HIT = 0.0 # HIT@10\n","\n","    recon_mat = model.pred.cpu()\n","    score = recon_mat * torch.from_numpy(1 - X)\n","    rec_list = score.argsort(dim = 1)\n","\n","    recon_mat2 = model2.pred.T.cpu()\n","    score2 = recon_mat2 * torch.from_numpy(1 - X)\n","    rec_list2 = score2.argsort(dim = 1)\n","\n","    score_li = np.array([1 / np.log2(rank + 2) for rank in range(0, candidate_cnt)])\n","\n","    for user, (rec1, rec2) in enumerate(zip(rec_list, rec_list2)):\n","        uv = user_valid[user]\n","\n","        # ranking\n","        rec1 = rec1[-candidate_cnt:].cpu().numpy().tolist()[::-1]\n","        rec2 = rec2[-candidate_cnt:].cpu().numpy().tolist()[::-1]\n","\n","        total_movie_list = list(set(rec1 + rec2))\n","        \n","        movie_df = pd.DataFrame(index = total_movie_list)\n","        movie_df.loc[rec1, 'rec1_score'] = score_li\n","        movie_df.loc[rec2, 'rec2_score'] = score_li\n","        movie_df = movie_df.fillna(min(score_li))\n","        movie_df['total_score'] = movie_df['rec1_score'] + movie_df['rec2_score']\n","        movie_df = movie_df.sort_values('total_score', ascending = False)\n","        up = movie_df.index.tolist()[:10]\n","\n","        NDCG += get_ndcg(pred_list = up, true_list = uv)\n","        HIT += get_hit(pred_list = up, true_list = uv)\n","\n","    NDCG /= len(rec_list)\n","    HIT /= len(rec_list)\n","\n","    return NDCG, HIT"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["candidate_cnt: 5| NDCG@10: 0.40381| HIT@10: 0.15412\n","candidate_cnt: 10| NDCG@10: 0.31017| HIT@10: 0.20377\n","candidate_cnt: 15| NDCG@10: 0.31043| HIT@10: 0.20400\n","candidate_cnt: 20| NDCG@10: 0.31048| HIT@10: 0.20407\n","candidate_cnt: 25| NDCG@10: 0.31066| HIT@10: 0.20422\n","candidate_cnt: 30| NDCG@10: 0.31063| HIT@10: 0.20423\n","candidate_cnt: 35| NDCG@10: 0.31052| HIT@10: 0.20417\n"]}],"source":["for candidate_cnt in [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60]:\n","    ndcg, hit = evaluate2(model = model, model2 = model2, X = X.todense(), user_train = user_train, user_valid = user_valid, candidate_cnt = candidate_cnt)\n","    print(f'candidate_cnt: {candidate_cnt}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"]},{"cell_type":"markdown","metadata":{},"source":["```\n","1 / rank\n","\n","candidate_cnt: 5| NDCG@10: 0.94358| HIT@10: 0.15412\n","candidate_cnt: 10| NDCG@10: 1.09908| HIT@10: 0.20377\n","candidate_cnt: 15| NDCG@10: 1.09999| HIT@10: 0.20402\n","candidate_cnt: 20| NDCG@10: 1.09996| HIT@10: 0.20398\n","candidate_cnt: 25| NDCG@10: 1.10004| HIT@10: 0.20403\n","candidate_cnt: 30| NDCG@10: 1.10003| HIT@10: 0.20404\n","candidate_cnt: 35| NDCG@10: 1.10004| HIT@10: 0.20401\n","\n","```"]},{"cell_type":"markdown","metadata":{},"source":["```\n","1 / log2(rank)\n","\n","candidate_cnt: 5| NDCG@10: 0.40381| HIT@10: 0.15412\n","candidate_cnt: 10| NDCG@10: 0.31017| HIT@10: 0.20377\n","candidate_cnt: 15| NDCG@10: 0.31043| HIT@10: 0.20400\n","candidate_cnt: 20| NDCG@10: 0.31048| HIT@10: 0.20407\n","candidate_cnt: 25| NDCG@10: 0.31066| HIT@10: 0.20422\n","candidate_cnt: 30| NDCG@10: 0.31063| HIT@10: 0.20423\n","candidate_cnt: 35| NDCG@10: 0.31052| HIT@10: 0.20417\n","\n","```"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["X_test = make_matrix_data_set.make_sparse_matrix(test = True)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["model1 = EASE(X = X_test, reg = 750)\n","model1.fit()"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["model2 = EASE(X = X_test.T, reg = 4400)\n","model2.fit()"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def predict2(model1, model2, X, candidate_cnt):\n","    user2rec = {}\n","\n","    recon_mat1 = model1.pred.cpu()\n","    score1 = recon_mat1 * torch.from_numpy(1 - X)\n","    rec_list1 = score1.argsort(dim = 1)\n","\n","    recon_mat2 = model2.pred.T.cpu()\n","    score2 = recon_mat2 * torch.from_numpy(1 - X)\n","    rec_list2 = score2.argsort(dim = 1)\n","\n","    score_li = np.array([1 / np.log2(rank + 2) for rank in range(0, candidate_cnt)])\n","\n","    for user, (rec1, rec2) in enumerate(zip(rec_list1, rec_list2)):\n","        # ranking\n","        rec1 = rec1[-candidate_cnt:].cpu().numpy().tolist()[::-1]\n","        rec2 = rec2[-candidate_cnt:].cpu().numpy().tolist()[::-1]\n","\n","        total_movie_list = list(set(rec1 + rec2))\n","        \n","        movie_df = pd.DataFrame(index = total_movie_list)\n","        movie_df.loc[rec1, 'rec1_score'] = score_li\n","        movie_df.loc[rec2, 'rec2_score'] = score_li\n","        movie_df = movie_df.fillna(min(score_li))\n","        movie_df['total_score'] = movie_df['rec1_score'] + movie_df['rec2_score']\n","        movie_df = movie_df.sort_values('total_score', ascending = False)\n","        up = movie_df.index.tolist()[:10]\n","\n","        user2rec[user] = up\n","    \n","    return user2rec"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["user2rec_list = predict2(\n","    model1 = model1, \n","    model2 = model2, \n","    X = X_test.todense(), \n","    candidate_cnt = 30,\n","    )\n","\n","submision = []\n","users = [i for i in range(0, make_matrix_data_set.num_user)]\n","for user in users:\n","    rec_item_list = user2rec_list[user]\n","    for item in rec_item_list:\n","        submision.append(\n","            {   \n","                'user' : make_matrix_data_set.user_decoder[user],\n","                'item' : make_matrix_data_set.item_decoder[item],\n","            }\n","        )\n","\n","submision = pd.DataFrame(submision)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["submision.to_csv(os.path.join(config.submission_path, config.submission_name), index=False)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user</th>\n","      <th>item</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>11</td>\n","      <td>4370</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>11</td>\n","      <td>40815</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>11</td>\n","      <td>4886</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11</td>\n","      <td>7373</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>11</td>\n","      <td>8961</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>313595</th>\n","      <td>138493</td>\n","      <td>5349</td>\n","    </tr>\n","    <tr>\n","      <th>313596</th>\n","      <td>138493</td>\n","      <td>2080</td>\n","    </tr>\n","    <tr>\n","      <th>313597</th>\n","      <td>138493</td>\n","      <td>3000</td>\n","    </tr>\n","    <tr>\n","      <th>313598</th>\n","      <td>138493</td>\n","      <td>110</td>\n","    </tr>\n","    <tr>\n","      <th>313599</th>\n","      <td>138493</td>\n","      <td>32587</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>313600 rows × 2 columns</p>\n","</div>"],"text/plain":["          user   item\n","0           11   4370\n","1           11  40815\n","2           11   4886\n","3           11   7373\n","4           11   8961\n","...        ...    ...\n","313595  138493   5349\n","313596  138493   2080\n","313597  138493   3000\n","313598  138493    110\n","313599  138493  32587\n","\n","[313600 rows x 2 columns]"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["submision"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMB0K8DmOwclQIFyyBerAFJ","collapsed_sections":[],"mount_file_id":"1C1AHkN-z-DCxUIAjFfd7K56P-8-YrAJO","name":"EASE.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}
