{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from box import Box\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.학습 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    'data_path' : \"/opt/ml/input/data/train\" , # 데이터 경로\n",
    "    'model_path' : \"../model\", # 모델 저장 경로\n",
    "    'model_name' : 'Multi-BRET4Rec_v1.pt',\n",
    "\n",
    "    'max_len' : 2922,\n",
    "    'hidden_units' : 50, # Embedding size\n",
    "    'num_heads' : 1, # Multi-head layer 의 수 (병렬 처리)\n",
    "    'num_layers': 2, # block의 개수 (encoder layer의 개수)\n",
    "    'dropout_rate' : 0.5, # dropout 비율\n",
    "\n",
    "    'lr' : 0.001,\n",
    "    'batch_size' : 32,\n",
    "    'num_epochs' : 200,\n",
    "    'num_workers' : 6,\n",
    "    \n",
    "    'valid_samples' : 10, # 검증에 사용할 sample 수\n",
    "    'seed' : 22,\n",
    "}\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "config = Box(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeSequenceDataSet():\n",
    "    \"\"\"\n",
    "    SequenceData 생성\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.df = pd.read_csv(os.path.join(self.config.data_path, 'train_ratings.csv'))\n",
    "\n",
    "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('item')\n",
    "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('user')\n",
    "        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n",
    "\n",
    "        self.df['item_idx'] = self.df['item'].apply(lambda x : self.item_encoder[x]) # padding 고려\n",
    "        self.df['user_idx'] = self.df['user'].apply(lambda x : self.user_encoder[x])\n",
    "        self.df = self.df.sort_values(['user_idx', 'time']) # 시간에 따라 정렬\n",
    "        self.user_train, self.user_valid = self.generate_sequence_data()\n",
    "\n",
    "    def generate_encoder_decoder(self, col : str) -> dict:\n",
    "        \"\"\"\n",
    "        encoder, decoder 생성\n",
    "\n",
    "        Args:\n",
    "            col (str): 생성할 columns 명\n",
    "        Returns:\n",
    "            dict: 생성된 user encoder, decoder\n",
    "        \"\"\"\n",
    "\n",
    "        encoder = {}\n",
    "        decoder = {}\n",
    "        ids = self.df[col].unique()\n",
    "\n",
    "        for idx, _id in enumerate(ids):\n",
    "            encoder[_id] = idx\n",
    "            decoder[idx] = _id\n",
    "\n",
    "        return encoder, decoder\n",
    "    \n",
    "    def generate_sequence_data(self) -> dict:\n",
    "        \"\"\"\n",
    "        sequence_data 생성\n",
    "\n",
    "        Returns:\n",
    "            dict: train user sequence / valid user sequence\n",
    "        \"\"\"\n",
    "        users = defaultdict(list)\n",
    "        user_train = {}\n",
    "        user_valid = {}\n",
    "        for user, item, time in zip(self.df['user_idx'], self.df['item_idx'], self.df['time']):\n",
    "            users[user].append(item)\n",
    "        \n",
    "        for user in users:\n",
    "            np.random.seed(self.config.seed)\n",
    "\n",
    "            user_total = users[user]\n",
    "            valid = np.random.choice(user_total, size = self.config.valid_samples, replace = False).tolist()\n",
    "            train = list(set(user_total) - set(valid))\n",
    "\n",
    "            user_train[user] = train\n",
    "            user_valid[user] = valid # valid_samples 개수 만큼 검증에 활용 (현재 Task와 가장 유사하게)\n",
    "\n",
    "        return user_train, user_valid\n",
    "    \n",
    "    def get_train_valid_data(self):\n",
    "        return self.user_train, self.user_valid\n",
    "\n",
    "    def make_matrix(self, user_list, train = True):\n",
    "        \"\"\"\n",
    "        user_item_dict를 바탕으로 행렬 생성\n",
    "        \"\"\"\n",
    "        mat = torch.zeros(size = (user_list.size(0), self.num_item))\n",
    "        for idx, user in enumerate(user_list):\n",
    "            if train:\n",
    "                mat[idx, self.user_train[user.item()]] = 1\n",
    "            else:\n",
    "                mat[idx, self.user_train[user.item()] + self.user_valid[user.item()]] = 1\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTRecDataSet(Dataset):\n",
    "    def __init__(self, user_train, num_user, max_len = 2922):\n",
    "        self.user_train = user_train\n",
    "        self.max_len = max_len\n",
    "        self.num_user = num_user\n",
    "\n",
    "    def __len__(self):\n",
    "        # 총 user의 수 = 학습에 사용할 sequence의 수\n",
    "        return self.num_user\n",
    "\n",
    "    def __getitem__(self, user): \n",
    "        user_seq = (np.array(self.user_train[user]) + 1).tolist()\n",
    "        mask_len = self.max_len - len(user_seq)\n",
    "        user_seq = [0] * mask_len + user_seq\n",
    "\n",
    "        return torch.LongTensor([user]), torch.LongTensor(user_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n",
    "\n",
    "    def forward(self, Q, K, V, mask):\n",
    "        attn_score = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.hidden_units)\n",
    "        attn_score = attn_score.masked_fill(mask == 0, -1e9)  # 유사도가 0인 지점은 -infinity로 보내 softmax 결과가 0이 되도록 함\n",
    "        attn_dist = self.dropout(F.softmax(attn_score, dim=-1))  # attention distribution\n",
    "        output = torch.matmul(attn_dist, V)  # dim of output : batchSize x num_head x seqLen x hidden_units\n",
    "        return output, attn_dist\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads # head의 수\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "        # query, key, value, output 생성을 위해 Linear 모델 생성\n",
    "        self.W_Q = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_K = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_V = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_O = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(hidden_units, dropout_rate) # scaled dot product attention module을 사용하여 attention 계산\n",
    "        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n",
    "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
    "\n",
    "    def forward(self, enc, mask):\n",
    "        residual = enc # residual connection을 위해 residual 부분을 저장\n",
    "        batch_size, seqlen = enc.size(0), enc.size(1)\n",
    "        \n",
    "        # Query, Key, Value를 (num_head)개의 Head로 나누어 각기 다른 Linear projection을 통과시킴\n",
    "        Q = self.W_Q(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units) \n",
    "        K = self.W_K(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units)\n",
    "        V = self.W_V(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units)\n",
    "\n",
    "        # Head별로 각기 다른 attention이 가능하도록 Transpose 후 각각 attention에 통과시킴\n",
    "        Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2)\n",
    "        output, attn_dist = self.attention(Q, K, V, mask)\n",
    "\n",
    "        # 다시 Transpose한 후 모든 head들의 attention 결과를 합칩니다.\n",
    "        output = output.transpose(1, 2).contiguous() \n",
    "        output = output.view(batch_size, seqlen, -1)\n",
    "\n",
    "        # Linear Projection, Dropout, Residual sum, and Layer Normalization\n",
    "        output = self.layerNorm(self.dropout(self.W_O(output)) + residual)\n",
    "        return output, attn_dist\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        \n",
    "        # SASRec과의 dimension 차이가 있습니다.\n",
    "        self.W_1 = nn.Linear(hidden_units, 4 * hidden_units) \n",
    "        self.W_2 = nn.Linear(4 * hidden_units, hidden_units)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = self.W_2(F.gelu(self.dropout(self.W_1(x)))) # activation: relu -> gelu\n",
    "        output = self.layerNorm(self.dropout(output) + residual)\n",
    "        return output\n",
    "\n",
    "\n",
    "class BERT4RecBlock(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
    "        super(BERT4RecBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads, hidden_units, dropout_rate)\n",
    "        self.pointwise_feedforward = PositionwiseFeedForward(hidden_units, dropout_rate)\n",
    "\n",
    "    def forward(self, input_enc, mask):\n",
    "        output_enc, attn_dist = self.attention(input_enc, mask)\n",
    "        output_enc = self.pointwise_feedforward(output_enc)\n",
    "        return output_enc, attn_dist\n",
    "\n",
    "        \n",
    "class BERT4Rec(nn.Module):\n",
    "    def __init__(self, num_user, num_item, hidden_units, num_heads, num_layers, max_len, dropout_rate, device):\n",
    "        super(BERT4Rec, self).__init__()\n",
    "\n",
    "        self.num_user = num_user\n",
    "        self.num_item = num_item\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers \n",
    "        self.device = device\n",
    "        \n",
    "        self.item_emb = nn.Embedding(num_item + 1, hidden_units, padding_idx=0) # TODO2: mask와 padding을 고려하여 embedding을 생성해보세요.\n",
    "        self.pos_emb = nn.Embedding(max_len, hidden_units) # learnable positional encoding\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.emb_layernorm = nn.LayerNorm(hidden_units, eps=1e-6)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([BERT4RecBlock(num_heads, hidden_units, dropout_rate) for _ in range(num_layers)])\n",
    "        self.out = nn.Linear(hidden_units, num_item) # TODO3: 예측을 위한 output layer를 구현해보세요. (num_item 주의)\n",
    "        \n",
    "    def forward(self, log_seqs):\n",
    "        seqs = self.item_emb(torch.LongTensor(log_seqs).to(self.device))\n",
    "        # positions = np.tile(np.array(range(log_seqs.shape[1])), [log_seqs.shape[0], 1])\n",
    "        # seqs += self.pos_emb(torch.LongTensor(positions).to(self.device))\n",
    "        seqs = self.emb_layernorm(self.dropout(seqs))\n",
    "\n",
    "        mask = torch.BoolTensor(log_seqs > 0).unsqueeze(1).repeat(1, log_seqs.shape[1], 1).unsqueeze(1).to(self.device) # mask for zero pad\n",
    "        for block in self.blocks:\n",
    "            seqs, attn_dist = block(seqs, mask)\n",
    "        out = self.out(seqs)\n",
    "        return out.mean(dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ndcg(pred_list, true_list):\n",
    "    idcg = sum((1 / np.log2(rank + 2) for rank in range(1, len(pred_list))))\n",
    "    dcg = 0\n",
    "    for rank, pred in enumerate(pred_list):\n",
    "        if pred in true_list:\n",
    "            dcg += 1 / np.log2(rank + 2)\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg\n",
    "\n",
    "# hit == recall == precision\n",
    "def get_hit(pred_list, true_list):\n",
    "    hit_list = set(true_list) & set(pred_list)\n",
    "    hit = len(hit_list) / len(true_list)\n",
    "    return hit\n",
    "\n",
    "def train(model, criterion, optimizer, data_loader, make_data_set):\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "    for users, seq in data_loader:\n",
    "        mat = make_data_set.make_matrix(users)\n",
    "        mat = mat.to(device)\n",
    "        \n",
    "        logits = model(seq)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = criterion(logits, mat)\n",
    "\n",
    "        loss_val += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_val /= len(data_loader)\n",
    "\n",
    "    return loss_val\n",
    "\n",
    "def evaluate(model, user_train, user_valid, data_loader, make_data_set):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    NDCG = 0.0 # NDCG@10\n",
    "    HIT = 0.0 # HIT@10\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for users, seq in data_loader:\n",
    "            mat = make_data_set.make_matrix(users)\n",
    "            mat = mat.to(device)\n",
    "\n",
    "            recon_mat = model(seq)\n",
    "            recon_mat[mat == 1] = -np.inf\n",
    "            rec_list = recon_mat.argsort(dim = 1)\n",
    "\n",
    "            for user, rec in zip(users, rec_list):\n",
    "                uv = user_valid[user.item()]\n",
    "                up = rec[-10:].cpu().numpy().tolist()\n",
    "                NDCG += get_ndcg(pred_list = up, true_list = uv)\n",
    "                HIT += get_hit(pred_list = up, true_list = uv)\n",
    "\n",
    "    NDCG /= len(data_loader.dataset)\n",
    "    HIT /= len(data_loader.dataset)\n",
    "\n",
    "    return NDCG, HIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, x_pred, user_ratings):\n",
    "        mll = (F.log_softmax(x_pred, dim=-1) * user_ratings).sum(dim=-1).mean()\n",
    "        return -mll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_sequence_dataset = MakeSequenceDataSet(config = config)\n",
    "user_train, user_valid = make_sequence_dataset.get_train_valid_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertrec_dataset = BERTRecDataSet(\n",
    "    user_train = user_train, \n",
    "    num_user = make_sequence_dataset.num_user,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "    bertrec_dataset, \n",
    "    batch_size = config.batch_size,\n",
    "    shuffle = True, \n",
    "    pin_memory = True,\n",
    "    num_workers = config.num_workers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for num_worker in range(7):\n",
    "    data_loader = DataLoader(\n",
    "    bertrec_dataset, \n",
    "    batch_size = config.batch_size,\n",
    "    shuffle = True, \n",
    "    pin_memory = True,\n",
    "    num_workers = num_worker,\n",
    "    )\n",
    "    start = time.time()\n",
    "    for _ in data_loader:\n",
    "        pass\n",
    "    end = time.time()\n",
    "\n",
    "    print(num_worker, end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT4Rec(\n",
    "    num_user = make_sequence_dataset.num_user, \n",
    "    num_item = make_sequence_dataset.num_item, \n",
    "    hidden_units = config.hidden_units, \n",
    "    num_heads = config.num_heads, \n",
    "    num_layers = config.num_layers, \n",
    "    max_len = config.max_len, \n",
    "    dropout_rate = config.dropout_rate, \n",
    "    device = device,\n",
    "    ).to(device)\n",
    "\n",
    "criterion = MultiLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ndcg = 0\n",
    "for epoch in range(1, config.num_epochs + 1):\n",
    "    tbar = tqdm(range(1))\n",
    "    for _ in tbar:\n",
    "        train_loss = train(\n",
    "            model = model, \n",
    "            criterion = criterion, \n",
    "            optimizer = optimizer, \n",
    "            data_loader = data_loader,\n",
    "            make_data_set = make_sequence_dataset)\n",
    "\n",
    "        ndcg, hit = evaluate(\n",
    "            model = model, \n",
    "            user_train = user_train, \n",
    "            user_valid = user_valid,\n",
    "            data_loader = data_loader,\n",
    "            make_data_set = make_sequence_dataset,\n",
    "            )\n",
    "        \n",
    "        if best_ndcg < ndcg:\n",
    "            best_ndcg = ndcg\n",
    "            torch.save(model.state_dict(), os.path.join(config.model_path, config.model_name))\n",
    "\n",
    "        tbar.set_description(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 임베딩 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_emb = model.item_emb.weight[1 : -1].data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state = 0)\n",
    "new_arr = tsne.fit_transform(item_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(8, 8))\n",
    "\n",
    "ax.scatter(new_arr[:, 0], new_arr[:, 1], alpha = 0.7)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
