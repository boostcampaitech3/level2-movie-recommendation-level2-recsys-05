{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from box import Box\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 정리\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 학습 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data_path' : \"/opt/ml/input/data/train\" , # 데이터 경로\n",
    "    \n",
    "    'submission_path' : \"../submission\",\n",
    "    'submission_name' : 'LightGCN_submission.csv', \n",
    "\n",
    "    'model_path' : \"../model\", # 모델 저장 경로\n",
    "    'model_name' : 'LightGCN_v1.pt',\n",
    "\n",
    "    'num_epochs' : 50,\n",
    "    \"reg\" : 1e-5,\n",
    "    'lr' : 0.0001,\n",
    "    \"emb_dim\" : 512,\n",
    "    \"n_layers\" : 3,\n",
    "    'batch_size' : 1024,\n",
    "    \"node_dropout\" : 0.2,\n",
    "\n",
    "    'valid_samples' : 10, # 검증에 사용할 sample 수\n",
    "    'seed' : 22,\n",
    "    'n_batch' : 30,\n",
    "}\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "config = Box(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(config.model_path):\n",
    "    os.mkdir(config.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(config.submission_path):\n",
    "    os.mkdir(config.submission_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeGraphDataSet():\n",
    "    \"\"\"\n",
    "    GraphDataSet 생성\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.df = pd.read_csv(os.path.join(self.config.data_path, 'train_ratings.csv'))\n",
    "\n",
    "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('item')\n",
    "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('user')\n",
    "        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n",
    "\n",
    "        self.df['item_idx'] = self.df['item'].apply(lambda x : self.item_encoder[x])\n",
    "        self.df['user_idx'] = self.df['user'].apply(lambda x : self.user_encoder[x])\n",
    "        \n",
    "        self.exist_users = [i for i in range(self.num_user)]\n",
    "        self.exist_items = [i for i in range(self.num_item)]\n",
    "        self.user_train, self.user_valid = self.generate_sequence_data()\n",
    "        self.R_train, self.R_valid, self.R_total = self.generate_dok_matrix()\n",
    "        self.ngcf_adj_matrix = self.generate_ngcf_adj_matrix()\n",
    "        self.n_train = len(self.R_train)\n",
    "        self.batch_size = self.config.batch_size\n",
    "\n",
    "    def generate_encoder_decoder(self, col : str) -> dict:\n",
    "        \"\"\"\n",
    "        encoder, decoder 생성\n",
    "\n",
    "        Args:\n",
    "            col (str): 생성할 columns 명\n",
    "        Returns:\n",
    "            dict: 생성된 user encoder, decoder\n",
    "        \"\"\"\n",
    "\n",
    "        encoder = {}\n",
    "        decoder = {}\n",
    "        ids = self.df[col].unique()\n",
    "\n",
    "        for idx, _id in enumerate(ids):\n",
    "            encoder[_id] = idx\n",
    "            decoder[idx] = _id\n",
    "\n",
    "        return encoder, decoder\n",
    "    \n",
    "    def generate_sequence_data(self) -> dict:\n",
    "        \"\"\"\n",
    "        sequence_data 생성\n",
    "\n",
    "        Returns:\n",
    "            dict: train user sequence / valid user sequence\n",
    "        \"\"\"\n",
    "        users = defaultdict(list)\n",
    "        user_train = {}\n",
    "        user_valid = {}\n",
    "        for user, item, time in zip(self.df['user_idx'], self.df['item_idx'], self.df['time']):\n",
    "            users[user].append(item)\n",
    "        \n",
    "        for user in users:\n",
    "            np.random.seed(self.config.seed)\n",
    "\n",
    "            user_total = users[user]\n",
    "            valid = np.random.choice(user_total, size = self.config.valid_samples, replace = False).tolist()\n",
    "            train = list(set(user_total) - set(valid))\n",
    "\n",
    "            user_train[user] = train\n",
    "            user_valid[user] = valid # valid_samples 개수 만큼 검증에 활용 (현재 Task와 가장 유사하게)\n",
    "\n",
    "        return user_train, user_valid\n",
    "    \n",
    "    def generate_dok_matrix(self):\n",
    "        R_train = sp.dok_matrix((self.num_user, self.num_item), dtype=np.float32)\n",
    "        R_valid = sp.dok_matrix((self.num_user, self.num_item), dtype=np.float32)\n",
    "        R_total = sp.dok_matrix((self.num_user, self.num_item), dtype=np.float32)\n",
    "        user_list = self.exist_users\n",
    "        for user in user_list:\n",
    "            train_items = self.user_train[user]\n",
    "            valid_items = self.user_valid[user]\n",
    "            \n",
    "            for train_item in train_items:\n",
    "                R_train[user, train_item] = 1.0\n",
    "                R_total[user, train_item] = 1.0\n",
    "            \n",
    "            for valid_item in valid_items:\n",
    "                R_valid[user, valid_item] = 1.0\n",
    "                R_total[user, valid_item] = 1.0\n",
    "        \n",
    "        return R_train, R_valid, R_total\n",
    "\n",
    "    def generate_ngcf_adj_matrix(self):\n",
    "        adj_mat = sp.dok_matrix((self.num_user + self.num_item, self.num_user + self.num_item), dtype=np.float32)\n",
    "        adj_mat = adj_mat.tolil() # to_list\n",
    "        R = self.R_train.tolil()\n",
    "\n",
    "        adj_mat[:self.num_user, self.num_user:] = R\n",
    "        adj_mat[self.num_user:, :self.num_user] = R.T\n",
    "        adj_mat = adj_mat.todok() # to_dok_matrix\n",
    "\n",
    "        def normalized_adj_single(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "            d_inv = np.power(rowsum, -.5).flatten()  \n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "            norm_adj = d_mat_inv.dot(adj).dot(d_mat_inv)\n",
    "\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        ngcf_adj_matrix = normalized_adj_single(adj_mat)\n",
    "        return ngcf_adj_matrix.tocsr()\n",
    "\n",
    "    def sampling(self):\n",
    "        users = random.sample(self.exist_users, self.config.batch_size)\n",
    "\n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            pos_items = self.user_train[u]\n",
    "            pos_batch = random.sample(pos_items, num)\n",
    "            return pos_batch\n",
    "        \n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            neg_items = list(set(self.exist_items) - set(self.user_train[u]))\n",
    "            neg_batch = random.sample(neg_items, num)\n",
    "            return neg_batch\n",
    "        \n",
    "        pos_items, neg_items = [], []\n",
    "        for user in users:\n",
    "            pos_items += sample_pos_items_for_u(user, 1)\n",
    "            neg_items += sample_neg_items_for_u(user, 1)\n",
    "        \n",
    "        return users, pos_items, neg_items\n",
    "\n",
    "    def get_train_valid_data(self):\n",
    "        return self.user_train, self.user_valid\n",
    "\n",
    "    def get_R_data(self):\n",
    "        return self.R_train, self.R_valid, self.R_total\n",
    "\n",
    "    def get_ngcf_adj_matrix_data(self):\n",
    "        return self.ngcf_adj_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_dim, n_layers, reg, node_dropout, adj_mtx):\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize Class attributes\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.emb_dim = emb_dim\n",
    "        self.l = adj_mtx\n",
    "        self.graph = self._convert_sp_mat_to_sp_tensor(self.l)\n",
    "\n",
    "        self.reg = reg\n",
    "        self.n_layers = n_layers\n",
    "        self.node_dropout = node_dropout\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weight_dict = self._init_weights()\n",
    "        print(\"Weights initialized.\")\n",
    "\n",
    "    # initialize weights\n",
    "    def _init_weights(self):\n",
    "        print(\"Initializing weights...\")\n",
    "        weight_dict = nn.ParameterDict()\n",
    "\n",
    "        initializer = torch.nn.init.xavier_uniform_\n",
    "        \n",
    "        weight_dict['user_embedding'] = nn.Parameter(initializer(torch.empty(self.n_users, self.emb_dim).to(device)))\n",
    "        weight_dict['item_embedding'] = nn.Parameter(initializer(torch.empty(self.n_items, self.emb_dim).to(device)))\n",
    "           \n",
    "        return weight_dict\n",
    "\n",
    "    # convert sparse matrix into sparse PyTorch tensor\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        \"\"\"\n",
    "        Convert scipy sparse matrix to PyTorch sparse matrix\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        X = Adjacency matrix, scipy sparse matrix\n",
    "        \"\"\"\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        i = torch.LongTensor(np.mat([coo.row, coo.col]))\n",
    "        v = torch.FloatTensor(coo.data)\n",
    "        res = torch.sparse.FloatTensor(i, v, coo.shape).to(device)\n",
    "        return res\n",
    "\n",
    "    # apply node_dropout\n",
    "    def _droupout_sparse(self, X):\n",
    "        \"\"\"\n",
    "        Drop individual locations in X\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "        X = adjacency matrix (PyTorch sparse tensor)\n",
    "        dropout = fraction of nodes to drop\n",
    "        noise_shape = number of non non-zero entries of X\n",
    "        \"\"\"\n",
    "        node_dropout_mask = ((self.node_dropout) + torch.rand(X._nnz())).floor().bool().to(device)\n",
    "        i = X.coalesce().indices()\n",
    "        v = X.coalesce()._values()\n",
    "        i[:,node_dropout_mask] = 0\n",
    "        v[node_dropout_mask] = 0\n",
    "        X_dropout = torch.sparse.FloatTensor(i, v, X.shape).to(X.device)\n",
    "\n",
    "        return  X_dropout.mul(1/(1-self.node_dropout))\n",
    "\n",
    "    def forward(self, u, i, j):\n",
    "        \"\"\"\n",
    "        Computes the forward pass\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "        u = user\n",
    "        i = positive item (user interacted with item)\n",
    "        j = negative item (user did not interact with item)\n",
    "        \"\"\"\n",
    "        # apply drop-out mask\n",
    "        graph = self._droupout_sparse(self.graph) if self.node_dropout > 0 else self.graph\n",
    "        ego_embeddings = torch.cat([self.weight_dict['user_embedding'], self.weight_dict['item_embedding']], 0)\n",
    "        final_embeddings = [ego_embeddings]\n",
    "\n",
    "        for k in range(self.n_layers):\n",
    "            ego_embeddings = torch.sparse.mm(graph, final_embeddings[k])\n",
    "            final_embeddings.append(ego_embeddings)                                       \n",
    "\n",
    "        final_embeddings = torch.stack(final_embeddings, dim=1)\n",
    "        final_embeddings = torch.mean(final_embeddings, dim=1)\n",
    "        \n",
    "        u_final_embeddings, i_final_embeddings = final_embeddings.split([self.n_users, self.n_items], 0)\n",
    "\n",
    "        self.u_final_embeddings = nn.Parameter(u_final_embeddings)\n",
    "        self.i_final_embeddings = nn.Parameter(i_final_embeddings)\n",
    "        \n",
    "        # loss 계산\n",
    "        u_emb = u_final_embeddings[u] # user embeddings\n",
    "        p_emb = i_final_embeddings[i] # positive item embeddings\n",
    "        n_emb = i_final_embeddings[j] # negative item embeddings\n",
    "        \n",
    "        y_ui = torch.sum(torch.mul(u_emb, p_emb), dim = 1)                        \n",
    "        y_uj = torch.sum(torch.mul(u_emb, n_emb), dim = 1)\n",
    "        \n",
    "        log_prob = torch.mean(torch.log(torch.sigmoid(y_ui - y_uj))) \n",
    "        bpr_loss = -log_prob        \n",
    "        if self.reg > 0.:\n",
    "            l2norm = (torch.sum(u_emb**2)/2. + torch.sum(p_emb**2)/2. + torch.sum(n_emb**2)/2.) / u_emb.shape[0]\n",
    "            l2reg = self.reg * l2norm\n",
    "            bpr_loss += l2reg\n",
    "\n",
    "        \n",
    "        return bpr_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, make_graph_data_set, optimizer, n_batch):\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "    for step in range(1, n_batch + 1):\n",
    "        user, pos, neg = make_graph_data_set.sampling()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(user, pos, neg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_val += loss.item()\n",
    "    loss_val /= n_batch\n",
    "    return loss_val\n",
    "\n",
    "def split_matrix(X, n_splits=10):\n",
    "    splits = []\n",
    "    chunk_size = X.shape[0] // n_splits\n",
    "    for i in range(n_splits):\n",
    "        start = i * chunk_size\n",
    "        end = X.shape[0] if i == n_splits - 1 else (i + 1) * chunk_size\n",
    "        splits.append(X[start:end])\n",
    "    return splits\n",
    "\n",
    "def compute_ndcg_k(pred_items, test_items, test_indices, k):\n",
    "    \n",
    "    r = (test_items * pred_items).gather(1, test_indices)\n",
    "    f = torch.from_numpy(np.log2(np.arange(2, k+2))).float().to(device)\n",
    "    \n",
    "    dcg = (r[:, :k]/f).sum(1)                                               \n",
    "    dcg_max = (torch.sort(r, dim=1, descending=True)[0][:, :k]/f).sum(1)   \n",
    "    ndcg = dcg/dcg_max                                                     \n",
    "    \n",
    "    ndcg[torch.isnan(ndcg)] = 0\n",
    "    return ndcg\n",
    "\n",
    "def evaluate(u_emb, i_emb, Rtr, Rte, k = 10):\n",
    "\n",
    "    # split matrices\n",
    "    ue_splits = split_matrix(u_emb)\n",
    "    tr_splits = split_matrix(Rtr)\n",
    "    te_splits = split_matrix(Rte)\n",
    "\n",
    "    recall_k, ndcg_k= [], []\n",
    "    # compute results for split matrices\n",
    "    for ue_f, tr_f, te_f in zip(ue_splits, tr_splits, te_splits):\n",
    "\n",
    "        scores = torch.mm(ue_f, i_emb.t())\n",
    "\n",
    "        test_items = torch.from_numpy(te_f.todense()).float().to(device)\n",
    "        non_train_items = torch.from_numpy(1-(tr_f.todense())).float().to(device)\n",
    "        scores = scores * non_train_items\n",
    "\n",
    "        _, test_indices = torch.topk(scores, dim=1, k=k)\n",
    "        \n",
    "        pred_items = torch.zeros_like(scores).float()\n",
    "        pred_items.scatter_(dim=1, index=test_indices, src=torch.ones_like(test_indices).float().to(device))\n",
    "\n",
    "        topk_preds = torch.zeros_like(scores).float()\n",
    "        topk_preds.scatter_(dim=1, index=test_indices[:, :k], src=torch.ones_like(test_indices).float())\n",
    "        \n",
    "        TP = (test_items * topk_preds).sum(1)                      \n",
    "        rec = TP/test_items.sum(1)\n",
    "   \n",
    "        ndcg = compute_ndcg_k(pred_items, test_items, test_indices, k)\n",
    "\n",
    "        recall_k.append(rec)\n",
    "        ndcg_k.append(ndcg)\n",
    "\n",
    "    return torch.cat(ndcg_k).mean(), torch.cat(recall_k).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_graph_data_set = MakeGraphDataSet(config = config)\n",
    "ngcf_adj_matrix = make_graph_data_set.get_ngcf_adj_matrix_data()\n",
    "R_train, R_valid, R_total = make_graph_data_set.get_R_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightGCN(\n",
    "    n_users = make_graph_data_set.num_user,\n",
    "    n_items = make_graph_data_set.num_item,\n",
    "    emb_dim = config.emb_dim,\n",
    "    n_layers = config.n_layers,\n",
    "    reg = config.reg,\n",
    "    node_dropout = config.node_dropout,\n",
    "    adj_mtx = ngcf_adj_matrix,\n",
    "    ).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hit = 0\n",
    "for epoch in range(1, config.num_epochs + 1):\n",
    "    tbar = tqdm(range(1))\n",
    "    for _ in tbar:\n",
    "        train_loss = train(\n",
    "            model = model, \n",
    "            make_graph_data_set = make_graph_data_set, \n",
    "            optimizer = optimizer,\n",
    "            n_batch = config.n_batch,\n",
    "            )\n",
    "        with torch.no_grad():\n",
    "            ndcg, hit = evaluate(\n",
    "                u_emb = model.u_final_embeddings.detach(), \n",
    "                i_emb = model.i_final_embeddings.detach(), \n",
    "                Rtr = R_train, \n",
    "                Rte = R_valid, \n",
    "                k = 10,\n",
    "                )\n",
    "\n",
    "        if best_hit < hit:\n",
    "            best_hit = hit\n",
    "            torch.save(model.state_dict(), os.path.join(config.model_path, config.model_name))\n",
    "\n",
    "        tbar.set_description(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(config.model_path, config.model_name)))\n",
    "submision = []\n",
    "with torch.no_grad():\n",
    "    k = 10\n",
    "    u_emb = model.u_final_embeddings.detach()\n",
    "    i_emb = model.i_final_embeddings.detach()\n",
    "\n",
    "    scores = torch.mm(u_emb, i_emb.t())\n",
    "\n",
    "    test_items = torch.from_numpy(R_total.todense()).float().to(device)\n",
    "    non_train_items = torch.from_numpy(1-(R_total.todense())).float().to(device)\n",
    "    scores = scores * non_train_items\n",
    "\n",
    "    _, test_indices = torch.topk(scores, dim=1, k=k)\n",
    "\n",
    "    pred_items = torch.zeros_like(scores).float()\n",
    "    pred_items.scatter_(dim=1, index=test_indices, src=torch.ones_like(test_indices).float().to(device))\n",
    "    pred_items = pred_items.argsort(dim = 1)\n",
    "    for user, item_list in enumerate(pred_items):\n",
    "        item_list = item_list[-10:].cpu().numpy().tolist()\n",
    "        for item in item_list:\n",
    "            submision.append(\n",
    "                {   \n",
    "                    'user' : make_graph_data_set.user_decoder[user],\n",
    "                    'item' : make_graph_data_set.item_decoder[item],\n",
    "                }\n",
    "            )\n",
    "\n",
    "submision = pd.DataFrame(submision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "submision.to_csv(os.path.join(config.submission_path, config.submission_name), index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
