{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from box import Box\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 정리\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 학습 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data_path' : \"/opt/ml/input/data/train\" , # 데이터 경로\n",
    "    \n",
    "    'submission_path' : \"../submission\",\n",
    "    'submission_name' : 'LightGCN_submission.csv', \n",
    "\n",
    "    'model_path' : \"../model\", # 모델 저장 경로\n",
    "    'model_name' : 'LightGCN_v1.pt',\n",
    "\n",
    "    'num_epochs' : 50,\n",
    "    \"reg\" : 1e-5,\n",
    "    'lr' : 0.0001,\n",
    "    \"emb_dim\" : 512,\n",
    "    \"n_layers\" : 3,\n",
    "    'batch_size' : 1024,\n",
    "    \"node_dropout\" : 0.2,\n",
    "\n",
    "    'valid_samples' : 10, # 검증에 사용할 sample 수\n",
    "    'seed' : 22,\n",
    "    'n_batch' : 30,\n",
    "}\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "config = Box(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(config.model_path):\n",
    "    os.mkdir(config.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(config.submission_path):\n",
    "    os.mkdir(config.submission_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeGraphDataSet():\n",
    "    \"\"\"\n",
    "    GraphDataSet 생성\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.df = pd.read_csv(os.path.join(self.config.data_path, 'train_ratings.csv'))\n",
    "\n",
    "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('item')\n",
    "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('user')\n",
    "        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n",
    "\n",
    "        self.df['item_idx'] = self.df['item'].apply(lambda x : self.item_encoder[x])\n",
    "        self.df['user_idx'] = self.df['user'].apply(lambda x : self.user_encoder[x])\n",
    "        \n",
    "        self.exist_users = [i for i in range(self.num_user)]\n",
    "        self.exist_items = [i for i in range(self.num_item)]\n",
    "        self.user_train, self.user_valid = self.generate_sequence_data()\n",
    "        self.R_train, self.R_valid, self.R_total = self.generate_dok_matrix()\n",
    "        self.ngcf_adj_matrix = self.generate_ngcf_adj_matrix()\n",
    "        self.n_train = len(self.R_train)\n",
    "        self.batch_size = self.config.batch_size\n",
    "\n",
    "    def generate_encoder_decoder(self, col : str) -> dict:\n",
    "        \"\"\"\n",
    "        encoder, decoder 생성\n",
    "\n",
    "        Args:\n",
    "            col (str): 생성할 columns 명\n",
    "        Returns:\n",
    "            dict: 생성된 user encoder, decoder\n",
    "        \"\"\"\n",
    "\n",
    "        encoder = {}\n",
    "        decoder = {}\n",
    "        ids = self.df[col].unique()\n",
    "\n",
    "        for idx, _id in enumerate(ids):\n",
    "            encoder[_id] = idx\n",
    "            decoder[idx] = _id\n",
    "\n",
    "        return encoder, decoder\n",
    "    \n",
    "    def generate_sequence_data(self) -> dict:\n",
    "        \"\"\"\n",
    "        sequence_data 생성\n",
    "\n",
    "        Returns:\n",
    "            dict: train user sequence / valid user sequence\n",
    "        \"\"\"\n",
    "        users = defaultdict(list)\n",
    "        user_train = {}\n",
    "        user_valid = {}\n",
    "        for user, item, time in zip(self.df['user_idx'], self.df['item_idx'], self.df['time']):\n",
    "            users[user].append(item)\n",
    "        \n",
    "        for user in users:\n",
    "            np.random.seed(self.config.seed)\n",
    "\n",
    "            user_total = users[user]\n",
    "            valid = np.random.choice(user_total, size = self.config.valid_samples, replace = False).tolist()\n",
    "            train = list(set(user_total) - set(valid))\n",
    "\n",
    "            user_train[user] = train\n",
    "            user_valid[user] = valid # valid_samples 개수 만큼 검증에 활용 (현재 Task와 가장 유사하게)\n",
    "\n",
    "        return user_train, user_valid\n",
    "    \n",
    "    def generate_dok_matrix(self):\n",
    "        R_train = sp.dok_matrix((self.num_user, self.num_item), dtype=np.float32)\n",
    "        R_valid = sp.dok_matrix((self.num_user, self.num_item), dtype=np.float32)\n",
    "        R_total = sp.dok_matrix((self.num_user, self.num_item), dtype=np.float32)\n",
    "        user_list = self.exist_users\n",
    "        for user in user_list:\n",
    "            train_items = self.user_train[user]\n",
    "            valid_items = self.user_valid[user]\n",
    "            \n",
    "            for train_item in train_items:\n",
    "                R_train[user, train_item] = 1.0\n",
    "                R_total[user, train_item] = 1.0\n",
    "            \n",
    "            for valid_item in valid_items:\n",
    "                R_valid[user, valid_item] = 1.0\n",
    "                R_total[user, valid_item] = 1.0\n",
    "        \n",
    "        return R_train, R_valid, R_total\n",
    "\n",
    "    def generate_ngcf_adj_matrix(self):\n",
    "        adj_mat = sp.dok_matrix((self.num_user + self.num_item, self.num_user + self.num_item), dtype=np.float32)\n",
    "        adj_mat = adj_mat.tolil() # to_list\n",
    "        R = self.R_train.tolil()\n",
    "\n",
    "        adj_mat[:self.num_user, self.num_user:] = R\n",
    "        adj_mat[self.num_user:, :self.num_user] = R.T\n",
    "        adj_mat = adj_mat.todok() # to_dok_matrix\n",
    "\n",
    "        def normalized_adj_single(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "            d_inv = np.power(rowsum, -.5).flatten()  \n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "            norm_adj = d_mat_inv.dot(adj).dot(d_mat_inv)\n",
    "\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        ngcf_adj_matrix = normalized_adj_single(adj_mat)\n",
    "        return ngcf_adj_matrix.tocsr()\n",
    "\n",
    "    def sampling(self):\n",
    "        users = random.sample(self.exist_users, self.config.batch_size)\n",
    "\n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            pos_items = self.user_train[u]\n",
    "            pos_batch = random.sample(pos_items, num)\n",
    "            return pos_batch\n",
    "        \n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            neg_items = list(set(self.exist_items) - set(self.user_train[u]))\n",
    "            neg_batch = random.sample(neg_items, num)\n",
    "            return neg_batch\n",
    "        \n",
    "        pos_items, neg_items = [], []\n",
    "        for user in users:\n",
    "            pos_items += sample_pos_items_for_u(user, 1)\n",
    "            neg_items += sample_neg_items_for_u(user, 1)\n",
    "        \n",
    "        return users, pos_items, neg_items\n",
    "\n",
    "    def get_train_valid_data(self):\n",
    "        return self.user_train, self.user_valid\n",
    "\n",
    "    def get_R_data(self):\n",
    "        return self.R_train, self.R_valid, self.R_total\n",
    "\n",
    "    def get_ngcf_adj_matrix_data(self):\n",
    "        return self.ngcf_adj_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_dim, n_layers, reg, node_dropout, adj_mtx):\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize Class attributes\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.emb_dim = emb_dim\n",
    "        self.l = adj_mtx\n",
    "        self.graph = self._convert_sp_mat_to_sp_tensor(self.l)\n",
    "\n",
    "        self.reg = reg\n",
    "        self.n_layers = n_layers\n",
    "        self.node_dropout = node_dropout\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weight_dict = self._init_weights()\n",
    "        print(\"Weights initialized.\")\n",
    "\n",
    "    # initialize weights\n",
    "    def _init_weights(self):\n",
    "        print(\"Initializing weights...\")\n",
    "        weight_dict = nn.ParameterDict()\n",
    "\n",
    "        initializer = torch.nn.init.xavier_uniform_\n",
    "        \n",
    "        weight_dict['user_embedding'] = nn.Parameter(initializer(torch.empty(self.n_users, self.emb_dim).to(device)))\n",
    "        weight_dict['item_embedding'] = nn.Parameter(initializer(torch.empty(self.n_items, self.emb_dim).to(device)))\n",
    "           \n",
    "        return weight_dict\n",
    "\n",
    "    # convert sparse matrix into sparse PyTorch tensor\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        \"\"\"\n",
    "        Convert scipy sparse matrix to PyTorch sparse matrix\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        X = Adjacency matrix, scipy sparse matrix\n",
    "        \"\"\"\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        i = torch.LongTensor(np.mat([coo.row, coo.col]))\n",
    "        v = torch.FloatTensor(coo.data)\n",
    "        res = torch.sparse.FloatTensor(i, v, coo.shape).to(device)\n",
    "        return res\n",
    "\n",
    "    # apply node_dropout\n",
    "    def _droupout_sparse(self, X):\n",
    "        \"\"\"\n",
    "        Drop individual locations in X\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "        X = adjacency matrix (PyTorch sparse tensor)\n",
    "        dropout = fraction of nodes to drop\n",
    "        noise_shape = number of non non-zero entries of X\n",
    "        \"\"\"\n",
    "        node_dropout_mask = ((self.node_dropout) + torch.rand(X._nnz())).floor().bool().to(device)\n",
    "        i = X.coalesce().indices()\n",
    "        v = X.coalesce()._values()\n",
    "        i[:,node_dropout_mask] = 0\n",
    "        v[node_dropout_mask] = 0\n",
    "        X_dropout = torch.sparse.FloatTensor(i, v, X.shape).to(X.device)\n",
    "\n",
    "        return  X_dropout.mul(1/(1-self.node_dropout))\n",
    "\n",
    "    def forward(self, u, i, j):\n",
    "        \"\"\"\n",
    "        Computes the forward pass\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "        u = user\n",
    "        i = positive item (user interacted with item)\n",
    "        j = negative item (user did not interact with item)\n",
    "        \"\"\"\n",
    "        # apply drop-out mask\n",
    "        graph = self._droupout_sparse(self.graph) if self.node_dropout > 0 else self.graph\n",
    "        ego_embeddings = torch.cat([self.weight_dict['user_embedding'], self.weight_dict['item_embedding']], 0)\n",
    "        final_embeddings = [ego_embeddings]\n",
    "\n",
    "        for k in range(self.n_layers):\n",
    "            ego_embeddings = torch.sparse.mm(graph, final_embeddings[k])\n",
    "            final_embeddings.append(ego_embeddings)                                       \n",
    "\n",
    "        final_embeddings = torch.stack(final_embeddings, dim=1)\n",
    "        final_embeddings = torch.mean(final_embeddings, dim=1)\n",
    "        \n",
    "        u_final_embeddings, i_final_embeddings = final_embeddings.split([self.n_users, self.n_items], 0)\n",
    "\n",
    "        self.u_final_embeddings = nn.Parameter(u_final_embeddings)\n",
    "        self.i_final_embeddings = nn.Parameter(i_final_embeddings)\n",
    "        \n",
    "        # loss 계산\n",
    "        u_emb = u_final_embeddings[u] # user embeddings\n",
    "        p_emb = i_final_embeddings[i] # positive item embeddings\n",
    "        n_emb = i_final_embeddings[j] # negative item embeddings\n",
    "        \n",
    "        y_ui = torch.sum(torch.mul(u_emb, p_emb), dim = 1)                        \n",
    "        y_uj = torch.sum(torch.mul(u_emb, n_emb), dim = 1)\n",
    "        \n",
    "        log_prob = torch.mean(torch.log(torch.sigmoid(y_ui - y_uj))) \n",
    "        bpr_loss = -log_prob        \n",
    "        if self.reg > 0.:\n",
    "            l2norm = (torch.sum(u_emb**2)/2. + torch.sum(p_emb**2)/2. + torch.sum(n_emb**2)/2.) / u_emb.shape[0]\n",
    "            l2reg = self.reg * l2norm\n",
    "            bpr_loss += l2reg\n",
    "\n",
    "        \n",
    "        return bpr_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, make_graph_data_set, optimizer, n_batch):\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "    for step in range(1, n_batch + 1):\n",
    "        user, pos, neg = make_graph_data_set.sampling()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(user, pos, neg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_val += loss.item()\n",
    "    loss_val /= n_batch\n",
    "    return loss_val\n",
    "\n",
    "def split_matrix(X, n_splits=10):\n",
    "    splits = []\n",
    "    chunk_size = X.shape[0] // n_splits\n",
    "    for i in range(n_splits):\n",
    "        start = i * chunk_size\n",
    "        end = X.shape[0] if i == n_splits - 1 else (i + 1) * chunk_size\n",
    "        splits.append(X[start:end])\n",
    "    return splits\n",
    "\n",
    "def compute_ndcg_k(pred_items, test_items, test_indices, k):\n",
    "    \n",
    "    r = (test_items * pred_items).gather(1, test_indices)\n",
    "    f = torch.from_numpy(np.log2(np.arange(2, k+2))).float().to(device)\n",
    "    \n",
    "    dcg = (r[:, :k]/f).sum(1)                                               \n",
    "    dcg_max = (torch.sort(r, dim=1, descending=True)[0][:, :k]/f).sum(1)   \n",
    "    ndcg = dcg/dcg_max                                                     \n",
    "    \n",
    "    ndcg[torch.isnan(ndcg)] = 0\n",
    "    return ndcg\n",
    "\n",
    "def evaluate(u_emb, i_emb, Rtr, Rte, k = 10):\n",
    "\n",
    "    # split matrices\n",
    "    ue_splits = split_matrix(u_emb)\n",
    "    tr_splits = split_matrix(Rtr)\n",
    "    te_splits = split_matrix(Rte)\n",
    "\n",
    "    recall_k, ndcg_k= [], []\n",
    "    # compute results for split matrices\n",
    "    for ue_f, tr_f, te_f in zip(ue_splits, tr_splits, te_splits):\n",
    "\n",
    "        scores = torch.mm(ue_f, i_emb.t())\n",
    "\n",
    "        test_items = torch.from_numpy(te_f.todense()).float().to(device)\n",
    "        non_train_items = torch.from_numpy(1-(tr_f.todense())).float().to(device)\n",
    "        scores = scores * non_train_items\n",
    "\n",
    "        _, test_indices = torch.topk(scores, dim=1, k=k)\n",
    "        \n",
    "        pred_items = torch.zeros_like(scores).float()\n",
    "        pred_items.scatter_(dim=1, index=test_indices, src=torch.ones_like(test_indices).float().to(device))\n",
    "\n",
    "        topk_preds = torch.zeros_like(scores).float()\n",
    "        topk_preds.scatter_(dim=1, index=test_indices[:, :k], src=torch.ones_like(test_indices).float())\n",
    "        \n",
    "        TP = (test_items * topk_preds).sum(1)                      \n",
    "        rec = TP/test_items.sum(1)\n",
    "   \n",
    "        ndcg = compute_ndcg_k(pred_items, test_items, test_indices, k)\n",
    "\n",
    "        recall_k.append(rec)\n",
    "        ndcg_k.append(ndcg)\n",
    "\n",
    "    return torch.cat(ndcg_k).mean(), torch.cat(recall_k).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_graph_data_set = MakeGraphDataSet(config = config)\n",
    "ngcf_adj_matrix = make_graph_data_set.get_ngcf_adj_matrix_data()\n",
    "R_train, R_valid, R_total = make_graph_data_set.get_R_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing weights...\n",
      "Weights initialized.\n"
     ]
    }
   ],
   "source": [
    "model = LightGCN(\n",
    "    n_users = make_graph_data_set.num_user,\n",
    "    n_items = make_graph_data_set.num_item,\n",
    "    emb_dim = config.emb_dim,\n",
    "    n_layers = config.n_layers,\n",
    "    reg = config.reg,\n",
    "    node_dropout = config.node_dropout,\n",
    "    adj_mtx = ngcf_adj_matrix,\n",
    "    ).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   1| Train loss: 0.69310| NDCG@10: 0.15678| HIT@10: 0.03682: 100%|██████████| 1/1 [01:42<00:00, 102.55s/it]\n",
      "Epoch:   2| Train loss: 0.69277| NDCG@10: 0.30541| HIT@10: 0.08269: 100%|██████████| 1/1 [01:40<00:00, 100.12s/it]\n",
      "Epoch:   3| Train loss: 0.69145| NDCG@10: 0.31800| HIT@10: 0.08658: 100%|██████████| 1/1 [01:41<00:00, 101.38s/it]\n",
      "Epoch:   4| Train loss: 0.68792| NDCG@10: 0.32011| HIT@10: 0.08687: 100%|██████████| 1/1 [01:41<00:00, 101.42s/it]\n",
      "Epoch:   5| Train loss: 0.68066| NDCG@10: 0.32013| HIT@10: 0.08704: 100%|██████████| 1/1 [01:41<00:00, 101.72s/it]\n",
      "Epoch:   6| Train loss: 0.66885| NDCG@10: 0.32091| HIT@10: 0.08708: 100%|██████████| 1/1 [01:41<00:00, 101.79s/it]\n",
      "Epoch:   7| Train loss: 0.65215| NDCG@10: 0.32087| HIT@10: 0.08689: 100%|██████████| 1/1 [01:41<00:00, 101.27s/it]\n",
      "Epoch:   8| Train loss: 0.63180| NDCG@10: 0.31983| HIT@10: 0.08689: 100%|██████████| 1/1 [01:40<00:00, 100.42s/it]\n",
      "Epoch:   9| Train loss: 0.60826| NDCG@10: 0.32082| HIT@10: 0.08701: 100%|██████████| 1/1 [01:41<00:00, 101.05s/it]\n",
      "Epoch:  10| Train loss: 0.58273| NDCG@10: 0.31975| HIT@10: 0.08684: 100%|██████████| 1/1 [01:39<00:00, 99.82s/it]\n",
      "Epoch:  11| Train loss: 0.55678| NDCG@10: 0.31995| HIT@10: 0.08696: 100%|██████████| 1/1 [01:39<00:00, 99.61s/it]\n",
      "Epoch:  12| Train loss: 0.53323| NDCG@10: 0.31981| HIT@10: 0.08680: 100%|██████████| 1/1 [01:41<00:00, 101.10s/it]\n",
      "Epoch:  13| Train loss: 0.50830| NDCG@10: 0.32060| HIT@10: 0.08678: 100%|██████████| 1/1 [01:39<00:00, 99.65s/it]\n",
      "Epoch:  14| Train loss: 0.48486| NDCG@10: 0.31991| HIT@10: 0.08681: 100%|██████████| 1/1 [01:41<00:00, 101.21s/it]\n",
      "Epoch:  15| Train loss: 0.46363| NDCG@10: 0.32024| HIT@10: 0.08686: 100%|██████████| 1/1 [01:40<00:00, 100.94s/it]\n",
      "Epoch:  16| Train loss: 0.44281| NDCG@10: 0.31973| HIT@10: 0.08689: 100%|██████████| 1/1 [01:38<00:00, 98.99s/it]\n",
      "Epoch:  17| Train loss: 0.42846| NDCG@10: 0.32004| HIT@10: 0.08691: 100%|██████████| 1/1 [01:40<00:00, 100.96s/it]\n",
      "Epoch:  18| Train loss: 0.41375| NDCG@10: 0.31922| HIT@10: 0.08671: 100%|██████████| 1/1 [01:40<00:00, 100.54s/it]\n",
      "Epoch:  19| Train loss: 0.39913| NDCG@10: 0.32041| HIT@10: 0.08694: 100%|██████████| 1/1 [01:40<00:00, 100.95s/it]\n",
      "Epoch:  20| Train loss: 0.38704| NDCG@10: 0.31989| HIT@10: 0.08686: 100%|██████████| 1/1 [01:39<00:00, 99.48s/it]\n",
      "Epoch:  21| Train loss: 0.37599| NDCG@10: 0.32039| HIT@10: 0.08693: 100%|██████████| 1/1 [01:40<00:00, 100.51s/it]\n",
      "Epoch:  22| Train loss: 0.36758| NDCG@10: 0.31951| HIT@10: 0.08665: 100%|██████████| 1/1 [01:38<00:00, 98.46s/it]\n",
      "Epoch:  23| Train loss: 0.35395| NDCG@10: 0.32016| HIT@10: 0.08690: 100%|██████████| 1/1 [01:39<00:00, 99.37s/it]\n",
      "Epoch:  24| Train loss: 0.34636| NDCG@10: 0.32022| HIT@10: 0.08698: 100%|██████████| 1/1 [01:39<00:00, 99.51s/it]\n",
      "Epoch:  25| Train loss: 0.34088| NDCG@10: 0.31998| HIT@10: 0.08692: 100%|██████████| 1/1 [01:41<00:00, 101.22s/it]\n",
      "Epoch:  26| Train loss: 0.33516| NDCG@10: 0.31989| HIT@10: 0.08688: 100%|██████████| 1/1 [01:40<00:00, 100.43s/it]\n",
      "Epoch:  27| Train loss: 0.32985| NDCG@10: 0.31978| HIT@10: 0.08684: 100%|██████████| 1/1 [01:40<00:00, 100.26s/it]\n",
      "Epoch:  28| Train loss: 0.32470| NDCG@10: 0.31957| HIT@10: 0.08668: 100%|██████████| 1/1 [01:39<00:00, 99.25s/it]\n",
      "Epoch:  29| Train loss: 0.31629| NDCG@10: 0.31987| HIT@10: 0.08677: 100%|██████████| 1/1 [01:40<00:00, 100.76s/it]\n",
      "Epoch:  30| Train loss: 0.31788| NDCG@10: 0.32019| HIT@10: 0.08694: 100%|██████████| 1/1 [01:39<00:00, 99.91s/it]\n",
      "Epoch:  31| Train loss: 0.31772| NDCG@10: 0.31937| HIT@10: 0.08673: 100%|██████████| 1/1 [01:39<00:00, 99.80s/it]\n",
      "Epoch:  32| Train loss: 0.30668| NDCG@10: 0.32020| HIT@10: 0.08694: 100%|██████████| 1/1 [01:39<00:00, 99.29s/it]\n",
      "Epoch:  33| Train loss: 0.31015| NDCG@10: 0.31945| HIT@10: 0.08692: 100%|██████████| 1/1 [01:41<00:00, 101.80s/it]\n",
      "  0%|          | 0/1 [00:27<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/level2-movie-recommendation-level2-recsys-05/jupyter/LightGCN.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.181.67/opt/ml/level2-movie-recommendation-level2-recsys-05/jupyter/LightGCN.ipynb#ch0000025vscode-remote?line=2'>3</a>\u001b[0m tbar \u001b[39m=\u001b[39m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.181.67/opt/ml/level2-movie-recommendation-level2-recsys-05/jupyter/LightGCN.ipynb#ch0000025vscode-remote?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m tbar:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B115.85.181.67/opt/ml/level2-movie-recommendation-level2-recsys-05/jupyter/LightGCN.ipynb#ch0000025vscode-remote?line=4'>5</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.181.67/opt/ml/level2-movie-recommendation-level2-recsys-05/jupyter/LightGCN.ipynb#ch0000025vscode-remote?line=5'>6</a>\u001b[0m         model \u001b[39m=\u001b[39;49m model, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.181.67/opt/ml/level2-movie-recommendation-level2-recsys-05/jupyter/LightGCN.ipynb#ch0000025vscode-remote?line=6'>7</a>\u001b[0m         make_graph_data_set \u001b[39m=\u001b[39;49m make_graph_data_set, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.181.67/opt/ml/level2-movie-recommendation-level2-recsys-05/jupyter/LightGCN.ipynb#ch0000025vscode-remote?line=7'>8</a>\u001b[0m         optimizer \u001b[39m=\u001b[39;49m optimizer,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.181.67/opt/ml/level2-movie-recommendation-level2-recsys-05/jupyter/LightGCN.ipynb#ch0000025vscode-remote?line=8'>9</a>\u001b[0m         n_batch \u001b[39m=\u001b[39;49m config\u001b[39m.\u001b[39;49mn_batch,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B115.85.181.67/opt/ml/level2-movie-recommendation-level2-recsys-05/jupyter/LightGCN.ipynb#ch0000025vscode-remote?line=9'>10</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B115.85.181.67/opt/ml/level2-movie-recommendation-level2-recsys-05/jupyter/LightGCN.ipynb#ch0000025vscode-remote?line=10'>11</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B115.85.181.67/opt/ml/level2-movie-recommendation-level2-recsys-05/jupyter/LightGCN.ipynb#ch0000025vscode-remote?line=11'>12</a>\u001b[0m         ndcg, hit \u001b[39m=\u001b[39m evaluate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B115.85.181.67/opt/ml/level2-movie-recommendation-level2-recsys-05/jupyter/LightGCN.ipynb#ch0000025vscode-remote?line=12'>13</a>\u001b[0m             u_emb \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mu_final_embeddings\u001b[39m.\u001b[39mdetach(), \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B115.85.181.67/opt/ml/level2-movie-recommendation-level2-recsys-05/jupyter/LightGCN.ipynb#ch0000025vscode-remote?line=13'>14</a>\u001b[0m             i_emb \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mi_final_embeddings\u001b[39m.\u001b[39mdetach(), \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B115.85.181.67/opt/ml/level2-movie-recommendation-level2-recsys-05/jupyter/LightGCN.ipynb#ch0000025vscode-remote?line=16'>17</a>\u001b[0m             k \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B115.85.181.67/opt/ml/level2-movie-recommendation-level2-recsys-05/jupyter/LightGCN.ipynb#ch0000025vscode-remote?line=17'>18</a>\u001b[0m             )\n",
      "\u001b[1;32m/opt/ml/level2-movie-recommendation-level2-recsys-05/jupyter/LightGCN.ipynb Cell 12'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, make_graph_data_set, optimizer, n_batch)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.181.67/opt/ml/level2-movie-recommendation-level2-recsys-05/jupyter/LightGCN.ipynb#ch0000019vscode-remote?line=5'>6</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.181.67/opt/ml/level2-movie-recommendation-level2-recsys-05/jupyter/LightGCN.ipynb#ch0000019vscode-remote?line=6'>7</a>\u001b[0m loss \u001b[39m=\u001b[39m model(user, pos, neg)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B115.85.181.67/opt/ml/level2-movie-recommendation-level2-recsys-05/jupyter/LightGCN.ipynb#ch0000019vscode-remote?line=7'>8</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B115.85.181.67/opt/ml/level2-movie-recommendation-level2-recsys-05/jupyter/LightGCN.ipynb#ch0000019vscode-remote?line=8'>9</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B115.85.181.67/opt/ml/level2-movie-recommendation-level2-recsys-05/jupyter/LightGCN.ipynb#ch0000019vscode-remote?line=9'>10</a>\u001b[0m loss_val \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/_tensor.py?line=297'>298</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/_tensor.py?line=298'>299</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/_tensor.py?line=299'>300</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/_tensor.py?line=300'>301</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/_tensor.py?line=304'>305</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/_tensor.py?line=305'>306</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///opt/conda/lib/python3.8/site-packages/torch/_tensor.py?line=306'>307</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py?line=150'>151</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py?line=151'>152</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> <a href='file:///opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py?line=153'>154</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py?line=154'>155</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py?line=155'>156</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_hit = 0\n",
    "for epoch in range(1, config.num_epochs + 1):\n",
    "    tbar = tqdm(range(1))\n",
    "    for _ in tbar:\n",
    "        train_loss = train(\n",
    "            model = model, \n",
    "            make_graph_data_set = make_graph_data_set, \n",
    "            optimizer = optimizer,\n",
    "            n_batch = config.n_batch,\n",
    "            )\n",
    "        with torch.no_grad():\n",
    "            ndcg, hit = evaluate(\n",
    "                u_emb = model.u_final_embeddings.detach(), \n",
    "                i_emb = model.i_final_embeddings.detach(), \n",
    "                Rtr = R_train, \n",
    "                Rte = R_valid, \n",
    "                k = 10,\n",
    "                )\n",
    "\n",
    "        if best_hit < hit:\n",
    "            best_hit = hit\n",
    "            torch.save(model.state_dict(), os.path.join(config.model_path, config.model_name))\n",
    "\n",
    "        tbar.set_description(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(config.model_path, config.model_name)))\n",
    "submision = []\n",
    "with torch.no_grad():\n",
    "    k = 10\n",
    "    u_emb = model.u_final_embeddings.detach()\n",
    "    i_emb = model.i_final_embeddings.detach()\n",
    "\n",
    "    scores = torch.mm(u_emb, i_emb.t())\n",
    "\n",
    "    test_items = torch.from_numpy(R_total.todense()).float().to(device)\n",
    "    non_train_items = torch.from_numpy(1-(R_total.todense())).float().to(device)\n",
    "    scores = scores * non_train_items\n",
    "\n",
    "    _, test_indices = torch.topk(scores, dim=1, k=k)\n",
    "\n",
    "    pred_items = torch.zeros_like(scores).float()\n",
    "    pred_items.scatter_(dim=1, index=test_indices, src=torch.ones_like(test_indices).float().to(device))\n",
    "    pred_items = pred_items.argsort(dim = 1)\n",
    "    for user, item_list in enumerate(pred_items):\n",
    "        item_list = item_list[-10:].cpu().numpy().tolist()\n",
    "        for item in item_list:\n",
    "            submision.append(\n",
    "                {   \n",
    "                    'user' : make_graph_data_set.user_decoder[user],\n",
    "                    'item' : make_graph_data_set.item_decoder[item],\n",
    "                }\n",
    "            )\n",
    "\n",
    "submision = pd.DataFrame(submision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "submision.to_csv(os.path.join(config.submission_path, config.submission_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>8961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>4886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>1704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313595</th>\n",
       "      <td>138493</td>\n",
       "      <td>2762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313596</th>\n",
       "      <td>138493</td>\n",
       "      <td>4226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313597</th>\n",
       "      <td>138493</td>\n",
       "      <td>858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313598</th>\n",
       "      <td>138493</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313599</th>\n",
       "      <td>138493</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>313600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user  item\n",
       "0           11  8961\n",
       "1           11  4886\n",
       "2           11   858\n",
       "3           11  1704\n",
       "4           11    50\n",
       "...        ...   ...\n",
       "313595  138493  2762\n",
       "313596  138493  4226\n",
       "313597  138493   858\n",
       "313598  138493    50\n",
       "313599  138493    47\n",
       "\n",
       "[313600 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
