{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ug-br-pPu9vZ"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from box import Box\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "torch.set_printoptions(sci_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbRKDSg4u9vc"
   },
   "source": [
    "# 1. 학습 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEhK_fLIu9vd"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data_path' : \"/opt/ml/input/data/train\" , # 데이터 경로\n",
    "    'model_path' : \"../model\",\n",
    "\n",
    "\n",
    "    'submission_path' : \"../submission\",\n",
    "    'submission_name' : 'Confidence_Ensembel_submission.csv',\n",
    "\n",
    "    'candidate_item_num' : 50,\n",
    "    'valid_samples' : 10, # 검증에 사용할 sample 수\n",
    "    'seed' : 22,\n",
    "}\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "config = Box(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjDxy0fJu9vf"
   },
   "source": [
    "# 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W64BYWl0u9vg"
   },
   "outputs": [],
   "source": [
    "class MakeMatrixDataSet():\n",
    "    \"\"\"\n",
    "    MatrixDataSet 생성\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.df = pd.read_csv(os.path.join(self.config.data_path, 'train_ratings.csv'))\n",
    "        \n",
    "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('item')\n",
    "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('user')\n",
    "        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n",
    "\n",
    "        self.df['item_idx'] = self.df['item'].apply(lambda x : self.item_encoder[x])\n",
    "        self.df['user_idx'] = self.df['user'].apply(lambda x : self.user_encoder[x])\n",
    "\n",
    "        self.user_train, self.user_valid = self.generate_sequence_data()\n",
    "\n",
    "    def generate_encoder_decoder(self, col : str) -> dict:\n",
    "        \"\"\"\n",
    "        encoder, decoder 생성\n",
    "\n",
    "        Args:\n",
    "            col (str): 생성할 columns 명\n",
    "        Returns:\n",
    "            dict: 생성된 user encoder, decoder\n",
    "        \"\"\"\n",
    "\n",
    "        encoder = {}\n",
    "        decoder = {}\n",
    "        ids = self.df[col].unique()\n",
    "\n",
    "        for idx, _id in enumerate(ids):\n",
    "            encoder[_id] = idx\n",
    "            decoder[idx] = _id\n",
    "\n",
    "        return encoder, decoder\n",
    "    \n",
    "    def generate_sequence_data(self) -> dict:\n",
    "        \"\"\"\n",
    "        sequence_data 생성\n",
    "\n",
    "        Returns:\n",
    "            dict: train user sequence / valid user sequence\n",
    "        \"\"\"\n",
    "        users = defaultdict(list)\n",
    "        user_train = {}\n",
    "        user_valid = {}\n",
    "        for user, item, time in zip(self.df['user_idx'], self.df['item_idx'], self.df['time']):\n",
    "            users[user].append(item)\n",
    "        \n",
    "        for user in users:\n",
    "            np.random.seed(self.config.seed)\n",
    "\n",
    "            user_total = users[user]\n",
    "            valid = np.random.choice(user_total, size = self.config.valid_samples, replace = False).tolist()\n",
    "            train = list(set(user_total) - set(valid))\n",
    "\n",
    "            user_train[user] = train\n",
    "            user_valid[user] = valid # valid_samples 개수 만큼 검증에 활용 (현재 Task와 가장 유사하게)\n",
    "\n",
    "        return user_train, user_valid\n",
    "    \n",
    "    def get_train_valid_data(self):\n",
    "        return self.user_train, self.user_valid\n",
    "\n",
    "    def make_matrix(self, user_list, train = True):\n",
    "        \"\"\"\n",
    "        user_item_dict를 바탕으로 행렬 생성\n",
    "        \"\"\"\n",
    "        mat = torch.zeros(size = (user_list.size(0), self.num_item))\n",
    "        for idx, user in enumerate(user_list):\n",
    "            if train:\n",
    "                mat[idx, self.user_train[user.item()]] = 1\n",
    "            else:\n",
    "                mat[idx, self.user_train[user.item()] + self.user_valid[user.item()]] = 1\n",
    "        return mat\n",
    "\n",
    "    def make_sparse_matrix(self, test = False):\n",
    "        X = sp.dok_matrix((self.num_user, self.num_item), dtype=np.float32)\n",
    "        \n",
    "        for user in self.user_train.keys():\n",
    "            item_list = self.user_train[user]\n",
    "            X[user, item_list] = 1.0\n",
    "        \n",
    "        if test:\n",
    "            for user in self.user_valid.keys():\n",
    "                item_list = self.user_valid[user]\n",
    "                X[user, item_list] = 1.0\n",
    "\n",
    "        return X.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IldCGmY8u9vh"
   },
   "outputs": [],
   "source": [
    "class AEDataSet(Dataset):\n",
    "    def __init__(self, num_user):\n",
    "        self.num_user = num_user\n",
    "        self.users = [i for i in range(num_user)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_user\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        user = self.users[idx]\n",
    "        return torch.LongTensor([user])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysia457Su9vi"
   },
   "source": [
    "# 3. 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOSLIM():\n",
    "    def __init__(self, threshold = 3500, lambdaBB = 500, lambdaCC = 5000, rho = 100000, epochs = 40):\n",
    "        self.threshold = threshold\n",
    "        self.lambdaBB = lambdaBB\n",
    "        self.lambdaCC = lambdaCC\n",
    "        self.rho = rho\n",
    "        self.epochs = epochs\n",
    "    \n",
    "    def create_list_feature_pairs(self, XtX):\n",
    "        AA = np.triu(np.abs(XtX))\n",
    "        AA[ np.diag_indices(AA.shape[0]) ]=0.0\n",
    "        ii_pairs = np.where((AA > self.threshold) == True)\n",
    "        return ii_pairs\n",
    "    \n",
    "    def create_matrix_Z(self, ii_pairs, X):\n",
    "        MM = np.zeros( (len(ii_pairs[0]), X.shape[1]),    dtype=np.float)\n",
    "        MM[np.arange(MM.shape[0]) , ii_pairs[0]   ]=1.0\n",
    "        MM[np.arange(MM.shape[0]) , ii_pairs[1]   ]=1.0\n",
    "        CCmask = 1.0-MM\n",
    "        MM = MM.T\n",
    "        Z=  X.dot(MM)\n",
    "        Z= (Z == 2.0 )\n",
    "        Z=Z*1.0\n",
    "        return Z, CCmask\n",
    "\n",
    "    def train_higher(self, XtX, XtXdiag, ZtZ, ZtZdiag, CCmask, ZtX):\n",
    "        ii_diag=np.diag_indices(XtX.shape[0])\n",
    "        XtX[ii_diag] = XtXdiag + self.lambdaBB\n",
    "        PP = np.linalg.inv(XtX)\n",
    "        ii_diag_ZZ=np.diag_indices(ZtZ.shape[0])\n",
    "        ZtZ[ii_diag_ZZ] = ZtZdiag + self.lambdaCC + self.rho\n",
    "        QQ=np.linalg.inv(ZtZ)\n",
    "        CC = np.zeros( (ZtZ.shape[0], XtX.shape[0]),dtype=np.float )\n",
    "        DD = np.zeros( (ZtZ.shape[0], XtX.shape[0]),dtype=np.float )\n",
    "        UU = np.zeros( (ZtZ.shape[0], XtX.shape[0]),dtype=np.float )\n",
    "\n",
    "        for iter in range(self.epochs):\n",
    "            # learn BB\n",
    "            XtX[ii_diag] = XtXdiag\n",
    "            BB= PP.dot(XtX-ZtX.T.dot(CC))\n",
    "            gamma = np.diag(BB) / np.diag(PP)\n",
    "            BB-= PP * gamma\n",
    "            # learn CC\n",
    "            CC= QQ.dot(ZtX-ZtX.dot(BB) + self.rho * (DD-UU))\n",
    "            # learn DD\n",
    "            DD=  CC  * CCmask \n",
    "            #DD= np.maximum(0.0, DD) # if you want to enforce non-negative parameters\n",
    "            # learn UU (is Gamma in paper)\n",
    "            UU+= CC-DD\n",
    "        \n",
    "        return BB, DD\n",
    "\n",
    "    def fit(self, X):\n",
    "        XtX = X.T.dot(X)\n",
    "        XtXdiag = deepcopy(np.diag(XtX))\n",
    "        ii_pairs = self.create_list_feature_pairs(XtX)\n",
    "        Z, CCmask = self.create_matrix_Z(ii_pairs, X)\n",
    "\n",
    "        ZtZ = Z.T.dot(Z)\n",
    "        ZtZdiag = deepcopy(np.diag(ZtZ))\n",
    "\n",
    "        ZtX = Z.T.dot(X)\n",
    "\n",
    "        BB, CC = self.train_higher(XtX, XtXdiag, ZtZ, ZtZdiag, CCmask, ZtX)\n",
    "\n",
    "        self.pred = torch.from_numpy(X.dot(BB) + Z.dot(CC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdmmSlim():\n",
    "    def __init__(self, lambda_1=1, lambda_2=500, rho=10000, positive=True, n_iter=50, eps_rel=1e-4, eps_abs=1e-3, verbose=False):\n",
    "        self.lambda_1 = lambda_1\n",
    "        self.lambda_2 = lambda_2\n",
    "        self.rho = rho\n",
    "        self.positive = positive\n",
    "        self.n_iter = n_iter\n",
    "        self.eps_rel = eps_rel\n",
    "        self.eps_abs = eps_abs\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def soft_thresholding(self, B, Gamma):\n",
    "        if self.lambda_1 == 0:\n",
    "            if self.positive:\n",
    "                return np.abs(B)\n",
    "            else:\n",
    "                return B\n",
    "        else:\n",
    "            x = B + Gamma / self.rho\n",
    "            threshold = self.lambda_1 / self.rho\n",
    "            if self.positive:\n",
    "                return np.where(threshold < x, x - threshold, 0)\n",
    "            else:\n",
    "                return np.where(threshold < x, x - threshold,\n",
    "                                np.where(x < - threshold, x + threshold, 0))\n",
    "\n",
    "    def is_converged(self, B, C, C_old, Gamma):\n",
    "        B_norm = np.linalg.norm(B)\n",
    "        C_norm = np.linalg.norm(C)\n",
    "        Gamma_norm = np.linalg.norm(Gamma)\n",
    "\n",
    "        eps_primal = self.eps_abs * B.shape[0] - self.eps_rel * np.max([B_norm, C_norm])\n",
    "        eps_dual = self.eps_abs * B.shape[0] - self.eps_rel * Gamma_norm\n",
    "\n",
    "        R_primal_norm = np.linalg.norm(B - C)\n",
    "        R_dual_norm = np.linalg.norm(C  - C_old) * self.rho\n",
    "\n",
    "        converged = R_primal_norm < eps_primal and R_dual_norm < eps_dual\n",
    "        return converged\n",
    "\n",
    "    def fit(self, X):\n",
    "        XtX = X.T.dot(X)\n",
    "        if sp.issparse(XtX):\n",
    "            XtX = XtX.todense().A\n",
    "\n",
    "        if self.verbose:\n",
    "            print(' --- init')\n",
    "        identity_mat = np.identity(XtX.shape[0])\n",
    "        diags = identity_mat * (self.lambda_2 + self.rho)\n",
    "        P = np.linalg.inv(XtX + diags).astype(np.float32)\n",
    "        B_aux = P.dot(XtX)\n",
    "\n",
    "        Gamma = np.zeros_like(XtX, dtype=np.float32)\n",
    "        C = np.zeros_like(XtX, dtype=np.float32)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(' --- iteration start.')\n",
    "        for iter in range(self.n_iter):\n",
    "            if self.verbose:\n",
    "                print(f' --- iteration {iter+1}/{self.n_iter}')\n",
    "            C_old = C.copy()\n",
    "            B_tilde = B_aux + P.dot(self.rho * C - Gamma)\n",
    "            gamma = np.diag(B_tilde) / (np.diag(P) + 1e-8)\n",
    "            B = B_tilde - P * gamma\n",
    "            C = self.soft_thresholding(B, Gamma)\n",
    "            Gamma = Gamma + self.rho * (B - C)\n",
    "            if self.is_converged(B, C, C_old, Gamma):\n",
    "                if self.verbose:\n",
    "                    print(f' --- Converged. Stopped iteration.')\n",
    "                break\n",
    "\n",
    "        coef = C\n",
    "\n",
    "        self.pred = torch.from_numpy(X.dot(coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Container module for Multi-DAE.\n",
    "\n",
    "    Multi-DAE : Denoising Autoencoder with Multinomial Likelihood\n",
    "    See Variational Autoencoders for Collaborative Filtering\n",
    "    https://arxiv.org/abs/1802.05814\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p_dims, dropout_rate = 0.5):\n",
    "        super(MultiVAE, self).__init__()\n",
    "        self.p_dims = p_dims\n",
    "        self.q_dims = p_dims[::-1]\n",
    "\n",
    "        temp_q_dims = self.q_dims[:-1] + [self.q_dims[-1] * 2]\n",
    "\n",
    "        self.q_layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
    "            d_in, d_out in zip(temp_q_dims[:-1], temp_q_dims[1:])])\n",
    "\n",
    "        self.p_layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
    "            d_in, d_out in zip(self.p_dims[:-1], self.p_dims[1:])])\n",
    "\n",
    "        self.drop = nn.Dropout(dropout_rate)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        mu, logvar = self.encode(input)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "    def encode(self, input):\n",
    "        h = F.normalize(input)\n",
    "        h = self.drop(h)\n",
    "\n",
    "        for i, layer in enumerate(self.q_layers):\n",
    "            h = layer(h)\n",
    "            if i != len(self.q_layers) - 1:\n",
    "                h = F.tanh(h)\n",
    "            else:\n",
    "                mu = h[:, :self.q_dims[-1]]\n",
    "                logvar = h[:, self.q_dims[-1]:]\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h = z\n",
    "        for i, layer in enumerate(self.p_layers):\n",
    "            h = layer(h)\n",
    "            if i != len(self.p_layers) - 1:\n",
    "                h = F.tanh(h)\n",
    "        return h\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.q_layers:\n",
    "            # Xavier Initialization for weights\n",
    "            size = layer.weight.size()\n",
    "            fan_out = size[0]\n",
    "            fan_in = size[1]\n",
    "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
    "            layer.weight.data.normal_(0.0, std)\n",
    "\n",
    "            # Normal Initialization for Biases\n",
    "            layer.bias.data.normal_(0.0, 0.001)\n",
    "        \n",
    "        for layer in self.p_layers:\n",
    "            # Xavier Initialization for weights\n",
    "            size = layer.weight.size()\n",
    "            fan_out = size[0]\n",
    "            fan_in = size[1]\n",
    "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
    "            layer.weight.data.normal_(0.0, std)\n",
    "\n",
    "            # Normal Initialization for Biases\n",
    "            layer.bias.data.normal_(0.0, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Container module for Multi-DAE.\n",
    "\n",
    "    Multi-DAE : Denoising Autoencoder with Multinomial Likelihood\n",
    "    See Variational Autoencoders for Collaborative Filtering\n",
    "    https://arxiv.org/abs/1802.05814\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p_dims, dropout_rate = 0.5):\n",
    "        super(MultiDAE, self).__init__()\n",
    "        self.p_dims = p_dims\n",
    "        self.q_dims = p_dims[::-1]\n",
    "\n",
    "        self.dims = self.q_dims + self.p_dims[1:]\n",
    "        self.layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
    "            d_in, d_out in zip(self.dims[:-1], self.dims[1:])])\n",
    "        self.drop = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        h = F.normalize(input)\n",
    "        h = self.drop(h)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h = layer(h)\n",
    "            if i != len(self.layers) - 1:\n",
    "                h = F.tanh(h)\n",
    "        return h\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.layers:\n",
    "            # Xavier Initialization for weights\n",
    "            size = layer.weight.size()\n",
    "            fan_out = size[0]\n",
    "            fan_in = size[1]\n",
    "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
    "            layer.weight.data.normal_(0.0, std)\n",
    "\n",
    "            # Normal Initialization for Biases\n",
    "            layer.bias.data.normal_(0.0, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRec(nn.Module):\n",
    "    def __init__(self, num, num_factor):\n",
    "        super(AutoRec, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(num, num_factor),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(num_factor, num_factor // 2),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(num_factor // 2, num_factor),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(num_factor, num),\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, mat):\n",
    "        latent = self.encoder(mat)\n",
    "        recont_mat = self.decoder(latent)\n",
    "\n",
    "        return recont_mat\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.encoder:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                size = layer.weight.size()\n",
    "                fan_out = size[0]\n",
    "                fan_in = size[1]\n",
    "                std = np.sqrt(2.0/(fan_in + fan_out))\n",
    "                layer.weight.data.normal_(0.0, std)\n",
    "                layer.bias.data.normal_(0.0, 0.001)\n",
    "        \n",
    "        for layer in self.decoder:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                size = layer.weight.size()\n",
    "                fan_out = size[0]\n",
    "                fan_in = size[1]\n",
    "                std = np.sqrt(2.0/(fan_in + fan_out))\n",
    "                layer.weight.data.normal_(0.0, std)\n",
    "                layer.bias.data.normal_(0.0, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EASE():\n",
    "    def __init__(self, X, reg):\n",
    "        self.X = self._convert_sp_mat_to_sp_tensor(X)\n",
    "        self.reg = reg\n",
    "    \n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        \"\"\"\n",
    "        Convert scipy sparse matrix to PyTorch sparse matrix\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        X = Adjacency matrix, scipy sparse matrix\n",
    "        \"\"\"\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        i = torch.LongTensor(np.mat([coo.row, coo.col]))\n",
    "        v = torch.FloatTensor(coo.data)\n",
    "        res = torch.sparse.FloatTensor(i, v, coo.shape).to(device)\n",
    "        return res\n",
    "    \n",
    "    def fit(self):\n",
    "        '''\n",
    "\n",
    "        진짜 정말 간단한 식으로 모델을 만듬\n",
    "\n",
    "        '''\n",
    "        G = self.X.to_dense().t() @ self.X.to_dense()\n",
    "        diagIndices = torch.eye(G.shape[0]) == 1\n",
    "        G[diagIndices] += self.reg\n",
    "\n",
    "        P = G.inverse()\n",
    "        B = P / (-1 * P.diag())\n",
    "        B[diagIndices] = 0\n",
    "\n",
    "        self.pred = self.X.to_dense() @ B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    return x.mul(torch.sigmoid(x))\n",
    "\n",
    "def log_norm_pdf(x, mu, logvar):\n",
    "    return -0.5*(logvar + np.log(2 * np.pi) + (x - mu).pow(2) / logvar.exp())\n",
    "\n",
    "class CompositePrior(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, input_dim, mixture_weights=[3/20, 3/4, 1/10]):\n",
    "        super(CompositePrior, self).__init__()\n",
    "        \n",
    "        self.mixture_weights = mixture_weights\n",
    "        \n",
    "        self.mu_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
    "        self.mu_prior.data.fill_(0)\n",
    "        \n",
    "        self.logvar_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
    "        self.logvar_prior.data.fill_(0)\n",
    "        \n",
    "        self.logvar_uniform_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
    "        self.logvar_uniform_prior.data.fill_(10)\n",
    "        \n",
    "        self.encoder_old = Encoder(hidden_dim, latent_dim, input_dim)\n",
    "        self.encoder_old.requires_grad_(False)\n",
    "        \n",
    "    def forward(self, x, z):\n",
    "\n",
    "        post_mu, post_logvar = self.encoder_old(x, dropout_rate = 0)\n",
    "\n",
    "        stnd_prior = log_norm_pdf(z, self.mu_prior, self.logvar_prior)\n",
    "        post_prior = log_norm_pdf(z, post_mu, post_logvar)\n",
    "        unif_prior = log_norm_pdf(z, self.mu_prior, self.logvar_uniform_prior)\n",
    "        \n",
    "        gaussians = [stnd_prior, post_prior, unif_prior]\n",
    "        gaussians = [g.add(np.log(w)) for g, w in zip(gaussians, self.mixture_weights)]\n",
    "\n",
    "        density_per_gaussian = torch.stack(gaussians, dim=-1)\n",
    "\n",
    "        return torch.logsumexp(density_per_gaussian, dim=-1)\n",
    "\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, input_dim, eps=1e-1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln3 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln4 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln5 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "    def forward(self, x, dropout_rate):\n",
    "        norm = x.pow(2).sum(dim=-1).sqrt()\n",
    "        x = x / norm[:, None]\n",
    "    \n",
    "        x = F.dropout(x, p=dropout_rate, training=self.training)\n",
    "        \n",
    "        h1 = self.ln1(swish(self.fc1(x)))\n",
    "        h2 = self.ln2(swish(self.fc2(h1) + h1))\n",
    "        h3 = self.ln3(swish(self.fc3(h2) + h1 + h2))\n",
    "        h4 = self.ln4(swish(self.fc4(h3) + h1 + h2 + h3))\n",
    "        h5 = self.ln5(swish(self.fc5(h4) + h1 + h2 + h3 + h4))\n",
    "        return self.fc_mu(h5), self.fc_logvar(h5)\n",
    "\n",
    "\n",
    "class RecVAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim = 600, latent_dim = 200):\n",
    "        super(RecVAE, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(hidden_dim, latent_dim, input_dim)\n",
    "        self.prior = CompositePrior(hidden_dim, latent_dim, input_dim)\n",
    "        self.decoder = nn.Linear(latent_dim, input_dim)\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5*logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, user_ratings, beta=None, gamma=0.0005, dropout_rate=0.7, calculate_loss=True):\n",
    "        mu, logvar = self.encoder(user_ratings, dropout_rate=dropout_rate)    \n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_pred = self.decoder(z)\n",
    "\n",
    "        if calculate_loss:\n",
    "            if gamma:\n",
    "                norm = user_ratings.sum(dim=-1)\n",
    "                kl_weight = gamma * norm\n",
    "            elif beta:\n",
    "                kl_weight = beta\n",
    "\n",
    "            mll = (F.log_softmax(x_pred, dim=-1) * user_ratings).sum(dim=-1).mean()\n",
    "            kld = (log_norm_pdf(z, mu, logvar) - self.prior(user_ratings, z)).sum(dim=-1).mul(kl_weight).mean()\n",
    "            negative_elbo = -(mll - kld)\n",
    "            \n",
    "            return (mll, kld), negative_elbo\n",
    "            \n",
    "        else:\n",
    "            return x_pred\n",
    "\n",
    "    def update_prior(self):\n",
    "        self.prior.encoder_old.load_state_dict(deepcopy(self.encoder.state_dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwSexh43u9vk"
   },
   "source": [
    "# 4. 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ndcg(pred_list, true_list):\n",
    "    idcg = sum((1 / np.log2(rank + 2) for rank in range(1, len(pred_list))))\n",
    "    dcg = 0\n",
    "    for rank, pred in enumerate(pred_list):\n",
    "        if pred in true_list:\n",
    "            dcg += 1 / np.log2(rank + 2)\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg\n",
    "\n",
    "# hit == recall == precision\n",
    "def get_hit(pred_list, true_list):\n",
    "    hit_list = set(true_list) & set(pred_list)\n",
    "    hit = len(hit_list) / len(true_list)\n",
    "    return hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_ensemble_df(User_EASE, Item_EASE, AdmmSlim, HOSLIM, RecVAE, MultiVAE, MultiDAE, AutoRec, X, candidate_cnt):\n",
    "    \n",
    "    weighted_ensemble_df = {}\n",
    "\n",
    "    RecVAE.eval()\n",
    "    MultiVAE.eval()\n",
    "    MultiDAE.eval()\n",
    "    AutoRec.eval()\n",
    "\n",
    "    mat = torch.from_numpy(X)\n",
    "    \n",
    "    User_EASE_recon_mat = User_EASE.pred.cpu()\n",
    "    User_EASE_recon_mat[mat == 1] = -np.inf\n",
    "    User_EASE_rec_list = User_EASE_recon_mat.argsort(dim = 1)\n",
    "\n",
    "    Item_EASE_recon_mat = Item_EASE.pred.T.cpu()\n",
    "    Item_EASE_recon_mat[mat == 1] = -np.inf\n",
    "    Item_EASE_rec_list = Item_EASE_recon_mat.argsort(dim = 1)\n",
    "\n",
    "    AdmmSlim_recon_mat = AdmmSlim.pred.cpu()\n",
    "    AdmmSlim_recon_mat[mat == 1] = -np.inf\n",
    "    AdmmSlim_rec_list = AdmmSlim_recon_mat.argsort(dim = 1)\n",
    "\n",
    "    HOSLIM_recon_mat = HOSLIM.pred.cpu()\n",
    "    HOSLIM_recon_mat[mat == 1] = -np.inf\n",
    "    HOSLIM_rec_list = HOSLIM_recon_mat.argsort(dim = 1)\n",
    "\n",
    "    RecVAE_recon_mat = RecVAE(mat.to(device), calculate_loss = False).cpu().detach()\n",
    "    RecVAE_recon_mat[mat == 1] = -np.inf\n",
    "    RecVAE_rec_list = RecVAE_recon_mat.argsort(dim = 1)\n",
    "\n",
    "    AutoRec_recon_mat = AutoRec(mat.to(device)).cpu().detach()\n",
    "    AutoRec_recon_mat[mat == 1] = -np.inf\n",
    "    AutoRec_rec_list = AutoRec_recon_mat.argsort(dim = 1)\n",
    "\n",
    "    MultiDAE_recon_mat = MultiDAE(mat.to(device)).cpu().detach()\n",
    "    MultiDAE_recon_mat[mat == 1] = -np.inf\n",
    "    MultiDAE_rec_list = MultiDAE_recon_mat.argsort(dim = 1)\n",
    "\n",
    "    MultiVAE_recon_mat, _, _ = MultiVAE(mat.to(device))\n",
    "    MultiVAE_recon_mat = MultiVAE_recon_mat.cpu().detach()\n",
    "    MultiVAE_recon_mat[mat == 1] = -np.inf\n",
    "    MultiVAE_rec_list = MultiVAE_recon_mat.argsort(dim = 1)\n",
    "\n",
    "    score_li = np.array([1/np.log2(rank + 2) for rank in range(0, candidate_cnt)])\n",
    "\n",
    "    for user, (User_EASE_rec, Item_EASE_rec, AdmmSlim_rec, HOSLIM_rec, RecVAE_rec, AutoRec_rec, MultiDAE_rec, MultiVAE_rec) in tqdm(enumerate(zip(User_EASE_rec_list, Item_EASE_rec_list, AdmmSlim_rec_list, HOSLIM_rec_list, RecVAE_rec_list, AutoRec_rec_list, MultiDAE_rec_list, MultiVAE_rec_list))):\n",
    "\n",
    "        # ranking\n",
    "        User_EASE_rec = User_EASE_rec[-candidate_cnt:].cpu().numpy().tolist()[::-1]\n",
    "        Item_EASE_rec = Item_EASE_rec[-candidate_cnt:].cpu().numpy().tolist()[::-1]\n",
    "        AdmmSlim_rec = AdmmSlim_rec[-candidate_cnt:].cpu().numpy().tolist()[::-1]\n",
    "        HOSLIM_rec = HOSLIM_rec[-candidate_cnt:].cpu().numpy().tolist()[::-1]\n",
    "        RecVAE_rec = RecVAE_rec[-candidate_cnt:].cpu().numpy().tolist()[::-1]\n",
    "        AutoRec_rec = AutoRec_rec[-candidate_cnt:].cpu().numpy().tolist()[::-1]\n",
    "        MultiDAE_rec = MultiDAE_rec[-candidate_cnt:].cpu().numpy().tolist()[::-1]\n",
    "        MultiVAE_rec = MultiVAE_rec[-candidate_cnt:].cpu().numpy().tolist()[::-1]\n",
    "\n",
    "        all_rec = list(set(User_EASE_rec + Item_EASE_rec + AdmmSlim_rec + HOSLIM_rec + RecVAE_rec + AutoRec_rec + MultiDAE_rec + MultiVAE_rec))\n",
    "\n",
    "        rec_df = pd.DataFrame(index = all_rec)\n",
    "        rec_df.loc[User_EASE_rec, 'User_EASE_rec_score'] = score_li\n",
    "        rec_df.loc[Item_EASE_rec, 'Item_EASE_rec_score'] = score_li\n",
    "        rec_df.loc[AdmmSlim_rec, 'AdmmSlim_rec_score'] = score_li\n",
    "        rec_df.loc[HOSLIM_rec, 'HOSLIM_rec_score'] = score_li\n",
    "        rec_df.loc[RecVAE_rec, 'RecVAE_rec_score'] = score_li\n",
    "        rec_df.loc[AutoRec_rec, 'AutoRec_rec_score'] = score_li\n",
    "        rec_df.loc[MultiDAE_rec, 'MultiDAE_rec_score'] = score_li\n",
    "        rec_df.loc[MultiVAE_rec, 'MultiVAE_rec_score'] = score_li\n",
    "\n",
    "        weighted_ensemble_df[user] = rec_df\n",
    "\n",
    "    return weighted_ensemble_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gupkaJHMslCi"
   },
   "source": [
    "# 5. 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-1. 모델 init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_matrix_data_set = MakeMatrixDataSet(config = config)\n",
    "user_train, user_valid = make_matrix_data_set.get_train_valid_data()\n",
    "X = make_matrix_data_set.make_sparse_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoslim = HOSLIM(threshold = 3500, lambdaBB = 500, lambdaCC = 10000, rho = 50000)\n",
    "hoslim.fit(X = X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admm_slim = AdmmSlim(lambda_2 = 1, rho = 1000)\n",
    "admm_slim.fit(X = X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ease = EASE(X = X, reg = 750)\n",
    "user_ease.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ease = EASE(X = X.T, reg = 4400)\n",
    "item_ease.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_vae = RecVAE(\n",
    "    input_dim = make_matrix_data_set.num_item,).to(device)\n",
    "\n",
    "rec_vae.load_state_dict(torch.load(os.path.join(config.model_path, 'RecVAE_v3.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_rec = AutoRec(\n",
    "    num = make_matrix_data_set.num_item, \n",
    "    num_factor = 64).to(device)\n",
    "\n",
    "auto_rec.load_state_dict(torch.load(os.path.join(config.model_path, 'AutoRec_v1.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_dae = MultiDAE(\n",
    "    p_dims = [200, 600] + [make_matrix_data_set.num_item], \n",
    "    dropout_rate = 0.5).to(device)\n",
    "\n",
    "multi_dae.load_state_dict(torch.load(os.path.join(config.model_path, 'Multi-DAE_v2.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_vae = MultiVAE(\n",
    "    p_dims = [200, 600] + [make_matrix_data_set.num_item], \n",
    "    dropout_rate = 0.7).to(device)\n",
    "\n",
    "multi_vae.load_state_dict(torch.load(os.path.join(config.model_path, 'Multi-VAE_v2.pt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-4. get_weighted_ensemble_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_ensemble_df = get_weighted_ensemble_df(\n",
    "    User_EASE = user_ease, \n",
    "    Item_EASE = item_ease, \n",
    "    AdmmSlim = admm_slim, \n",
    "    HOSLIM = hoslim, \n",
    "    RecVAE = rec_vae, \n",
    "    MultiVAE = multi_vae, \n",
    "    MultiDAE = multi_dae, \n",
    "    AutoRec = auto_rec,\n",
    "    X = X.todense(),\n",
    "    candidate_cnt = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del hoslim, admm_slim, user_ease, item_ease, rec_vae, auto_rec, multi_dae, multi_vae, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = weighted_ensemble_df.keys()\n",
    "cols = ['HOSLIM_rec_score', 'User_EASE_rec_score', 'AdmmSlim_rec_score', 'Item_EASE_rec_score', 'RecVAE_rec_score', 'MultiVAE_rec_score', 'MultiDAE_rec_score']\n",
    "\n",
    "train_x_df = []\n",
    "train_y_df = []\n",
    "\n",
    "for user in tqdm(users):\n",
    "    train_rec_df = weighted_ensemble_df[user].copy()\n",
    "    train_target_rec = user_valid[user]\n",
    "    train_rec_df = train_rec_df.fillna(train_rec_df.min().min())\n",
    "    in_train_target_rec = list(set(train_rec_df.index.tolist()) & set(train_target_rec))\n",
    "    train_rec_df.loc[in_train_target_rec, 'target'] = 1\n",
    "    train_rec_df = train_rec_df.fillna(0)\n",
    "    train_x = train_rec_df[cols].values\n",
    "    train_y = train_rec_df['target'].values\n",
    "\n",
    "    train_x_df.append(train_x)\n",
    "    train_y_df.append(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "train_x = np.concatenate(train_x_df)\n",
    "train_y = np.concatenate(train_y_df)\n",
    "\n",
    "clf = LinearRegression(fit_intercept = False, positive = True, normalize = False)\n",
    "clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 1 - clf.coef_.max()\n",
    "clf.coef_ + val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['HOSLIM_rec_score', 'User_EASE_rec_score', 'AdmmSlim_rec_score', 'Item_EASE_rec_score', 'RecVAE_rec_score', 'MultiVAE_rec_score', 'MultiDAE_rec_score']\n",
    "weightes = [0.11157284, 0.00380821, 0.11187069, 0.0549102 , 0.07556451, 0.        , 0.        ]\n",
    "users = weighted_ensemble_df.keys()\n",
    "NDCG = 0\n",
    "HIT = 0\n",
    "\n",
    "for user in tqdm(users):\n",
    "    uv = user_valid[user]\n",
    "    df = weighted_ensemble_df[user].copy()\n",
    "    df = df.fillna(df[cols].min().min())\n",
    "\n",
    "    for c, w in zip(cols, weightes):\n",
    "        df[c] = df[c] * w\n",
    "    \n",
    "    df['total_score'] = df[cols].sum(axis = 1)\n",
    "    df = df.sort_values('total_score', ascending = False)\n",
    "    up = df.index.tolist()[:10]\n",
    "\n",
    "    NDCG += get_ndcg(pred_list = up, true_list = uv)\n",
    "    HIT += get_hit(pred_list = up, true_list = uv)\n",
    "\n",
    "NDCG /= len(users)\n",
    "HIT /= len(users)\n",
    "\n",
    "print(f'cols : {cols}| weighte : {weightes}| NDCG@10: {NDCG:.5f}| HIT@10: {HIT:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "users = weighted_ensemble_df.keys()\n",
    "cols = ['HOSLIM_rec_score', 'User_EASE_rec_score', 'AdmmSlim_rec_score', 'Item_EASE_rec_score', 'RecVAE_rec_score', 'MultiVAE_rec_score', 'MultiDAE_rec_score']\n",
    "NDCG = 0\n",
    "HIT = 0\n",
    "\n",
    "for user in tqdm(users):\n",
    "    train_rec_df = weighted_ensemble_df[user].copy()\n",
    "    train_target_rec = user_valid[user]\n",
    "    train_rec_df = train_rec_df.fillna(train_rec_df.min().min())\n",
    "    in_train_target_rec = list(set(train_rec_df.index.tolist()) & set(train_target_rec))\n",
    "\n",
    "    train_rec_df.loc[in_train_target_rec, 'target'] = 1\n",
    "    train_rec_df = train_rec_df.fillna(0)\n",
    "\n",
    "    clf = LinearRegression(fit_intercept = False, positive = True, normalize = False)\n",
    "\n",
    "    train_x = train_rec_df[cols].values\n",
    "    train_y = train_rec_df['target'].values\n",
    "\n",
    "    clf.fit(train_x, train_y)\n",
    "\n",
    "    train_rec_df['probability']= clf.predict(train_x)\n",
    "    train_rec_df = train_rec_df.sort_values('probability', ascending = False)\n",
    "    up = train_rec_df.index.tolist()[:10]\n",
    "\n",
    "    NDCG += get_ndcg(pred_list = up, true_list = train_target_rec)\n",
    "    HIT += get_hit(pred_list = up, true_list = train_target_rec)\n",
    "\n",
    "NDCG /= len(users)\n",
    "HIT /= len(users)\n",
    "\n",
    "print(f'cols : {cols}| NDCG@10: {NDCG:.5f}| HIT@10: {HIT:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def LinearRegression_weighted_ensemble(\n",
    "    weighted_ensemble_df, \n",
    "    user_valid, \n",
    "    User_EASE, \n",
    "    Item_EASE, \n",
    "    AdmmSlim, \n",
    "    HOSLIM, \n",
    "    RecVAE, \n",
    "    MultiVAE, \n",
    "    MultiDAE, \n",
    "    AutoRec, \n",
    "    X, \n",
    "    candidate_cnt, \n",
    "    cols):\n",
    "    \n",
    "    user2rec = {}\n",
    "\n",
    "    RecVAE.eval()\n",
    "    MultiVAE.eval()\n",
    "    MultiDAE.eval()\n",
    "    AutoRec.eval()\n",
    "\n",
    "    mat = torch.from_numpy(X)\n",
    "    \n",
    "    User_EASE_recon_mat = User_EASE.pred.cpu()\n",
    "    User_EASE_recon_mat[mat == 1] = -np.inf\n",
    "    User_EASE_rec_list = User_EASE_recon_mat.argsort(dim = 1)\n",
    "\n",
    "    Item_EASE_recon_mat = Item_EASE.pred.T.cpu()\n",
    "    Item_EASE_recon_mat[mat == 1] = -np.inf\n",
    "    Item_EASE_rec_list = Item_EASE_recon_mat.argsort(dim = 1)\n",
    "\n",
    "    AdmmSlim_recon_mat = AdmmSlim.pred.cpu()\n",
    "    AdmmSlim_recon_mat[mat == 1] = -np.inf\n",
    "    AdmmSlim_rec_list = AdmmSlim_recon_mat.argsort(dim = 1)\n",
    "\n",
    "    HOSLIM_recon_mat = HOSLIM.pred.cpu()\n",
    "    HOSLIM_recon_mat[mat == 1] = -np.inf\n",
    "    HOSLIM_rec_list = HOSLIM_recon_mat.argsort(dim = 1)\n",
    "\n",
    "    RecVAE_recon_mat = RecVAE(mat.to(device), calculate_loss = False).cpu().detach()\n",
    "    RecVAE_recon_mat[mat == 1] = -np.inf\n",
    "    RecVAE_rec_list = RecVAE_recon_mat.argsort(dim = 1)\n",
    "\n",
    "    AutoRec_recon_mat = AutoRec(mat.to(device)).cpu().detach()\n",
    "    AutoRec_recon_mat[mat == 1] = -np.inf\n",
    "    AutoRec_rec_list = AutoRec_recon_mat.argsort(dim = 1)\n",
    "\n",
    "    MultiDAE_recon_mat = MultiDAE(mat.to(device)).cpu().detach()\n",
    "    MultiDAE_recon_mat[mat == 1] = -np.inf\n",
    "    MultiDAE_rec_list = MultiDAE_recon_mat.argsort(dim = 1)\n",
    "\n",
    "    MultiVAE_recon_mat, _, _ = MultiVAE(mat.to(device))\n",
    "    MultiVAE_recon_mat = MultiVAE_recon_mat.cpu().detach()\n",
    "    MultiVAE_recon_mat[mat == 1] = -np.inf\n",
    "    MultiVAE_rec_list = MultiVAE_recon_mat.argsort(dim = 1)\n",
    "\n",
    "    score_li = np.array([1/np.log2(rank + 2) for rank in range(0, candidate_cnt)])\n",
    "\n",
    "    for user, (User_EASE_rec, Item_EASE_rec, AdmmSlim_rec, HOSLIM_rec, RecVAE_rec, AutoRec_rec, MultiDAE_rec, MultiVAE_rec) in tqdm(enumerate(zip(User_EASE_rec_list, Item_EASE_rec_list, AdmmSlim_rec_list, HOSLIM_rec_list, RecVAE_rec_list, AutoRec_rec_list, MultiDAE_rec_list, MultiVAE_rec_list))):\n",
    "        \n",
    "        train_rec_df = weighted_ensemble_df[user].copy()\n",
    "        train_target_rec = user_valid[user]\n",
    "        train_rec_df = train_rec_df.fillna(train_rec_df.min().min())\n",
    "        in_train_target_rec = list(set(train_rec_df.index.tolist()) & set(train_target_rec))\n",
    "\n",
    "        train_rec_df.loc[in_train_target_rec, 'target'] = 1\n",
    "        train_rec_df = train_rec_df.fillna(0)\n",
    "        \n",
    "        clf = LinearRegression(fit_intercept = False, positive = True, normalize = False)\n",
    "\n",
    "        train_x = train_rec_df[cols].values\n",
    "        train_y = train_rec_df['target'].values\n",
    "\n",
    "        clf.fit(train_x, train_y)\n",
    "\n",
    "        # ranking\n",
    "        User_EASE_rec = User_EASE_rec[-candidate_cnt:].cpu().numpy().tolist()[::-1]\n",
    "        Item_EASE_rec = Item_EASE_rec[-candidate_cnt:].cpu().numpy().tolist()[::-1]\n",
    "        AdmmSlim_rec = AdmmSlim_rec[-candidate_cnt:].cpu().numpy().tolist()[::-1]\n",
    "        HOSLIM_rec = HOSLIM_rec[-candidate_cnt:].cpu().numpy().tolist()[::-1]\n",
    "        RecVAE_rec = RecVAE_rec[-candidate_cnt:].cpu().numpy().tolist()[::-1]\n",
    "        AutoRec_rec = AutoRec_rec[-candidate_cnt:].cpu().numpy().tolist()[::-1]\n",
    "        MultiDAE_rec = MultiDAE_rec[-candidate_cnt:].cpu().numpy().tolist()[::-1]\n",
    "        MultiVAE_rec = MultiVAE_rec[-candidate_cnt:].cpu().numpy().tolist()[::-1]\n",
    "\n",
    "        all_rec = list(set(User_EASE_rec + Item_EASE_rec + AdmmSlim_rec + HOSLIM_rec + RecVAE_rec + AutoRec_rec + MultiDAE_rec + MultiVAE_rec))\n",
    "\n",
    "        rec_df = pd.DataFrame(index = all_rec)\n",
    "        rec_df.loc[User_EASE_rec, 'User_EASE_rec_score'] = score_li\n",
    "        rec_df.loc[Item_EASE_rec, 'Item_EASE_rec_score'] = score_li\n",
    "        rec_df.loc[AdmmSlim_rec, 'AdmmSlim_rec_score'] = score_li\n",
    "        rec_df.loc[HOSLIM_rec, 'HOSLIM_rec_score'] = score_li\n",
    "        rec_df.loc[RecVAE_rec, 'RecVAE_rec_score'] = score_li\n",
    "        rec_df.loc[AutoRec_rec, 'AutoRec_rec_score'] = score_li\n",
    "        rec_df.loc[MultiDAE_rec, 'MultiDAE_rec_score'] = score_li\n",
    "        rec_df.loc[MultiVAE_rec, 'MultiVAE_rec_score'] = score_li\n",
    "        rec_df = rec_df.fillna(rec_df.min().min())\n",
    "\n",
    "        test_x = rec_df[cols].values\n",
    "        rec_df['probability']= clf.predict(test_x)\n",
    "        rec_df = rec_df.sort_values('probability', ascending = False)\n",
    "        up = rec_df.index.tolist()[:10]\n",
    "\n",
    "        user2rec[user] = up\n",
    "\n",
    "    return user2rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_matrix_data_set = MakeMatrixDataSet(config = config)\n",
    "X_test = make_matrix_data_set.make_sparse_matrix(test = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoslim = HOSLIM(threshold = 3500, lambdaBB = 500, lambdaCC = 10000, rho = 50000)\n",
    "hoslim.fit(X = X_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admm_slim = AdmmSlim(lambda_2 = 1, rho = 1000)\n",
    "admm_slim.fit(X = X_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ease = EASE(X = X_test, reg = 750)\n",
    "user_ease.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ease = EASE(X = X_test.T, reg = 4400)\n",
    "item_ease.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_vae = RecVAE(\n",
    "    input_dim = make_matrix_data_set.num_item,).to(device)\n",
    "\n",
    "rec_vae.load_state_dict(torch.load(os.path.join(config.model_path, 'RecVAE_v3.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_rec = AutoRec(\n",
    "    num = make_matrix_data_set.num_item, \n",
    "    num_factor = 64).to(device)\n",
    "\n",
    "auto_rec.load_state_dict(torch.load(os.path.join(config.model_path, 'AutoRec_v1.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_dae = MultiDAE(\n",
    "    p_dims = [200, 600] + [make_matrix_data_set.num_item], \n",
    "    dropout_rate = 0.5).to(device)\n",
    "\n",
    "multi_dae.load_state_dict(torch.load(os.path.join(config.model_path, 'Multi-DAE_v2.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_vae = MultiVAE(\n",
    "    p_dims = [200, 600] + [make_matrix_data_set.num_item], \n",
    "    dropout_rate = 0.7).to(device)\n",
    "\n",
    "multi_vae.load_state_dict(torch.load(os.path.join(config.model_path, 'Multi-VAE_v2.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user2rec_list = LinearRegression_weighted_ensemble(\n",
    "    weighted_ensemble_df = weighted_ensemble_df, \n",
    "    user_valid = user_valid, \n",
    "    User_EASE = user_ease, \n",
    "    Item_EASE = item_ease, \n",
    "    AdmmSlim = admm_slim, \n",
    "    HOSLIM = hoslim, \n",
    "    RecVAE = rec_vae, \n",
    "    MultiVAE = multi_vae, \n",
    "    MultiDAE = multi_dae, \n",
    "    AutoRec = auto_rec, \n",
    "    X = X_test.todense(), \n",
    "    candidate_cnt = 10, \n",
    "    cols = ['HOSLIM_rec_score', 'User_EASE_rec_score', 'AdmmSlim_rec_score', 'Item_EASE_rec_score', 'RecVAE_rec_score', 'MultiVAE_rec_score', 'MultiDAE_rec_score'])\n",
    "\n",
    "submision = []\n",
    "users = [i for i in range(0, make_matrix_data_set.num_user)]\n",
    "for user in users:\n",
    "    rec_item_list = user2rec_list[user]\n",
    "    for item in rec_item_list:\n",
    "        submision.append(\n",
    "            {   \n",
    "                'user' : make_matrix_data_set.user_decoder[user],\n",
    "                'item' : make_matrix_data_set.item_decoder[item],\n",
    "            }\n",
    "        )\n",
    "\n",
    "submision = pd.DataFrame(submision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submision.to_csv(os.path.join(config.submission_path, config.submission_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
