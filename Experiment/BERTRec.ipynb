{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from box import Box\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.학습 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    'data_path' : \"/opt/ml/input/data/train\" , # 데이터 경로\n",
    "    'model_path' : \"../model\", # 모델 저장 경로\n",
    "    'model_name' : 'BRETRec_v1.pt',\n",
    "\n",
    "    'max_len' : 50,\n",
    "    'hidden_units' : 50, # Embedding size\n",
    "    'num_heads' : 1, # Multi-head layer 의 수 (병렬 처리)\n",
    "    'num_layers': 2, # block의 개수 (encoder layer의 개수)\n",
    "    'dropout_rate' : 0.5, # dropout 비율\n",
    "\n",
    "    'lr' : 0.001,\n",
    "    'batch_size' : 128,\n",
    "    'num_epochs' : 200,\n",
    "    'num_workers' : 2,\n",
    "    'mask_prob' : 0.15, # for cloze task\n",
    "}\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "config = Box(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeSequenceDataSet():\n",
    "    \"\"\"\n",
    "    SequenceData 생성\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.df = pd.read_csv(os.path.join(self.config.data_path, 'train_ratings.csv'))\n",
    "\n",
    "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('item')\n",
    "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('user')\n",
    "        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n",
    "\n",
    "        self.df['item_idx'] = self.df['item'].apply(lambda x : self.item_encoder[x] + 1) # padding 고려\n",
    "        self.df['user_idx'] = self.df['user'].apply(lambda x : self.user_encoder[x])\n",
    "        self.df = self.df.sort_values(['user_idx', 'time']) # 시간에 따라 정렬\n",
    "        self.user_train, self.user_valid = self.generate_sequence_data()\n",
    "\n",
    "    def generate_encoder_decoder(self, col : str) -> dict:\n",
    "        \"\"\"\n",
    "        encoder, decoder 생성\n",
    "\n",
    "        Args:\n",
    "            col (str): 생성할 columns 명\n",
    "        Returns:\n",
    "            dict: 생성된 user encoder, decoder\n",
    "        \"\"\"\n",
    "\n",
    "        encoder = {}\n",
    "        decoder = {}\n",
    "        ids = self.df[col].unique()\n",
    "\n",
    "        for idx, _id in enumerate(ids):\n",
    "            encoder[_id] = idx\n",
    "            decoder[idx] = _id\n",
    "\n",
    "        return encoder, decoder\n",
    "    \n",
    "    def generate_sequence_data(self) -> dict:\n",
    "        \"\"\"\n",
    "        sequence_data 생성\n",
    "\n",
    "        Returns:\n",
    "            dict: train user sequence / valid user sequence\n",
    "        \"\"\"\n",
    "        users = defaultdict(list)\n",
    "        user_train = {}\n",
    "        user_valid = {}\n",
    "        for user, item, time in zip(self.df['user_idx'], self.df['item_idx'], self.df['time']):\n",
    "            users[user].append(item)\n",
    "        \n",
    "        for user in users:\n",
    "            user_train[user] = users[user][:-1]\n",
    "            user_valid[user] = [users[user][-1]] # 마지막 아이템을 예측\n",
    "\n",
    "        return user_train, user_valid\n",
    "    \n",
    "    def get_train_valid_data(self):\n",
    "        return self.user_train, self.user_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTRecDataSet(Dataset):\n",
    "    def __init__(self, user_train, max_len, num_user, num_item, mask_prob):\n",
    "        self.user_train = user_train\n",
    "        self.max_len = max_len\n",
    "        self.num_user = num_user\n",
    "        self.num_item = num_item\n",
    "        self.mask_prob = mask_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        # 총 user의 수 = 학습에 사용할 sequence의 수\n",
    "        return self.num_user\n",
    "\n",
    "    def __getitem__(self, user): \n",
    "        \n",
    "        user_seq = self.user_train[user]\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for s in user_seq[-self.max_len:]:\n",
    "            prob = np.random.random()\n",
    "            if prob < self.mask_prob:\n",
    "                prob /= self.mask_prob\n",
    "                if prob < 0.8:\n",
    "                    # masking\n",
    "                    tokens.append(self.num_item + 1)  # mask_index: num_item + 1, 0: pad, 1~num_item: item index\n",
    "                elif prob < 0.9:\n",
    "                    # noise\n",
    "                    tokens.append(self.random_neg_sampling(user_seq))  # item random sampling\n",
    "                else:\n",
    "                    tokens.append(s)\n",
    "                labels.append(s) # 학습에 사용 O\n",
    "            else:\n",
    "                tokens.append(s)\n",
    "                labels.append(0) # 학습에 사용 X\n",
    "\n",
    "        mask_len = self.max_len - len(tokens)\n",
    "        tokens = [0] * mask_len + tokens\n",
    "        labels = [0] * mask_len + labels\n",
    "\n",
    "        return torch.LongTensor(tokens), torch.LongTensor(labels)\n",
    "\n",
    "    def random_neg_sampling(self, rated_item : list):\n",
    "        nge_sample = np.random.randint(1, self.num_item + 1)\n",
    "        while nge_sample in rated_item:\n",
    "            nge_sample = np.random.randint(1, self.num_item + 1)\n",
    "        return nge_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n",
    "\n",
    "    def forward(self, Q, K, V, mask):\n",
    "        attn_score = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.hidden_units)\n",
    "        attn_score = attn_score.masked_fill(mask == 0, -1e9)  # 유사도가 0인 지점은 -infinity로 보내 softmax 결과가 0이 되도록 함\n",
    "        attn_dist = self.dropout(F.softmax(attn_score, dim=-1))  # attention distribution\n",
    "        output = torch.matmul(attn_dist, V)  # dim of output : batchSize x num_head x seqLen x hidden_units\n",
    "        return output, attn_dist\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads # head의 수\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "        # query, key, value, output 생성을 위해 Linear 모델 생성\n",
    "        self.W_Q = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_K = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_V = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_O = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(hidden_units, dropout_rate) # scaled dot product attention module을 사용하여 attention 계산\n",
    "        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n",
    "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
    "\n",
    "    def forward(self, enc, mask):\n",
    "        residual = enc # residual connection을 위해 residual 부분을 저장\n",
    "        batch_size, seqlen = enc.size(0), enc.size(1)\n",
    "        \n",
    "        # Query, Key, Value를 (num_head)개의 Head로 나누어 각기 다른 Linear projection을 통과시킴\n",
    "        Q = self.W_Q(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units) \n",
    "        K = self.W_K(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units)\n",
    "        V = self.W_V(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units)\n",
    "\n",
    "        # Head별로 각기 다른 attention이 가능하도록 Transpose 후 각각 attention에 통과시킴\n",
    "        Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2)\n",
    "        output, attn_dist = self.attention(Q, K, V, mask)\n",
    "\n",
    "        # 다시 Transpose한 후 모든 head들의 attention 결과를 합칩니다.\n",
    "        output = output.transpose(1, 2).contiguous() \n",
    "        output = output.view(batch_size, seqlen, -1)\n",
    "\n",
    "        # Linear Projection, Dropout, Residual sum, and Layer Normalization\n",
    "        output = self.layerNorm(self.dropout(self.W_O(output)) + residual)\n",
    "        return output, attn_dist\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        \n",
    "        # SASRec과의 dimension 차이가 있습니다.\n",
    "        self.W_1 = nn.Linear(hidden_units, 4 * hidden_units) \n",
    "        self.W_2 = nn.Linear(4 * hidden_units, hidden_units)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = self.W_2(F.gelu(self.dropout(self.W_1(x)))) # activation: relu -> gelu\n",
    "        output = self.layerNorm(self.dropout(output) + residual)\n",
    "        return output\n",
    "\n",
    "\n",
    "class BERT4RecBlock(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
    "        super(BERT4RecBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads, hidden_units, dropout_rate)\n",
    "        self.pointwise_feedforward = PositionwiseFeedForward(hidden_units, dropout_rate)\n",
    "\n",
    "    def forward(self, input_enc, mask):\n",
    "        output_enc, attn_dist = self.attention(input_enc, mask)\n",
    "        output_enc = self.pointwise_feedforward(output_enc)\n",
    "        return output_enc, attn_dist\n",
    "\n",
    "        \n",
    "class BERT4Rec(nn.Module):\n",
    "    def __init__(self, num_user, num_item, hidden_units, num_heads, num_layers, max_len, dropout_rate, device):\n",
    "        super(BERT4Rec, self).__init__()\n",
    "\n",
    "        self.num_user = num_user\n",
    "        self.num_item = num_item\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers \n",
    "        self.device = device\n",
    "        \n",
    "        self.item_emb = nn.Embedding(num_item + 2, hidden_units, padding_idx=0) # TODO2: mask와 padding을 고려하여 embedding을 생성해보세요.\n",
    "        self.pos_emb = nn.Embedding(max_len, hidden_units) # learnable positional encoding\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.emb_layernorm = nn.LayerNorm(hidden_units, eps=1e-6)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([BERT4RecBlock(num_heads, hidden_units, dropout_rate) for _ in range(num_layers)])\n",
    "        self.out = nn.Linear(hidden_units, num_item + 1) # TODO3: 예측을 위한 output layer를 구현해보세요. (num_item 주의)\n",
    "        \n",
    "    def forward(self, log_seqs):\n",
    "        seqs = self.item_emb(torch.LongTensor(log_seqs).to(self.device))\n",
    "        positions = np.tile(np.array(range(log_seqs.shape[1])), [log_seqs.shape[0], 1])\n",
    "        seqs += self.pos_emb(torch.LongTensor(positions).to(self.device))\n",
    "        seqs = self.emb_layernorm(self.dropout(seqs))\n",
    "\n",
    "        mask = torch.BoolTensor(log_seqs > 0).unsqueeze(1).repeat(1, log_seqs.shape[1], 1).unsqueeze(1).to(self.device) # mask for zero pad\n",
    "        for block in self.blocks:\n",
    "            seqs, attn_dist = block(seqs, mask)\n",
    "        out = self.out(seqs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, data_loader):\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "    for seq, labels in data_loader:\n",
    "        logits = model(seq)\n",
    "\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        labels = labels.view(-1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss_val += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_val /= len(data_loader)\n",
    "\n",
    "    return loss_val\n",
    "\n",
    "def evaluate(model, user_train, user_valid, max_len, dataset, make_sequence_dataset):\n",
    "    model.eval()\n",
    "\n",
    "    NDCG = 0.0 # NDCG@10\n",
    "    HIT = 0.0 # HIT@10\n",
    "\n",
    "    num_item_sample = 100\n",
    "    num_user_sample = 1000\n",
    "\n",
    "    users = np.random.randint(0, make_sequence_dataset.num_user, num_user_sample) # 1000개만 sampling 하여 evaluation\n",
    "    for user in users:\n",
    "        seq = (user_train[user] + [make_sequence_dataset.num_item + 1])[-max_len:]\n",
    "        rated = set(user_train[user] + user_valid[user])\n",
    "        item_idx = user_valid[user] + [dataset.random_neg_sampling(rated) for _ in range(num_item_sample)]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = - model(np.array([seq]))\n",
    "            predictions = predictions[0][-1][item_idx] # sampling\n",
    "            rank = predictions.argsort().argsort()[0].item()\n",
    "\n",
    "        if rank < 10: # @10\n",
    "            NDCG += 1 / np.log2(rank + 2)\n",
    "            HIT += 1\n",
    "\n",
    "    NDCG /= num_user_sample\n",
    "    HIT /= num_user_sample\n",
    "\n",
    "    return NDCG, HIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_sequence_dataset = MakeSequenceDataSet(config = config)\n",
    "user_train, user_valid = make_sequence_dataset.get_train_valid_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertrec_dataset = BERTRecDataSet(\n",
    "    user_train = user_train, \n",
    "    max_len = config.max_len, \n",
    "    num_user = make_sequence_dataset.num_user, \n",
    "    num_item = make_sequence_dataset.num_item,\n",
    "    mask_prob = config.mask_prob,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "    bertrec_dataset, \n",
    "    batch_size = config.batch_size, \n",
    "    shuffle = True, \n",
    "    pin_memory = True,\n",
    "    num_workers = config.num_workers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT4Rec(\n",
    "    num_user = make_sequence_dataset.num_user, \n",
    "    num_item = make_sequence_dataset.num_item, \n",
    "    hidden_units = config.hidden_units, \n",
    "    num_heads = config.num_heads, \n",
    "    num_layers = config.num_layers, \n",
    "    max_len = config.max_len, \n",
    "    dropout_rate = config.dropout_rate, \n",
    "    device = device,\n",
    "    ).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0) # label이 0인 경우 무시\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ndcg = 0\n",
    "for epoch in range(1, config.num_epochs + 1):\n",
    "    tbar = tqdm(range(1))\n",
    "    for _ in tbar:\n",
    "        train_loss = train(\n",
    "            model = model, \n",
    "            criterion = criterion, \n",
    "            optimizer = optimizer, \n",
    "            data_loader = data_loader)\n",
    "        \n",
    "        ndcg, hit = evaluate(\n",
    "            model = model, \n",
    "            user_train = user_train, \n",
    "            user_valid = user_valid, \n",
    "            max_len = config.max_len,\n",
    "            dataset = bertrec_dataset, \n",
    "            make_sequence_dataset = make_sequence_dataset,\n",
    "            )\n",
    "\n",
    "        if best_ndcg < ndcg:\n",
    "            best_ndcg = ndcg\n",
    "            torch.save(model.state_dict(), os.path.join(config.model_path, config.model_name))\n",
    "\n",
    "        tbar.set_description(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 임베딩 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_emb = model.item_emb.weight[1 : -1].data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state = 0)\n",
    "new_arr = tsne.fit_transform(item_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(8, 8))\n",
    "\n",
    "ax.scatter(new_arr[:, 0], new_arr[:, 1], alpha = 0.7)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
